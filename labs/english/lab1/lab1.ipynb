{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe8bdb2b",
   "metadata": {},
   "source": [
    "# Azure AI Foundry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede096d",
   "metadata": {},
   "source": [
    "<center><img src=\"../../../images/Azure-AI-Foundry_1600x900.jpg\" alt=\"Azure AI Foundry\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7089c5",
   "metadata": {},
   "source": [
    "## Laboratory 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd08e8",
   "metadata": {},
   "source": [
    "In this laboratory we will connect to Azure OpenAI and perform various tasks: request API responses, use text-based responses, analyze the obtained responses, perform text-to-embeddings conversion, make API calls sending images, and also make calls to other LLM models.\n",
    "\n",
    "The first step is to validate the configuration of environment variables in the `.env` file located at the repository root.\n",
    "\n",
    "Fill in the variable values as requested.\n",
    "\n",
    "### Exercise 1 - API Call\n",
    "\n",
    "Let's import the necessary libraries for the laboratory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b3fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b95d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install openai dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee238123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../../../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01777b5d",
   "metadata": {},
   "source": [
    "Let's load the credentials into variables to facilitate use in the laboratory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d2721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "api_version=os.getenv(\"API_VERSION\")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "embedding_model = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dbe93e",
   "metadata": {},
   "source": [
    "Now let's initialize the client with the provided credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "  azure_endpoint = azure_endpoint[0], \n",
    "  api_key=api_key[0],  \n",
    "  api_version=api_version\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8012e157",
   "metadata": {},
   "source": [
    "After creating the client, we will make a simple call where we will pass:\n",
    "\n",
    "1. A message for the \"system\" role defining the LLM's role\n",
    "2. An initial user question\n",
    "3. An assistant response demonstrating how it should respond (example)\n",
    "4. A new question for it to answer based on the previously established context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366dc568",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=deployment_name, \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Does Azure OpenAI support customer-managed keys?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Yes, customer-managed keys are supported by Azure OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Do other Azure services also support this?\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d84927c",
   "metadata": {},
   "source": [
    "Now let's access the LLM response directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552dffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81af3739",
   "metadata": {},
   "source": [
    "### Exercise 2 - Analyzing the Response\n",
    "\n",
    "Now that we've made a call to Azure OpenAI, let's analyze the complete content of the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9e649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d3f6c8",
   "metadata": {},
   "source": [
    "Now let's structure the response in a more readable format for better data visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66377032",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict = {\n",
    "    \"id\": response.id,\n",
    "    \"model\": response.model,\n",
    "    \"created\": response.created,\n",
    "    \"usage\": {\n",
    "        \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "        \"completion_tokens\": response.usage.completion_tokens,\n",
    "        \"total_tokens\": response.usage.total_tokens\n",
    "    },\n",
    "    \"completion_tokens_details\": {\n",
    "        \"accepted_prediction_tokens\": response.usage.completion_tokens_details.accepted_prediction_tokens,\n",
    "        \"audio_tokens\": response.usage.completion_tokens_details.audio_tokens,\n",
    "        \"reasoning_tokens\": response.usage.completion_tokens_details.reasoning_tokens,\n",
    "        \"rejected_prediction_tokens\": response.usage.completion_tokens_details.rejected_prediction_tokens\n",
    "    },\n",
    "    \"choices\": [{\n",
    "        \"index\": choice.index,\n",
    "        \"message\": {\n",
    "            \"role\": choice.message.role,\n",
    "            \"content\": choice.message.content\n",
    "        },\n",
    "        \"finish_reason\": choice.finish_reason,\n",
    "        \"content_filter_results\": choice.content_filter_results\n",
    "    } for choice in response.choices],\n",
    "    \"prompt_filter_results\": response.prompt_filter_results\n",
    "}\n",
    "\n",
    "print(json.dumps(response_dict, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf68207",
   "metadata": {},
   "source": [
    "The API doesn't respond only with the text generated by the LLM. We have much more information in this response, such as:\n",
    "- Whether it uses audio or image\n",
    "- Content filtering\n",
    "- Content evaluation\n",
    "- Prompt token count\n",
    "- Response token count\n",
    "- Details about reasoning tokens (for models that support it)\n",
    "- Applied filter results\n",
    "\n",
    "This information is essential for monitoring, costs, and application quality control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52eb10e",
   "metadata": {},
   "source": [
    "After making the call and exploring the response, test yourself by creating a custom prompt. Experiment with the following important parameters:\n",
    "\n",
    "- **max_completion_tokens**: Maximum number of tokens that can be generated in the response\n",
    "- **temperature**: Controls creativity (0.0 = more deterministic, 1.0 = more creative)\n",
    "- **top_p**: Controls response diversity via nucleus sampling\n",
    "- **frequency_penalty**: Penalizes token repetition based on frequency\n",
    "- **presence_penalty**: Penalizes token repetition regardless of frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca8fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I'm traveling to Paris, what should I see?\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks and romantic atmosphere. Here are some of the main attractions to see in Paris:\\n \\n 1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers stunning views of the city.\\n 2. The Louvre Museum: The Louvre is one of the largest and most famous museums in the world, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n 3. Notre-Dame Cathedral: This beautiful cathedral is one of Paris's most famous landmarks and is known for its Gothic architecture and stunning stained glass windows.\\n \\n These are just some of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the world's most popular tourist destinations.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What's so special about #1?\",\n",
    "        }\n",
    "    ],\n",
    "    max_completion_tokens=800,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0,\n",
    "    model=deployment_name\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1789c63a",
   "metadata": {},
   "source": [
    "### Exercise 3 - Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb2ca1a",
   "metadata": {},
   "source": [
    "Embeddings are numerical representations of text that capture the semantic meaning of words or phrases. In Azure OpenAI, you can use the embeddings model to convert text into numerical vectors that can be used for tasks like semantic search, classification, and similarity analysis.\n",
    "\n",
    "For more information on how to work with embeddings in Azure OpenAI, consult the [official documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/embeddings?tabs=python-new)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620c8480",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.embeddings.create(\n",
    "    input = \"dog\",\n",
    "    model= embedding_model\n",
    ")\n",
    "\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8b451a",
   "metadata": {},
   "source": [
    "Here we generated the embedding of a single word, but we can do the same for larger text segments. The model will automatically organize the content into numerical vectors that capture semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913cb16e",
   "metadata": {},
   "source": [
    "To store embeddings we can use a series of services available in Azure. Just choose the one that best fits your solution:\n",
    "\n",
    "- [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/vector-search-overview)\n",
    "- [Azure Cosmos DB for MongoDB vCore](https://learn.microsoft.com/en-us/azure/cosmos-db/mongodb/vcore/vector-search)\n",
    "- [Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/ai-artificial-intelligence-intelligent-applications?view=azuresql&preserve-view=true#vector-search)\n",
    "- [Azure Cosmos DB for NoSQL](https://learn.microsoft.com/en-us/azure/cosmos-db/vector-search)\n",
    "- [Azure Cosmos DB for PostgreSQL](https://learn.microsoft.com/en-us/azure/cosmos-db/postgresql/howto-use-pgvector)\n",
    "- [Azure Database for PostgreSQL - Flexible Server](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/how-to-use-pgvector)\n",
    "- [Azure Cache for Redis](https://learn.microsoft.com/en-us/azure/azure-cache-for-redis/cache-tutorial-vector-similarity)\n",
    "- [Use Eventhouse as a vector database - Real-Time Intelligence in Microsoft Fabric](https://learn.microsoft.com/en-us/fabric/real-time-intelligence/vector-database)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3936e0",
   "metadata": {},
   "source": [
    "### Exercise 4 - Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5c36b3",
   "metadata": {},
   "source": [
    "In Azure AI Foundry we can work with models that process images, both for image generation and multimodal models in which we can use images as context. In this exercise we will learn how to use images as prompt context.\n",
    "\n",
    "**Important first point**: we need to think about how to send an image along with the prompt. For this we have 2 main options:\n",
    "1. Send the image along with the prompt via base64 (encoded)\n",
    "2. Send the image as a link/URL\n",
    "\n",
    "Let's see the 2 practical examples below.\n",
    "\n",
    "First, let's leverage the client we already instantiated and send a URL of an image, asking the model to describe it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9537c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Itaim_Bibi_Business_District.jpg/250px-Itaim_Bibi_Business_District.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4a6bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "        { \"role\": \"user\", \"content\": [  \n",
    "            { \n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"Describe this image:\" \n",
    "            },\n",
    "            { \n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": image_url\n",
    "                }\n",
    "            }\n",
    "        ] } \n",
    "    ],\n",
    "    max_tokens=2000 \n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc71cffb",
   "metadata": {},
   "source": [
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Itaim_Bibi_Business_District.jpg/250px-Itaim_Bibi_Business_District.jpg\" alt=\"Azure AI Foundry\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924187c9",
   "metadata": {},
   "source": [
    "Now let's read a local image stored on our system and send it along with the message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from mimetypes import guess_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a22250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_image_to_data_url(image_path):\n",
    "    # Guess the MIME type of the image based on the file extension\n",
    "    mime_type, _ = guess_type(image_path)\n",
    "    if mime_type is None:\n",
    "        mime_type = 'application/octet-stream'  # Default MIME type if none is found\n",
    "\n",
    "    # Read and encode the image file\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_encoded_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    # Construct the data URL\n",
    "    return f\"data:{mime_type};base64,{base64_encoded_data}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6876fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"../../../samples/234039841.jpg\"\n",
    "data_url = local_image_to_data_url(image_path)\n",
    "print(\"Data URL:\", data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9fce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "        { \"role\": \"user\", \"content\": [  \n",
    "            { \n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"Describe this image:\" \n",
    "            },\n",
    "            { \n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": data_url\n",
    "                }\n",
    "            }\n",
    "        ] } \n",
    "    ],\n",
    "    max_tokens=2000 \n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03d6098",
   "metadata": {},
   "source": [
    "Using Azure OpenAI we have access to various types of functionalities beyond those we explored here. I recommend browsing and exploring the available options to understand what is the best approach for your specific application:\n",
    "\n",
    "- [Responses API](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/responses)\n",
    "- [Reasoning Models](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/reasoning)\n",
    "- [Chat completions API](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chatgpt)\n",
    "- [Computer Use](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/computer-use)\n",
    "- [Model router concepts](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/model-router)\n",
    "- [Function calling](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/function-calling)\n",
    "- [Predicted outputs](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/predicted-outputs)\n",
    "- [Prompt caching](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/prompt-caching)\n",
    "- [Structured outputs](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/structured-outputs)\n",
    "- [Vision-enabled chats](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/structured-outputs)\n",
    "- [JSON Mode](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/json-mode)\n",
    "- [Reproducible output](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/reproducible-output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e37d93d",
   "metadata": {},
   "source": [
    "### Exercise 5 - Other models in Azure AI Foundry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb000c2",
   "metadata": {},
   "source": [
    "Through Azure AI Foundry we can explore a series of models available in the [Model Catalog](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/foundry-models-overview). \n",
    "\n",
    "There we have access to models that are made available by Microsoft (OpenAI, Meta, Mistral AI, Deepseek, xAI, Black Forest Labs) as well as models made available by partners and the community (Nixtla, AI21, NTT Data, Core42, NVIDIA NIM Microservices, Stability AI). \n",
    "\n",
    "Through the provided documentation it is possible to understand the difference between the different modes of model availability and how to choose according to your specific scenario.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d8e99f",
   "metadata": {},
   "source": [
    "Now let's continue with a practical example of how to call a model made available by Azure AI Foundry through a chat completion call using a different library from the previous one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8170f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84294282",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-inference azure-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a445d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = os.getenv(\"AZURE_PHI4_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_PHI4_API_KEY\")\n",
    "model_name = os.getenv(\"AZURE_PHI4_DEPLOYMENT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53712c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientPhi = ChatCompletionsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(api_key),\n",
    "    api_version=\"2024-05-01-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372c1a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = clientPhi.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        UserMessage(content=\"I'm traveling to Paris, what should I see?\"),\n",
    "    ],\n",
    "    max_tokens=2048,\n",
    "    temperature=0.8,\n",
    "    top_p=0.1,\n",
    "    presence_penalty=0.0,\n",
    "    frequency_penalty=0.0,\n",
    "    model=model_name\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a23efb",
   "metadata": {},
   "source": [
    "## 🎯 Practical Activities \n",
    "\n",
    "Now that you've explored the basic concepts of Azure AI Foundry, let's practice with some simple and targeted activities to consolidate your learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3300ee",
   "metadata": {},
   "source": [
    "### 📝 Activity 1: Temperature Testing\n",
    "**Objective**: Understand how temperature affects the creativity of responses.\n",
    "\n",
    "Execute the code below and observe how the same prompt generates different responses with varied temperatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5a9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a creative slogan for a technology company.\"\n",
    "\n",
    "# Testing different temperatures\n",
    "temperatures = [0.1, 0.5, 1.0]\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n🌡️ TEMPERATURE: {temp}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a creative marketing assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temp,\n",
    "        max_completion_tokens=100\n",
    "    )\n",
    "    \n",
    "    print(response.choices[0].message.content)\n",
    "    print(f\"Tokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dfbbb4",
   "metadata": {},
   "source": [
    "### 🔍 Activity 2: Embeddings Comparison\n",
    "**Objective**: Compare how similar words have close embeddings.\n",
    "\n",
    "Let's generate embeddings for related words and see their sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0b6943",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e7a999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Words to compare\n",
    "words = [\"cat\", \"feline\", \"dog\", \"canine\", \"automobile\", \"car\"]\n",
    "\n",
    "embeddings_dict = {}\n",
    "\n",
    "print(\"Generating embeddings for the words...\")\n",
    "for word in words:\n",
    "    response = client.embeddings.create(\n",
    "        input=word,\n",
    "        model=embedding_model\n",
    "    )\n",
    "    embedding = response.data[0].embedding\n",
    "    embeddings_dict[word] = embedding\n",
    "    print(f\"✅ {word}: {len(embedding)} dimensions\")\n",
    "\n",
    "print(f\"\\nFirst 5 values of the embedding for the word 'cat':\")\n",
    "print(embeddings_dict[\"cat\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359ebc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity\n",
    "def calculate_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Comparing similarities\n",
    "print(\"🔍 Comparing similarities:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Cat vs Feline\n",
    "sim_cat_feline = calculate_similarity(embeddings_dict[\"cat\"], embeddings_dict[\"feline\"])\n",
    "print(f\"Cat ↔ Feline: {sim_cat_feline:.3f}\")\n",
    "\n",
    "# Dog vs Canine\n",
    "sim_dog_canine = calculate_similarity(embeddings_dict[\"dog\"], embeddings_dict[\"canine\"])\n",
    "print(f\"Dog ↔ Canine: {sim_dog_canine:.3f}\")\n",
    "\n",
    "# Automobile vs Car\n",
    "sim_auto_car = calculate_similarity(embeddings_dict[\"automobile\"], embeddings_dict[\"car\"])\n",
    "print(f\"Automobile ↔ Car: {sim_auto_car:.3f}\")\n",
    "\n",
    "# Cat vs Car (should be low)\n",
    "sim_cat_car = calculate_similarity(embeddings_dict[\"cat\"], embeddings_dict[\"car\"])\n",
    "print(f\"Cat ↔ Car: {sim_cat_car:.3f}\")\n",
    "\n",
    "print(f\"\\n💡 Similar words have higher similarity (close to 1.0)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40481f5e",
   "metadata": {},
   "source": [
    "### 🖼️ Activity 3: Image Analysis with Different Prompts\n",
    "**Objective**: Test how different prompts affect the analysis of the same image.\n",
    "\n",
    "Let's use different types of questions for the same image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508163fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the same image with different prompts\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Itaim_Bibi_Business_District.jpg/250px-Itaim_Bibi_Business_District.jpg\"\n",
    "\n",
    "# Different types of analysis\n",
    "prompts = [\n",
    "    \"Describe this image in one sentence:\",\n",
    "    \"What type of location is this?\",\n",
    "    \"What colors predominate in this image?\",\n",
    "    \"What feeling does this image convey?\",\n",
    "    \"Count the buildings you can see:\"\n",
    "]\n",
    "\n",
    "for i, prompt_text in enumerate(prompts, 1):\n",
    "    print(f\"\\n🔍 QUESTION {i}: {prompt_text}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant specialized in image analysis.\"},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt_text},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
    "            ]}\n",
    "        ],\n",
    "        max_tokens=150\n",
    "    )\n",
    "    \n",
    "    print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b068d",
   "metadata": {},
   "source": [
    "### 🔢 Activity 4: Token Counter\n",
    "**Objective**: Understand how prompt size affects token consumption.\n",
    "\n",
    "Let's test prompts of different sizes and see the impact on tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2071d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts of different sizes\n",
    "test_prompts = [\n",
    "    \"Hello\",\n",
    "    \"Explain what artificial intelligence is\",\n",
    "    \"Explain in detail what artificial intelligence is, how it works, its practical applications, benefits and challenges for modern society\"\n",
    "]\n",
    "\n",
    "print(\"📊 TOKEN CONSUMPTION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_completion_tokens=100  # Limiting response to focus on prompt\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🔍 TEST {i}:\")\n",
    "    print(f\"Prompt: '{prompt[:50]}{'...' if len(prompt) > 50 else ''}'\")\n",
    "    print(f\"Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "    print(f\"Response tokens: {response.usage.completion_tokens}\")\n",
    "    print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "    print(f\"Response: {response.choices[0].message.content[:100]}...\")\n",
    "\n",
    "print(\"\\n💡 Larger prompts consume more input tokens!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c8e959",
   "metadata": {},
   "source": [
    "### 🎭 Activity 5: Persona Testing\n",
    "**Objective**: See how different personas (system messages) affect responses.\n",
    "\n",
    "Let's ask the same question to different \"personalities\" of the assistant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf6f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different personas to test\n",
    "personas = [\n",
    "    {\"name\": \"Professor\", \"system\": \"You are a university professor who explains concepts in a didactic and detailed manner.\"},\n",
    "    {\"name\": \"Friend\", \"system\": \"You are a close friend who speaks in a casual and relaxed way.\"},\n",
    "    {\"name\": \"Expert\", \"system\": \"You are a technical expert who gives precise and direct answers.\"},\n",
    "    {\"name\": \"Poet\", \"system\": \"You are a poet who always responds in a creative and artistic way.\"}\n",
    "]\n",
    "\n",
    "question = \"What do you think about the future of technology?\"\n",
    "\n",
    "print(\"🎭 TESTING DIFFERENT PERSONAS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for persona in personas:\n",
    "    print(f\"\\n👤 PERSONA: {persona['name']}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": persona[\"system\"]},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        max_completion_tokens=200,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\n💡 The system message completely defines the assistant's 'way'!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
