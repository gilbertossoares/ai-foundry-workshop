{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d7d842",
   "metadata": {},
   "source": [
    "# Azure AI Foundry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604c22e3",
   "metadata": {},
   "source": [
    "<center><img src=\"../../../images/Azure-AI-Foundry_1600x900.jpg\" alt=\"Azure AI Foundry\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1994bd",
   "metadata": {},
   "source": [
    "## Lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781eddfe",
   "metadata": {},
   "source": [
    "In this lab we will explore the AI services present in Azure Foundry. This lab will cover the following services:\n",
    "- Speech\n",
    "- Language + Translator\n",
    "- Vision + Document\n",
    "- Content Safety\n",
    "\n",
    "Understanding these services allows us to add more capabilities to our applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698f59ae",
   "metadata": {},
   "source": [
    "### Exercise 1 - Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a483ed",
   "metadata": {},
   "source": [
    "The Speech service provides speech-to-text and text-to-speech conversion capabilities with a Speech resource. You can transcribe speech to text with high accuracy, produce natural text-to-speech voices, translate spoken audio, and use speaker recognition during conversations. Create custom voices, add specific words to your base vocabulary, or build your own models. Run Speech anywhere, in the cloud or at the edge in containers. It's easy to enable speech in your applications, tools, and devices with the Speech CLI, Speech SDK, and REST APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b03803",
   "metadata": {},
   "source": [
    "Common scenarios for speech usage:\n",
    "\n",
    "**Caption Generation:** Learn how to synchronize captions with your input audio, apply profanity filters, get partial results, apply customizations, and identify spoken languages for multilingual scenarios.\n",
    "\n",
    "**Audio Content Creation:** You can use neural voices to make interactions with chatbots and voice assistants more natural and engaging, convert digital texts like e-books into audiobooks, and enhance automotive navigation systems.\n",
    "\n",
    "**Call Center:** Transcribe calls in real-time or process a batch of calls, remove personally identifiable information, and extract insights like sentiment analysis to assist with your call center use case.\n",
    "\n",
    "**Language Learning:** Provide pronunciation assessment feedback for language learners, offer real-time transcription support for remote learning conversations, and read educational materials aloud using neural voices.\n",
    "\n",
    "**Voice Assistants:** Create natural, human-like conversational interfaces for your applications and experiences. The voice assistant feature offers fast and reliable interaction between a device and an assistant implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4389770f",
   "metadata": {},
   "source": [
    "To perform this exercise, verify that your `.env` file has the following variables filled:\n",
    "- SPEECH_ENDPOINT\n",
    "- SPEECH_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a883326c",
   "metadata": {},
   "source": [
    "After verification, let's start by loading the necessary libraries, initializing the client, and making a call to convert audio to text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e949bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-cognitiveservices-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ac5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous speech recognition to process all audio, even with initial silence\n",
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "speech_key = os.getenv('SPEECH_KEY')\n",
    "speech_region = os.getenv('SPEECH_REGION')\n",
    "audio_file = '../../../samples/audio001.wav'\n",
    "if speech_key and speech_region:\n",
    "    try:\n",
    "        speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "        speech_config.speech_recognition_language = \"en-US\"\n",
    "        audio_config = speechsdk.audio.AudioConfig(filename=audio_file)\n",
    "        speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "        recognized_texts = []\n",
    "        def recognized_cb(evt):\n",
    "            if evt.result.text:\n",
    "                print('Recognized:', evt.result.text)\n",
    "                recognized_texts.append(evt.result.text)\n",
    "\n",
    "        speech_recognizer.recognized.connect(recognized_cb)\n",
    "\n",
    "        print(\"Starting continuous recognition...\")\n",
    "        speech_recognizer.start_continuous_recognition()\n",
    "\n",
    "        # Wait for recognition to finish (adjust time according to audio size)\n",
    "        time.sleep(10)\n",
    "        speech_recognizer.stop_continuous_recognition()\n",
    "\n",
    "        print(\"Recognition completed. Full text:\")\n",
    "        print(' '.join(recognized_texts))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in continuous recognition: {e}\")\n",
    "else:\n",
    "    print(\"Please configure the SPEECH_KEY and SPEECH_REGION environment variables.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
