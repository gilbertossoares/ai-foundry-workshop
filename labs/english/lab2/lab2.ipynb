{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d7d842",
   "metadata": {},
   "source": [
    "# Azure AI Foundry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604c22e3",
   "metadata": {},
   "source": [
    "<center><img src=\"../../../images/Azure-AI-Foundry_1600x900.jpg\" alt=\"Azure AI Foundry\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1994bd",
   "metadata": {},
   "source": [
    "## Lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781eddfe",
   "metadata": {},
   "source": [
    "In this lab we will explore the AI services present in Azure Foundry. This lab will cover the following services:\n",
    "- Speech\n",
    "- Language + Translator\n",
    "- Vision + Document\n",
    "- Content Safety\n",
    "\n",
    "Understanding these services allows us to add more capabilities to our applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698f59ae",
   "metadata": {},
   "source": [
    "### Exercise 1 - Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a483ed",
   "metadata": {},
   "source": [
    "The Speech service provides speech-to-text and text-to-speech conversion capabilities with a Speech resource. You can transcribe speech to text with high accuracy, produce natural text-to-speech voices, translate spoken audio, and use speaker recognition during conversations. Create custom voices, add specific words to your base vocabulary, or build your own models. Run Speech anywhere, in the cloud or at the edge in containers. It's easy to enable speech in your applications, tools, and devices with the Speech CLI, Speech SDK, and REST APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b03803",
   "metadata": {},
   "source": [
    "Common scenarios for speech usage:\n",
    "\n",
    "**Caption Generation:** Learn how to synchronize captions with your input audio, apply profanity filters, get partial results, apply customizations, and identify spoken languages for multilingual scenarios.\n",
    "\n",
    "**Audio Content Creation:** You can use neural voices to make interactions with chatbots and voice assistants more natural and engaging, convert digital texts like e-books into audiobooks, and enhance automotive navigation systems.\n",
    "\n",
    "**Call Center:** Transcribe calls in real-time or process a batch of calls, remove personally identifiable information, and extract insights like sentiment analysis to assist with your call center use case.\n",
    "\n",
    "**Language Learning:** Provide pronunciation assessment feedback for language learners, offer real-time transcription support for remote learning conversations, and read educational materials aloud using neural voices.\n",
    "\n",
    "**Voice Assistants:** Create natural, human-like conversational interfaces for your applications and experiences. The voice assistant feature offers fast and reliable interaction between a device and an assistant implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4389770f",
   "metadata": {},
   "source": [
    "To perform this exercise, verify that your `.env` file has the following variables filled:\n",
    "- SPEECH_ENDPOINT\n",
    "- SPEECH_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a883326c",
   "metadata": {},
   "source": [
    "After verification, let's start by loading the necessary libraries, initializing the client, and making a call to convert audio to text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e949bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-cognitiveservices-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ac5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous speech recognition to process all audio, even with initial silence\n",
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "speech_key = os.getenv('SPEECH_KEY')\n",
    "speech_region = os.getenv('SPEECH_REGION')\n",
    "audio_file = '../../../samples/audio001.wav'\n",
    "if speech_key and speech_region:\n",
    "    try:\n",
    "        speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "        speech_config.speech_recognition_language = \"en-US\"\n",
    "        audio_config = speechsdk.audio.AudioConfig(filename=audio_file)\n",
    "        speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "        recognized_texts = []\n",
    "        def recognized_cb(evt):\n",
    "            if evt.result.text:\n",
    "                print('Recognized:', evt.result.text)\n",
    "                recognized_texts.append(evt.result.text)\n",
    "\n",
    "        speech_recognizer.recognized.connect(recognized_cb)\n",
    "\n",
    "        print(\"Starting continuous recognition...\")\n",
    "        speech_recognizer.start_continuous_recognition()\n",
    "\n",
    "        # Wait for recognition to finish (adjust time according to audio size)\n",
    "        time.sleep(10)\n",
    "        speech_recognizer.stop_continuous_recognition()\n",
    "\n",
    "        print(\"Recognition completed. Full text:\")\n",
    "        print(' '.join(recognized_texts))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in continuous recognition: {e}\")\n",
    "else:\n",
    "    print(\"Please configure the SPEECH_KEY and SPEECH_REGION environment variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f297913",
   "metadata": {},
   "source": [
    "### Exercise 2 - Language + Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d13999",
   "metadata": {},
   "source": [
    "Integrate natural language into applications, bots, and IoT devices. For example, this service can remove sensitive data, segment long meetings into chapters, analyze health records, and orchestrate conversational bots with your custom intents using factual responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5da064",
   "metadata": {},
   "source": [
    "This Language service unifies the following Azure AI services previously available separately: Text Analytics, QnA Maker, and LUIS.\n",
    "\n",
    "Azure AI Foundry allows you to use most of the following service features without needing to write code.\n",
    "\n",
    "**Named Entity Recognition (NER)** - Named entity recognition identifies different entries in text and categorizes them into predefined types.\n",
    "\n",
    "**Personal and health information detection** - PII detection identifies entities in text and conversations (chat or transcriptions) that are associated with individuals.\n",
    "\n",
    "**Language detection** - Language detection evaluates text and detects a wide range of languages and variant dialects.\n",
    "\n",
    "**Sentiment analysis and opinion mining** - Sentiment analysis and opinion mining are pre-configured features that help you understand public perception of your brand or topic. These features analyze text to identify positive or negative sentiments and can link them to specific elements within the text.\n",
    "\n",
    "**Summarization** - Summarization condenses information for text and conversations (chat and transcriptions). Text summarization generates a summary, supporting two approaches: Extractive summarization creates a summary by selecting key phrases from the document and preserving their original positions. In contrast, abstractive summarization generates a summary by producing new, concise, and coherent sentences or phrases that are not copied directly from the original document. Conversation summarization recaps and segments long meetings into chapters with timestamps. Call center summarization summarizes customer problems and their resolutions.\n",
    "\n",
    "**Key phrase extraction** - Key phrase extraction is a pre-configured feature that evaluates and returns the main concepts in unstructured text, returning them as a list.\n",
    "\n",
    "**Entity linking** - Entity linking is a pre-configured feature that disambiguates the identity of entities (words or phrases) found in unstructured text and returns links to Wikipedia.\n",
    "\n",
    "**Text analytics for health** - Text analytics for health extracts and labels relevant health information from unstructured text.\n",
    "\n",
    "**Custom text classification** - Custom text classification allows you to build custom AI models to classify unstructured text documents into custom classes you define.\n",
    "\n",
    "**Custom Named Entity Recognition (Custom NER)** - Custom NER allows you to build custom AI models to extract custom entity categories (labels for words or phrases), using unstructured text you provide.\n",
    "\n",
    "**Conversational language understanding** - Conversational language understanding (CLU) allows users to build custom natural language understanding models to predict the overall intent of an incoming statement and extract important information from it.\n",
    "\n",
    "**Orchestration workflow** - The orchestration workflow is a custom feature that allows you to connect Conversational Language Understanding (CLU), question answering, and LUIS applications.\n",
    "\n",
    "**Question answering** - Question answering is a custom feature that identifies the most appropriate answer for user inputs. This feature is typically used to develop conversational client applications, including social media platforms, chat bots, and voice-enabled desktop applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d583c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-textanalytics azure-ai-translation-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb5217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - Language Service\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.translation.text import TextTranslationClient\n",
    "from azure.ai.translation.text.models import InputTextItem\n",
    "\n",
    "# Configure Language client\n",
    "language_endpoint = os.getenv('AZURE_LANGUAGE_ENDPOINT')\n",
    "language_key = os.getenv('AZURE_LANGUAGE_KEY')\n",
    "\n",
    "# Configure Translator\n",
    "translator_endpoint = os.getenv('TRANSLATOR_ENDPOINT')\n",
    "translator_key = os.getenv('TRANSLATOR_KEY')\n",
    "translator_region = os.getenv('TRANSLATOR_REGION')\n",
    "\n",
    "if language_endpoint and language_key:\n",
    "    # Create client\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=language_endpoint,\n",
    "        credential=AzureKeyCredential(language_key)\n",
    "    )\n",
    "    \n",
    "    # Sample text\n",
    "    documents = [\n",
    "        \"I absolutely love this product! It's amazing and works perfectly.\",\n",
    "        \"This service is terrible, it didn't work as expected.\",\n",
    "        \"The service was adequate, nothing exceptional.\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Sentiment analysis\n",
    "        response = text_analytics_client.analyze_sentiment(documents=documents, language=\"en\")\n",
    "        \n",
    "        print(\"=== Sentiment Analysis ===\")\n",
    "        for idx, doc in enumerate(response):\n",
    "            if not doc.is_error:\n",
    "                print(f\"Document {idx + 1}:\")\n",
    "                print(f\"  Text: {documents[idx]}\")\n",
    "                print(f\"  Sentiment: {doc.sentiment}\")\n",
    "                print(f\"  Confidence: Positive={doc.confidence_scores.positive:.2f}, \"\n",
    "                      f\"Neutral={doc.confidence_scores.neutral:.2f}, \"\n",
    "                      f\"Negative={doc.confidence_scores.negative:.2f}\")\n",
    "                print()\n",
    "            else:\n",
    "                print(f\"Error in document {idx + 1}: {doc.error}\")\n",
    "                \n",
    "        # Key phrase extraction\n",
    "        key_phrases_response = text_analytics_client.extract_key_phrases(documents=documents, language=\"en\")\n",
    "        \n",
    "        print(\"=== Key Phrase Extraction ===\")\n",
    "        for idx, doc in enumerate(key_phrases_response):\n",
    "            if not doc.is_error:\n",
    "                print(f\"Document {idx + 1}:\")\n",
    "                print(f\"  Key phrases: {', '.join(doc.key_phrases)}\")\n",
    "                print()\n",
    "            else:\n",
    "                print(f\"Error in document {idx + 1}: {doc.error}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Define AZURE_LANGUAGE_ENDPOINT and AZURE_LANGUAGE_KEY variables in the .env file\")\n",
    "\n",
    "# Translator\n",
    "if translator_key and translator_region:\n",
    "    try:\n",
    "        print(\"\\n\\n=== Azure AI Translator ===\")\n",
    "        \n",
    "        # Create Translator client\n",
    "        translator_client = TextTranslationClient(\n",
    "            endpoint=translator_endpoint,\n",
    "            credential=AzureKeyCredential(translator_key),\n",
    "            region=translator_region\n",
    "        )\n",
    "        \n",
    "        # Text to translate\n",
    "        source_text = \"Hello, world! This is a test of the Azure AI Translator service.\"\n",
    "        input_text_elements = [InputTextItem(text=source_text)]\n",
    "        \n",
    "        print(f\"\\n📝 Original text: {source_text}\")\n",
    "        print(\"\\n🌍 Translations:\")\n",
    "        \n",
    "        # Translate to multiple languages\n",
    "        target_languages = ['pt', 'es', 'fr', 'de', 'ja']\n",
    "        \n",
    "        response = translator_client.translate(\n",
    "            content=input_text_elements,\n",
    "            to=target_languages\n",
    "        )\n",
    "        \n",
    "        for translation_result in response:\n",
    "            for translation in translation_result.translations:\n",
    "                language_names = {\n",
    "                    'pt': 'Portuguese',\n",
    "                    'es': 'Spanish', \n",
    "                    'fr': 'French',\n",
    "                    'de': 'German',\n",
    "                    'ja': 'Japanese'\n",
    "                }\n",
    "                language_name = language_names.get(translation.to, translation.to)\n",
    "                print(f\"  {language_name} ({translation.to}): {translation.text}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error in translator: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Define TRANSLATOR_KEY and TRANSLATOR_REGION variables in the .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd79bba",
   "metadata": {},
   "source": [
    "### Exercise 3 - Vision + Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd9c271",
   "metadata": {},
   "source": [
    "Give your applications the ability to read text, analyze images, process documents, and detect faces with technologies like optical character recognition (OCR) and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d41cab",
   "metadata": {},
   "source": [
    "The Azure AI Vision service provides access to advanced algorithms that process images and return information based on the visual characteristics of your interest. The following table lists the main product categories.\n",
    "\n",
    "**Optical Character Recognition (OCR)** - The Optical Character Recognition (OCR) service extracts text from images. You can use the Read API to extract printed and handwritten text from photos and documents. It uses deep learning-based models and works with text on various surfaces and backgrounds. This includes business documents, invoices, receipts, posters, business cards, letters, and whiteboards. The OCR APIs support extracting printed text in several languages.\n",
    "\n",
    "**Image Analysis** - The Image Analysis service extracts many visual features from images, such as objects, faces, adult content, and automatically generated text descriptions.\n",
    "\n",
    "**Face** - The Face service provides AI algorithms that detect, recognize, and analyze human faces in images. Facial recognition software is important in many different scenarios, such as identification, touchless access control, and face blurring for privacy.\n",
    "\n",
    "**Video Retrieval** - Video Retrieval allows you to create an index of videos that you can search with natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c14235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-vision-imageanalysis azure-ai-formrecognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85501886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Configure Vision client\n",
    "vision_endpoint = os.getenv('AZURE_VISION_ENDPOINT')\n",
    "vision_key = os.getenv('AZURE_VISION_KEY')\n",
    "\n",
    "if vision_endpoint and vision_key:\n",
    "    try:\n",
    "        print(\"=== Azure AI Vision - Image Analysis ===\")\n",
    "        \n",
    "        # Create client\n",
    "        client = ImageAnalysisClient(\n",
    "            endpoint=vision_endpoint,\n",
    "            credential=AzureKeyCredential(vision_key)\n",
    "        )\n",
    "        \n",
    "        # Analyze remote image\n",
    "        print(\"\\n1. Analyzing remote image...\")\n",
    "        result = client.analyze_from_url(\n",
    "            image_url=\"https://learn.microsoft.com/azure/ai-services/computer-vision/media/quickstarts/presentation.png\",\n",
    "            visual_features=[VisualFeatures.CAPTION, VisualFeatures.READ],\n",
    "            gender_neutral_caption=True,  # Optional (default is False)\n",
    "        )\n",
    "        \n",
    "        print(\"Analysis results:\")\n",
    "        \n",
    "        # Display caption results\n",
    "        print(\"\\n📝 Caption:\")\n",
    "        if result.caption is not None:\n",
    "            print(f\"   '{result.caption.text}', Confidence: {result.caption.confidence:.4f}\")\n",
    "        else:\n",
    "            print(\"   No caption found\")\n",
    "\n",
    "        # Display OCR results (extracted text)\n",
    "        print(\"\\n📖 Extracted text (OCR):\")\n",
    "        if result.read is not None:\n",
    "            for block in result.read.blocks:\n",
    "                for line in block.lines:\n",
    "                    print(f\"   Line: '{line.text}', Bounding box: {line.bounding_polygon}\")\n",
    "                    for word in line.words:\n",
    "                        print(f\"     Word: '{word.text}', Confidence: {word.confidence:.4f}\")\n",
    "        else:\n",
    "            print(\"   No text found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in image analysis: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Define AZURE_VISION_ENDPOINT and AZURE_VISION_KEY variables in the .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571cc40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced example - Complete local image analysis\n",
    "if vision_endpoint and vision_key:\n",
    "    try:\n",
    "        print(\"\\n=== Advanced Image Analysis ===\")\n",
    "        \n",
    "        # Analyze local image\n",
    "        image_path = \"../../../samples/car-accident.png\"\n",
    "        \n",
    "        print(f\"\\n2. Analyzing local image: {image_path}\")\n",
    "        \n",
    "        # Open local image\n",
    "        with open(image_path, \"rb\") as image_data:\n",
    "            # Complete analysis with multiple features\n",
    "            result = client.analyze(\n",
    "                image_data=image_data.read(),\n",
    "                visual_features=[\n",
    "                    VisualFeatures.CAPTION,\n",
    "                    VisualFeatures.READ,\n",
    "                    VisualFeatures.TAGS,\n",
    "                    VisualFeatures.OBJECTS,\n",
    "                    VisualFeatures.PEOPLE,\n",
    "                    VisualFeatures.SMART_CROPS\n",
    "                ],\n",
    "                gender_neutral_caption=True\n",
    "            )\n",
    "            \n",
    "            print(\"\\n📝 Caption:\")\n",
    "            if result.caption:\n",
    "                print(f\"   '{result.caption.text}', Confidence: {result.caption.confidence:.4f}\")\n",
    "            \n",
    "            print(\"\\n🏷️ Identified tags:\")\n",
    "            if result.tags:\n",
    "                for tag in result.tags.list:\n",
    "                    print(f\"   - {tag.name}: {tag.confidence:.4f}\")\n",
    "            \n",
    "            print(\"\\n📦 Detected objects:\")\n",
    "            if result.objects:\n",
    "                for obj in result.objects.list:\n",
    "                    print(f\"   - {obj.tags[0].name}: {obj.tags[0].confidence:.4f}\")\n",
    "                    print(f\"     Location: {obj.bounding_box}\")\n",
    "            \n",
    "            print(\"\\n👥 Detected people:\")\n",
    "            if result.people:\n",
    "                for person in result.people.list:\n",
    "                    print(f\"   - Person detected with confidence: {person.confidence:.4f}\")\n",
    "                    print(f\"     Location: {person.bounding_box}\")\n",
    "            \n",
    "            print(\"\\n✂️ Smart crops:\")\n",
    "            if result.smart_crops:\n",
    "                for crop in result.smart_crops.list:\n",
    "                    print(f\"   - Aspect {crop.aspect_ratio}: {crop.bounding_box}\")\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image file not found: {image_path}\")\n",
    "        print(\"Make sure the file exists at the specified path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in advanced analysis: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5aa915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - Analysis of different types of images\n",
    "if vision_endpoint and vision_key:\n",
    "    try:\n",
    "        print(\"\\n=== Comparison of Different Image Analyses ===\")\n",
    "        \n",
    "        # List of images to analyze\n",
    "        images_to_analyze = [\n",
    "            {\n",
    "                \"url\": \"https://learn.microsoft.com/azure/ai-services/computer-vision/media/quickstarts/presentation.png\",\n",
    "                \"description\": \"Calendar presentation\"\n",
    "            },\n",
    "            {\n",
    "                \"url\": \"https://images.unsplash.com/photo-1518791841217-8f162f1e1131?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1000&q=80\",\n",
    "                \"description\": \"Cat\"\n",
    "            },\n",
    "            {\n",
    "                \"url\": \"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1000&q=80\",\n",
    "                \"description\": \"Mountain landscape\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for i, image_info in enumerate(images_to_analyze, 1):\n",
    "            print(f\"\\n{i}. Analyzing: {image_info['description']}\")\n",
    "            print(f\"   URL: {image_info['url']}\")\n",
    "            \n",
    "            try:\n",
    "                # Analysis focused on caption and tags\n",
    "                result = client.analyze_from_url(\n",
    "                    image_url=image_info[\"url\"],\n",
    "                    visual_features=[VisualFeatures.CAPTION, VisualFeatures.TAGS],\n",
    "                    gender_neutral_caption=True\n",
    "                )\n",
    "                \n",
    "                if result.caption:\n",
    "                    print(f\"   📝 Caption: '{result.caption.text}' (Confidence: {result.caption.confidence:.4f})\")\n",
    "                \n",
    "                if result.tags:\n",
    "                    print(\"   🏷️ Top 5 tags:\")\n",
    "                    for tag in result.tags.list[:5]:\n",
    "                        print(f\"      - {tag.name}: {tag.confidence:.4f}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Error analyzing image {i}: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"General error in comparative analysis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9ee52",
   "metadata": {},
   "source": [
    "#### Azure AI Vision Features\n",
    "\n",
    "Azure AI Vision offers several features through the `azure-ai-vision-imageanalysis` SDK:\n",
    "\n",
    "**📝 Main Features:**\n",
    "- **CAPTION**: Generates descriptive captions for images\n",
    "- **READ**: Extracts text from images (OCR)\n",
    "- **TAGS**: Identifies objects, concepts, and actions in the image\n",
    "- **OBJECTS**: Detects and locates specific objects\n",
    "- **PEOPLE**: Identifies people in the image\n",
    "- **SMART_CROPS**: Suggests intelligent image crops\n",
    "- **FACES**: Detects and analyzes faces (separate functionality)\n",
    "\n",
    "**📊 Analysis Methods:**\n",
    "- `analyze_from_url()`: Analyzes image from a URL\n",
    "- `analyze()`: Analyzes image from binary data (local file)\n",
    "\n",
    "**⚙️ Important Parameters:**\n",
    "- `visual_features`: List of features to be analyzed\n",
    "- `gender_neutral_caption`: Generates gender-neutral captions\n",
    "- `language`: Language for results (default: English)\n",
    "- `smart_crops_aspect_ratios`: Proportions for intelligent crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f730b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Practical Exercise - Test your own images\n",
    "if vision_endpoint and vision_key:\n",
    "    print(\"=== Practical Exercise ===\")\n",
    "    print(\"\\n📝 Instructions:\")\n",
    "    print(\"1. Replace the URL below with an image of your choice\")\n",
    "    print(\"2. Choose the features you want to test\")\n",
    "    print(\"3. Run the cell and analyze the results\")\n",
    "    \n",
    "    # 🎯 MODIFY HERE: Paste an image URL to test\n",
    "    your_image_url = \"https://images.unsplash.com/photo-1544947950-fa07a98d237f?ixlib=rb-4.0.3&auto=format&fit=crop&w=800&q=80\"\n",
    "    \n",
    "    # 🎯 MODIFY HERE: Choose the features you want to test\n",
    "    selected_features = [\n",
    "        VisualFeatures.CAPTION,\n",
    "        VisualFeatures.TAGS,\n",
    "        VisualFeatures.OBJECTS,\n",
    "        # VisualFeatures.READ,          # Uncomment for OCR\n",
    "        # VisualFeatures.PEOPLE,        # Uncomment to detect people\n",
    "        # VisualFeatures.SMART_CROPS,   # Uncomment for smart crops\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n🔍 Analyzing your image...\")\n",
    "        print(f\"URL: {your_image_url}\")\n",
    "        \n",
    "        result = client.analyze_from_url(\n",
    "            image_url=your_image_url,\n",
    "            visual_features=selected_features,\n",
    "            gender_neutral_caption=True\n",
    "        )\n",
    "        \n",
    "        # Results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"🎉 YOUR ANALYSIS RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if result.caption:\n",
    "            print(f\"\\n📝 CAPTION:\")\n",
    "            print(f\"   '{result.caption.text}'\")\n",
    "            print(f\"   Confidence: {result.caption.confidence:.2%}\")\n",
    "        \n",
    "        if result.tags:\n",
    "            print(f\"\\n🏷️ IDENTIFIED TAGS:\")\n",
    "            for i, tag in enumerate(result.tags.list[:10], 1):\n",
    "                print(f\"   {i:2d}. {tag.name:<20} {tag.confidence:.2%}\")\n",
    "        \n",
    "        if result.objects:\n",
    "            print(f\"\\n📦 DETECTED OBJECTS:\")\n",
    "            for i, obj in enumerate(result.objects.list, 1):\n",
    "                print(f\"   {i}. {obj.tags[0].name} (Confidence: {obj.tags[0].confidence:.2%})\")\n",
    "        \n",
    "        if result.read:\n",
    "            print(f\"\\n📖 EXTRACTED TEXT:\")\n",
    "            for block in result.read.blocks:\n",
    "                for line in block.lines:\n",
    "                    print(f\"   '{line.text}'\")\n",
    "        \n",
    "        if result.people:\n",
    "            print(f\"\\n👥 DETECTED PEOPLE: {len(result.people.list)} person(s)\")\n",
    "        \n",
    "        if result.smart_crops:\n",
    "            print(f\"\\n✂️ SUGGESTED CROPS: {len(result.smart_crops.list)} option(s)\")\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error analyzing your image: {e}\")\n",
    "        print(\"Check if the image URL is correct and accessible.\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Configure AZURE_VISION_ENDPOINT and AZURE_VISION_KEY environment variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e4900a",
   "metadata": {},
   "source": [
    "#### 🎓 Next Steps\n",
    "\n",
    "**📚 Documentation and Resources:**\n",
    "- [Official Azure AI Vision Documentation](https://learn.microsoft.com/azure/ai-services/computer-vision/)\n",
    "- [Complete Guide to Image Analysis API 4.0](https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/call-analyze-image-40)\n",
    "- [Python SDK Reference](https://aka.ms/azsdk/image-analysis/ref-docs/python)\n",
    "- [Code Samples on GitHub](https://aka.ms/azsdk/image-analysis/samples/python)\n",
    "\n",
    "**🛠️ Also Try:**\n",
    "- [Azure AI Vision Studio](https://portal.vision.cognitive.azure.com/) - Visual interface to test features\n",
    "- [Azure AI Face Service](https://azure.microsoft.com/services/cognitive-services/face/) - Advanced facial analysis\n",
    "- [Azure AI Custom Vision](https://www.customvision.ai/) - Custom model training\n",
    "- [Azure AI Video Indexer](https://www.videoindexer.ai/) - Video analysis\n",
    "\n",
    "**💻 Suggested Practical Projects:**\n",
    "1. **Image Descriptor for Accessibility**: Create a website that generates automatic alt-text\n",
    "2. **Document Analyzer**: Extract text from invoices and documents\n",
    "3. **Content Moderator**: System to automatically classify images\n",
    "4. **Visual Assistant**: Mobile app that describes the surrounding environment\n",
    "\n",
    "**🔄 Integration with Other Services:**\n",
    "- Combine with Azure AI Language for sentiment analysis of extracted text\n",
    "- Use with Azure AI Translator for automatic translation of text in images\n",
    "- Integrate with Azure Storage for batch processing\n",
    "- Connect with Power BI for visual insights dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ed1c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - Document Intelligence\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Configure Document Intelligence client\n",
    "doc_intelligence_endpoint = os.getenv('DOC_INTELLIGENCE_ENDPOINT')\n",
    "doc_intelligence_key = os.getenv('DOC_INTELLIGENCE_KEY')\n",
    "\n",
    "if doc_intelligence_endpoint and doc_intelligence_key:\n",
    "    try:\n",
    "        print(\"\\n=== Document Intelligence ===\")\n",
    "        \n",
    "        # Create client\n",
    "        document_analysis_client = DocumentAnalysisClient(\n",
    "            endpoint=doc_intelligence_endpoint,\n",
    "            credential=AzureKeyCredential(doc_intelligence_key)\n",
    "        )\n",
    "        \n",
    "        # Analyze generic document\n",
    "        doc_path = \"../../../samples/placa.jpg\"\n",
    "        \n",
    "        with open(doc_path, \"rb\") as f:\n",
    "            # Use pre-built model for general layout\n",
    "            poller = document_analysis_client.begin_analyze_document(\n",
    "                \"prebuilt-layout\", document=f\n",
    "            )\n",
    "            \n",
    "        result = poller.result()\n",
    "        \n",
    "        print(\"Document Layout Analysis:\")\n",
    "        print(f\"Number of pages: {len(result.pages)}\")\n",
    "        \n",
    "        # Extract tables\n",
    "        if result.tables:\n",
    "            print(f\"\\nTables found: {len(result.tables)}\")\n",
    "            for idx, table in enumerate(result.tables):\n",
    "                print(f\"Table {idx + 1}: {table.row_count} rows x {table.column_count} columns\")\n",
    "                \n",
    "        # Extract paragraphs\n",
    "        if result.paragraphs:\n",
    "            print(f\"\\nParagraphs found: {len(result.paragraphs)}\")\n",
    "            for idx, paragraph in enumerate(result.paragraphs[:3]):  # Show only first 3\n",
    "                print(f\"Paragraph {idx + 1}: {paragraph.content[:100]}...\")\n",
    "                \n",
    "        # Extract key-value pairs\n",
    "        if result.key_value_pairs:\n",
    "            print(f\"\\nKey-value pairs found: {len(result.key_value_pairs)}\")\n",
    "            for kv_pair in result.key_value_pairs[:5]:  # Show only first 5\n",
    "                if kv_pair.key and kv_pair.value:\n",
    "                    print(f\"  {kv_pair.key.content}: {kv_pair.value.content}\")\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Document file not found: {doc_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Document Intelligence: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Define DOC_INTELLIGENCE_ENDPOINT and DOC_INTELLIGENCE_KEY variables in the .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9308a77",
   "metadata": {},
   "source": [
    "### Exercise 4 - Content Safety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efbb0a1",
   "metadata": {},
   "source": [
    "Azure AI Content Safety detects harmful user-generated and AI-generated content in applications and services. This service provides several different types of analysis.\n",
    "\n",
    "**Prompt Shields** - Examines text for risks of user input attacks on a Large Language Model.\n",
    "\n",
    "**Groundedness Detection (preview)** - Detects whether text responses from large language models (LLMs) are grounded in the source materials provided by users.\n",
    "\n",
    "**Protected Material Detection in Text** - Examines AI-generated text for known text content (e.g., song lyrics, articles, recipes, selected web content).\n",
    "\n",
    "**Custom Categories API (standard) (preview)** - Allows you to create and train your own custom content categories and examine text for matches.\n",
    "\n",
    "**Custom Categories API (rapid) (preview)** - Allows you to define emerging patterns of harmful content and examine text and images for matches.\n",
    "\n",
    "**Text Analysis API** - Examines text for sexual, violence, hate, and self-harm content with multiple severity levels.\n",
    "\n",
    "**Image Analysis API** - Examines images for sexual, violence, hate, and self-harm content with multiple severity levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741816c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-ai-contentsafety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da45af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - Content Safety (following official quickstart)\n",
    "from azure.ai.contentsafety import ContentSafetyClient\n",
    "from azure.ai.contentsafety.models import AnalyzeTextOptions, TextCategory\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "def analyze_text_content(text_to_analyze):\n",
    "    \"\"\"\n",
    "    Function to analyze text using Azure Content Safety\n",
    "    Based on Microsoft's official quickstart\n",
    "    \"\"\"\n",
    "    # Get credentials from environment variables\n",
    "    key = os.getenv('CONTENT_SAFETY_KEY')\n",
    "    endpoint = os.getenv('CONTENT_SAFETY_ENDPOINT')\n",
    "    \n",
    "    if not key or not endpoint:\n",
    "        print(\"❌ Error: Define CONTENT_SAFETY_KEY and CONTENT_SAFETY_ENDPOINT variables in the .env file\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Create Azure AI Content Safety client\n",
    "        client = ContentSafetyClient(endpoint, AzureKeyCredential(key))\n",
    "        \n",
    "        # Configure request\n",
    "        request = AnalyzeTextOptions(text=text_to_analyze)\n",
    "        \n",
    "        # Analyze text\n",
    "        response = client.analyze_text(request)\n",
    "        \n",
    "        # Extract results by specific category (following quickstart)\n",
    "        hate_result = next((item for item in response.categories_analysis if item.category == TextCategory.HATE), None)\n",
    "        self_harm_result = next((item for item in response.categories_analysis if item.category == TextCategory.SELF_HARM), None)\n",
    "        sexual_result = next((item for item in response.categories_analysis if item.category == TextCategory.SEXUAL), None)\n",
    "        violence_result = next((item for item in response.categories_analysis if item.category == TextCategory.VIOLENCE), None)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"📝 Analyzed text: '{text_to_analyze}'\")\n",
    "        print(\"🔍 Analysis results:\")\n",
    "        \n",
    "        if hate_result:\n",
    "            print(f\"  💬 Hate: Severity {hate_result.severity}\")\n",
    "        if self_harm_result:\n",
    "            print(f\"  🩹 Self-harm: Severity {self_harm_result.severity}\")\n",
    "        if sexual_result:\n",
    "            print(f\"  🔞 Sexual: Severity {sexual_result.severity}\")\n",
    "        if violence_result:\n",
    "            print(f\"  ⚔️ Violence: Severity {violence_result.severity}\")\n",
    "        \n",
    "        # Interpret overall risk level\n",
    "        max_severity = max([\n",
    "            hate_result.severity if hate_result else 0,\n",
    "            self_harm_result.severity if self_harm_result else 0,\n",
    "            sexual_result.severity if sexual_result else 0,\n",
    "            violence_result.severity if violence_result else 0\n",
    "        ])\n",
    "        \n",
    "        if max_severity == 0:\n",
    "            risk_level = \"✅ Safe\"\n",
    "        elif max_severity <= 2:\n",
    "            risk_level = \"⚠️ Low risk\"\n",
    "        elif max_severity <= 4:\n",
    "            risk_level = \"🔸 Moderate risk\"\n",
    "        else:\n",
    "            risk_level = \"🔴 High risk\"\n",
    "            \n",
    "        print(f\"📊 Overall assessment: {risk_level}\")\n",
    "        return response\n",
    "        \n",
    "    except HttpResponseError as e:\n",
    "        print(\"❌ Text analysis failed.\")\n",
    "        if e.error:\n",
    "            print(f\"Error code: {e.error.code}\")\n",
    "            print(f\"Error message: {e.error.message}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Configure Content Safety client\n",
    "content_safety_endpoint = os.getenv('CONTENT_SAFETY_ENDPOINT')\n",
    "content_safety_key = os.getenv('CONTENT_SAFETY_KEY')\n",
    "\n",
    "if content_safety_endpoint and content_safety_key:\n",
    "    print(\"=== Content Safety - Text Analysis ===\")\n",
    "    print(\"Based on Microsoft's official quickstart\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Sample texts for analysis (including different risk levels)\n",
    "    test_texts = [\n",
    "        \"Hello! How are you today? Have a great day!\",\n",
    "        \"This is a neutral text about technology and Python programming.\",\n",
    "        \"I'm very angry about this situation, but I'll resolve it in a civilized manner.\",\n",
    "        \"Test text for potentially problematic content moderation.\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"📋 Analyzing {len(test_texts)} sample texts...\\n\")\n",
    "    \n",
    "    for idx, text in enumerate(test_texts, 1):\n",
    "        print(f\"🔍 Analysis {idx}/{len(test_texts)}:\")\n",
    "        result = analyze_text_content(text)\n",
    "        \n",
    "        if result:\n",
    "            print(\"✅ Analysis completed successfully\")\n",
    "        else:\n",
    "            print(\"❌ Analysis failed\")\n",
    "            \n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Required configuration:\")\n",
    "    print(\"Define CONTENT_SAFETY_ENDPOINT and CONTENT_SAFETY_KEY variables in the .env file\")\n",
    "    print(\"\\nConfiguration example:\")\n",
    "    print(\"CONTENT_SAFETY_ENDPOINT=https://your-content-safety.cognitiveservices.azure.com/\")\n",
    "    print(\"CONTENT_SAFETY_KEY=your-content-safety-key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3767bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - Content Safety for Images (following official quickstart)\n",
    "from azure.ai.contentsafety.models import AnalyzeImageOptions, ImageData\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "def analyze_image_content(image_path):\n",
    "    \"\"\"\n",
    "    Function to analyze image using Azure Content Safety\n",
    "    Based on Microsoft's official quickstart\n",
    "    \"\"\"\n",
    "    # Get credentials from environment variables\n",
    "    key = os.getenv('CONTENT_SAFETY_KEY')\n",
    "    endpoint = os.getenv('CONTENT_SAFETY_ENDPOINT')\n",
    "    \n",
    "    if not key or not endpoint:\n",
    "        print(\"❌ Error: Define CONTENT_SAFETY_KEY and CONTENT_SAFETY_ENDPOINT variables in the .env file\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Create Azure AI Content Safety client\n",
    "        client = ContentSafetyClient(endpoint, AzureKeyCredential(key))\n",
    "        \n",
    "        # Read image\n",
    "        with open(image_path, \"rb\") as file:\n",
    "            image_data = file.read()\n",
    "            \n",
    "        # Configure request for image analysis\n",
    "        request = AnalyzeImageOptions(image=ImageData(content=image_data))\n",
    "        \n",
    "        # Analyze image\n",
    "        response = client.analyze_image(request)\n",
    "        \n",
    "        # Extract results by specific category\n",
    "        hate_result = next((item for item in response.categories_analysis if item.category == TextCategory.HATE), None)\n",
    "        self_harm_result = next((item for item in response.categories_analysis if item.category == TextCategory.SELF_HARM), None)\n",
    "        sexual_result = next((item for item in response.categories_analysis if item.category == TextCategory.SEXUAL), None)\n",
    "        violence_result = next((item for item in response.categories_analysis if item.category == TextCategory.VIOLENCE), None)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"🖼️ Analyzed image: {image_path}\")\n",
    "        print(\"🔍 Analysis results:\")\n",
    "        \n",
    "        if hate_result:\n",
    "            print(f\"  💬 Hate: Severity {hate_result.severity}\")\n",
    "        if self_harm_result:\n",
    "            print(f\"  🩹 Self-harm: Severity {self_harm_result.severity}\")\n",
    "        if sexual_result:\n",
    "            print(f\"  🔞 Sexual: Severity {sexual_result.severity}\")\n",
    "        if violence_result:\n",
    "            print(f\"  ⚔️ Violence: Severity {violence_result.severity}\")\n",
    "        \n",
    "        # Interpret overall risk level\n",
    "        max_severity = max([\n",
    "            hate_result.severity if hate_result else 0,\n",
    "            self_harm_result.severity if self_harm_result else 0,\n",
    "            sexual_result.severity if sexual_result else 0,\n",
    "            violence_result.severity if violence_result else 0\n",
    "        ])\n",
    "        \n",
    "        if max_severity == 0:\n",
    "            risk_level = \"✅ Safe\"\n",
    "        elif max_severity <= 2:\n",
    "            risk_level = \"⚠️ Low risk\"\n",
    "        elif max_severity <= 4:\n",
    "            risk_level = \"🔸 Moderate risk\"\n",
    "        else:\n",
    "            risk_level = \"🔴 High risk\"\n",
    "            \n",
    "        print(f\"📊 Overall assessment: {risk_level}\")\n",
    "        return response\n",
    "        \n",
    "    except HttpResponseError as e:\n",
    "        print(\"❌ Image analysis failed.\")\n",
    "        if e.error:\n",
    "            print(f\"Error code: {e.error.code}\")\n",
    "            print(f\"Error message: {e.error.message}\")\n",
    "        return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: Image file not found: {image_path}\")\n",
    "        print(\"Check if the image path is correct.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "if content_safety_endpoint and content_safety_key:\n",
    "    print(\"\\n=== Content Safety - Image Analysis ===\")\n",
    "    print(\"Based on Microsoft's official quickstart\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Paths to sample images\n",
    "    image_paths = [\n",
    "        \"../../../samples/234039841.jpg\",\n",
    "        \"../../../samples/car-accident.png\"\n",
    "    ]\n",
    "    \n",
    "    for idx, image_path in enumerate(image_paths, 1):\n",
    "        print(f\"\\n🔍 Analysis {idx}/{len(image_paths)}:\")\n",
    "        result = analyze_image_content(image_path)\n",
    "        \n",
    "        if result:\n",
    "            print(\"✅ Analysis completed successfully\")\n",
    "        else:\n",
    "            print(\"❌ Analysis failed\")\n",
    "            \n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Required configuration:\")\n",
    "    print(\"Define CONTENT_SAFETY_ENDPOINT and CONTENT_SAFETY_KEY variables in the .env file\")\n",
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "speech_key = os.getenv('SPEECH_KEY')\n",
    "speech_region = os.getenv('SPEECH_REGION')\n",
    "audio_file = '../../../samples/audio001.wav'\n",
    "if speech_key and speech_region:\n",
    "    try:\n",
    "        speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "        speech_config.speech_recognition_language = \"en-US\"\n",
    "        audio_config = speechsdk.audio.AudioConfig(filename=audio_file)\n",
    "        speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "        recognized_texts = []\n",
    "        def recognized_cb(evt):\n",
    "            if evt.result.text:\n",
    "                print('Recognized:', evt.result.text)\n",
    "                recognized_texts.append(evt.result.text)\n",
    "\n",
    "        speech_recognizer.recognized.connect(recognized_cb)\n",
    "\n",
    "        print(\"Starting continuous recognition...\")\n",
    "        speech_recognizer.start_continuous_recognition()\n",
    "\n",
    "        # Wait for recognition to finish (adjust time according to audio size)\n",
    "        time.sleep(10)\n",
    "        speech_recognizer.stop_continuous_recognition()\n",
    "\n",
    "        print(\"Recognition completed. Full text:\")\n",
    "        print(' '.join(recognized_texts))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in continuous recognition: {e}\")\n",
    "else:\n",
    "    print(\"Please configure the SPEECH_KEY and SPEECH_REGION environment variables.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
