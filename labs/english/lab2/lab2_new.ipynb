{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2671ab88",
   "metadata": {},
   "source": [
    "# Azure AI Foundry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814866ca",
   "metadata": {},
   "source": [
    "<center><img src=\"../../../images/Azure-AI-Foundry_1600x900.jpg\" alt=\"Azure AI Foundry\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9822b7",
   "metadata": {},
   "source": [
    "## Laboratory 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050dd053",
   "metadata": {},
   "source": [
    "In this laboratory we will explore the AI services present in Azure Foundry. This laboratory will cover the following services:\n",
    "- Speech\n",
    "- Language + Translator\n",
    "- Vision + Document \n",
    "- Content Safety\n",
    "\n",
    "Understanding these services allows us to add more capabilities to our applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1ea1b3",
   "metadata": {},
   "source": [
    "### Exercise 1 - Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c037dd6e",
   "metadata": {},
   "source": [
    "The Speech service provides speech-to-text and text-to-speech conversion capabilities with a Speech resource. You can transcribe speech to text with high accuracy, produce natural-sounding voices from text to speech, translate spoken audio, and use speaker recognition during conversations. Create custom voices, add specific words to your base vocabulary, or build your own models. Run Speech anywhere, in the cloud or at the edge in containers. It's easy to enable speech in your applications, tools, and devices with the Speech CLI, Speech SDK, and REST APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29741010",
   "metadata": {},
   "source": [
    "Common scenarios for using speech:\n",
    "\n",
    "**Caption Generation:** Learn how to synchronize captions with your input audio, apply profanity filters, get partial results, apply customizations, and identify spoken languages for multilingual scenarios.\n",
    "\n",
    "**Audio Content Creation:** You can use neural voices to make interactions with chatbots and voice assistants more natural and engaging, convert digital texts like e-books into audiobooks, and enhance automotive navigation systems.\n",
    "\n",
    "**Call Center:** Transcribe calls in real-time or process a batch of calls, remove personally identifiable information, and extract insights like sentiment analysis to assist with your call center use case.\n",
    "\n",
    "**Language Learning:** Provide pronunciation assessment feedback for language learners, offer real-time transcription support for remote learning conversations, and read instructional materials aloud using neural voices.\n",
    "\n",
    "**Voice Assistants:** Create natural, human-like conversational interfaces for your applications and experiences. The voice assistant feature offers fast and reliable interaction between a device and an assistant implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340b30e3",
   "metadata": {},
   "source": [
    "To perform this exercise, make sure your `.env` file has the following variables filled:\n",
    "- SPEECH_ENDPOINT \n",
    "- SPEECH_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e629640a",
   "metadata": {},
   "source": [
    "After verifying, let's start by loading the necessary libraries, initializing the client, and making a call to convert audio to text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04287664",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-cognitiveservices-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd71981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous speech recognition to process all audio, even with initial silence\n",
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "speech_key = os.getenv('SPEECH_KEY')\n",
    "speech_region = os.getenv('SPEECH_REGION')\n",
    "audio_file = '../../../samples/audio001.wav'\n",
    "if speech_key and speech_region:\n",
    "    try:\n",
    "        speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "        speech_config.speech_recognition_language = \"en-US\"\n",
    "        audio_config = speechsdk.audio.AudioConfig(filename=audio_file)\n",
    "        speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "        recognized_texts = []\n",
    "        def recognized_cb(evt):\n",
    "            if evt.result.text:\n",
    "                print('Recognized:', evt.result.text)\n",
    "                recognized_texts.append(evt.result.text)\n",
    "\n",
    "        speech_recognizer.recognized.connect(recognized_cb)\n",
    "\n",
    "        print(\"Starting continuous recognition...\")\n",
    "        speech_recognizer.start_continuous_recognition()\n",
    "\n",
    "        # Wait for recognition to finish (adjust time according to audio size)\n",
    "        time.sleep(10)\n",
    "        speech_recognizer.stop_continuous_recognition()\n",
    "\n",
    "        print(\"Recognition finished. Complete text:\")\n",
    "        print(' '.join(recognized_texts))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in continuous recognition: {e}\")\n",
    "else:\n",
    "    print(\"Please configure the SPEECH_KEY and SPEECH_REGION environment variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a5895",
   "metadata": {},
   "source": [
    "### Exercise 2 - Language + Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5906f995",
   "metadata": {},
   "source": [
    "Integrate natural language into applications, bots, and IoT devices. For example, this service can remove sensitive data, segment long meetings into chapters, analyze health records, and orchestrate conversational bots with your custom intents using factual responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fdde30",
   "metadata": {},
   "source": [
    "This Language service unifies the following Azure AI services previously available separately: Text Analytics, QnA Maker, and LUIS.\n",
    "\n",
    "Azure AI Foundry allows you to use most of the following service features without needing to write code.\n",
    "\n",
    "**Named Entity Recognition (NER)** - Named entity recognition identifies different entries in text and categorizes them into predefined types.\n",
    "\n",
    "**Personal and health information detection** - PII detection identifies entities in text and conversations (chat or transcriptions) that are associated with individuals.\n",
    "\n",
    "**Language detection** - Language detection evaluates text and detects a wide range of languages and variant dialects.\n",
    "\n",
    "**Sentiment analysis and opinion mining** - Sentiment analysis and opinion mining are pre-configured features that help you understand public perception of your brand or topic. These features analyze text to identify positive or negative sentiment and can link them to specific elements within the text.\n",
    "\n",
    "**Summarization** - Summarization condenses information for text and conversations (chat and transcriptions). Text summarization generates a summary, supporting two approaches: Extractive summarization creates a summary by selecting key phrases from the document and preserving their original positions. In contrast, abstractive summarization generates a summary by producing new, concise, and coherent sentences or phrases that are not directly copied from the original document. Conversation summarization recaps and segments long meetings into chapters with timestamps. Call center summarization summarizes customer problems and their resolutions.\n",
    "\n",
    "**Key phrase extraction** - Key phrase extraction is a pre-configured feature that evaluates and returns key concepts in unstructured text, returning them as a list.\n",
    "\n",
    "**Entity linking** - Entity linking is a pre-configured feature that disambiguates the identity of entities (words or phrases) found in unstructured text and returns links to Wikipedia.\n",
    "\n",
    "**Text analytics for health** - Text analytics for health extracts and labels relevant health information from unstructured text.\n",
    "\n",
    "**Custom text classification** - Custom text classification allows you to build custom AI models to classify unstructured text documents into custom classes you define.\n",
    "\n",
    "**Custom Named Entity Recognition (Custom NER)** - Custom NER allows you to build custom AI models to extract custom entity categories (labels for words or phrases), using unstructured text you provide.\n",
    "\n",
    "**Conversational language understanding** - Conversational language understanding (CLU) allows users to build custom natural language understanding models to predict the overall intent of an incoming utterance and extract important information from it.\n",
    "\n",
    "**Orchestration workflow** - Orchestration workflow is a custom feature that allows you to connect Conversational Language Understanding (CLU), question answering, and LUIS applications.\n",
    "\n",
    "**Question answering** - Question answering is a custom feature that identifies the most appropriate answer for user inputs. This feature is typically used to develop conversational client applications, including social media platforms, chat bots, and voice-enabled desktop applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5016f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure.ai.textanalytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198fff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - Language Service\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Configure Language client\n",
    "language_endpoint = os.getenv('AZURE_LANGUAGE_ENDPOINT')\n",
    "language_key = os.getenv('AZURE_LANGUAGE_KEY')\n",
    "\n",
    "if language_endpoint and language_key:\n",
    "    # Create client\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=language_endpoint,\n",
    "        credential=AzureKeyCredential(language_key)\n",
    "    )\n",
    "    \n",
    "    # Sample text\n",
    "    documents = [\n",
    "        \"I love this product! It's amazing and works perfectly.\",\n",
    "        \"This service is terrible, it didn't work as expected.\",\n",
    "        \"The service was adequate, nothing exceptional.\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Sentiment analysis\n",
    "        response = text_analytics_client.analyze_sentiment(documents=documents, language=\"en\")\n",
    "        \n",
    "        print(\"=== Sentiment Analysis ===\")\n",
    "        for idx, doc in enumerate(response):\n",
    "            if not doc.is_error:\n",
    "                print(f\"Document {idx + 1}:\")\n",
    "                print(f\"  Text: {documents[idx]}\")\n",
    "                print(f\"  Sentiment: {doc.sentiment}\")\n",
    "                print(f\"  Confidence: Positive={doc.confidence_scores.positive:.2f}, \"\n",
    "                      f\"Neutral={doc.confidence_scores.neutral:.2f}, \"\n",
    "                      f\"Negative={doc.confidence_scores.negative:.2f}\")\n",
    "                print()\n",
    "            else:\n",
    "                print(f\"Error in document {idx + 1}: {doc.error}\")\n",
    "                \n",
    "        # Key phrase extraction\n",
    "        key_phrases_response = text_analytics_client.extract_key_phrases(documents=documents, language=\"en\")\n",
    "        \n",
    "        print(\"=== Key Phrase Extraction ===\")\n",
    "        for idx, doc in enumerate(key_phrases_response):\n",
    "            if not doc.is_error:\n",
    "                print(f\"Document {idx + 1}:\")\n",
    "                print(f\"  Key phrases: {', '.join(doc.key_phrases)}\")\n",
    "                print()\n",
    "            else:\n",
    "                print(f\"Error in document {idx + 1}: {doc.error}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"Define the LANGUAGE_ENDPOINT and LANGUAGE_KEY variables in the .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418477ed",
   "metadata": {},
   "source": [
    "### Exercise 3 - Vision + Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafd3425",
   "metadata": {},
   "source": [
    "Give your applications the ability to read text, analyze images, process documents, and detect faces with technologies like optical character recognition (OCR) and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50a592c",
   "metadata": {},
   "source": [
    "The Azure AI Vision service provides access to advanced algorithms that process images and return information based on the visual features you're interested in. The following table lists the main product categories.\n",
    "\n",
    "**Optical Character Recognition (OCR)** - The Optical Character Recognition (OCR) service extracts text from images. You can use the Read API to extract printed and handwritten text from photos and documents. It uses deep learning-based models and works with text on various surfaces and backgrounds. This includes business documents, invoices, receipts, posters, business cards, letters, and whiteboards. The OCR APIs support extracting printed text in several languages.\n",
    "\n",
    "**Image Analysis** - The Image Analysis service extracts many visual features from images, such as objects, faces, adult content, and automatically generated text descriptions.\n",
    "\n",
    "**Face** - The Face service provides AI algorithms that detect, recognize, and analyze human faces in images. Facial recognition software is important in many different scenarios, such as identification, touchless access control, and face blurring for privacy.\n",
    "\n",
    "**Video Retrieval** - Video Retrieval allows you to create a video index that you can search with natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c7e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Configure Vision client\n",
    "vision_endpoint = os.getenv('AZURE_VISION_ENDPOINT')\n",
    "vision_key = os.getenv('AZURE_VISION_KEY')\n",
    "\n",
    "if vision_endpoint and vision_key:\n",
    "    try:\n",
    "        print(\"=== Azure AI Vision - Image Analysis ===\")\n",
    "        \n",
    "        # Create client\n",
    "        client = ImageAnalysisClient(\n",
    "            endpoint=vision_endpoint,\n",
    "            credential=AzureKeyCredential(vision_key)\n",
    "        )\n",
    "        \n",
    "        # Analyze remote image\n",
    "        print(\"\\n1. Analyzing remote image...\")\n",
    "        result = client.analyze_from_url(\n",
    "            image_url=\"https://learn.microsoft.com/azure/ai-services/computer-vision/media/quickstarts/presentation.png\",\n",
    "            visual_features=[VisualFeatures.CAPTION, VisualFeatures.READ],\n",
    "            gender_neutral_caption=True,  # Optional (default is False)\n",
    "        )\n",
    "        \n",
    "        print(\"Analysis results:\")\n",
    "        \n",
    "        # Display caption results\n",
    "        print(\"\\nüìù Caption:\")\n",
    "        if result.caption is not None:\n",
    "            print(f\"   '{result.caption.text}', Confidence: {result.caption.confidence:.4f}\")\n",
    "        else:\n",
    "            print(\"   No caption found\")\n",
    "\n",
    "        # Display OCR results (extracted text)\n",
    "        print(\"\\nüìñ Extracted text (OCR):\")\n",
    "        if result.read is not None:\n",
    "            for block in result.read.blocks:\n",
    "                for line in block.lines:\n",
    "                    print(f\"   Line: '{line.text}', Bounding box: {line.bounding_polygon}\")\n",
    "                    for word in line.words:\n",
    "                        print(f\"     Word: '{word.text}', Confidence: {word.confidence:.4f}\")\n",
    "        else:\n",
    "            print(\"   No text found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in image analysis: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Define the AZURE_VISION_ENDPOINT and AZURE_VISION_KEY variables in the .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a2eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced example - Complete local image analysis\n",
    "if vision_endpoint and vision_key:\n",
    "    try:\n",
    "        print(\"\\n=== Advanced Image Analysis ===\")\n",
    "        \n",
    "        # Analyze local image\n",
    "        image_path = \"../../../samples/car-accident.png\"\n",
    "        \n",
    "        print(f\"\\n2. Analyzing local image: {image_path}\")\n",
    "        \n",
    "        # Open local image\n",
    "        with open(image_path, \"rb\") as image_data:\n",
    "            # Complete analysis with multiple functionalities\n",
    "            result = client.analyze(\n",
    "                image_data=image_data.read(),\n",
    "                visual_features=[\n",
    "                    VisualFeatures.CAPTION,\n",
    "                    VisualFeatures.READ,\n",
    "                    VisualFeatures.TAGS,\n",
    "                    VisualFeatures.OBJECTS,\n",
    "                    VisualFeatures.PEOPLE,\n",
    "                    VisualFeatures.SMART_CROPS\n",
    "                ],\n",
    "                gender_neutral_caption=True\n",
    "            )\n",
    "            \n",
    "            print(\"\\nüìù Caption:\")\n",
    "            if result.caption:\n",
    "                print(f\"   '{result.caption.text}', Confidence: {result.caption.confidence:.4f}\")\n",
    "            \n",
    "            print(\"\\nüè∑Ô∏è  Identified tags:\")\n",
    "            if result.tags:\n",
    "                for tag in result.tags.list:\n",
    "                    print(f\"   - {tag.name}: {tag.confidence:.4f}\")\n",
    "            \n",
    "            print(\"\\nüì¶ Detected objects:\")\n",
    "            if result.objects:\n",
    "                for obj in result.objects.list:\n",
    "                    print(f\"   - {obj.tags[0].name}: {obj.tags[0].confidence:.4f}\")\n",
    "                    print(f\"     Location: {obj.bounding_box}\")\n",
    "            \n",
    "            print(\"\\nüë• Detected people:\")\n",
    "            if result.people:\n",
    "                for person in result.people.list:\n",
    "                    print(f\"   - Person detected with confidence: {person.confidence:.4f}\")\n",
    "                    print(f\"     Location: {person.bounding_box}\")\n",
    "            \n",
    "            print(\"\\n‚úÇÔ∏è  Smart crops:\")\n",
    "            if result.smart_crops:\n",
    "                for crop in result.smart_crops.list:\n",
    "                    print(f\"   - Aspect {crop.aspect_ratio}: {crop.bounding_box}\")\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image file not found: {image_path}\")\n",
    "        print(\"Make sure the file exists at the specified path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in advanced analysis: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a67bcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - Analysis of different types of images\n",
    "if vision_endpoint and vision_key:\n",
    "    try:\n",
    "        print(\"\\n=== Comparison of Different Image Analyses ===\")\n",
    "        \n",
    "        # List of images to analyze\n",
    "        images_to_analyze = [\n",
    "            {\n",
    "                \"url\": \"https://learn.microsoft.com/azure/ai-services/computer-vision/media/quickstarts/presentation.png\",\n",
    "                \"description\": \"Calendar presentation\"\n",
    "            },\n",
    "            {\n",
    "                \"url\": \"https://images.unsplash.com/photo-1518791841217-8f162f1e1131?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1000&q=80\",\n",
    "                \"description\": \"Cat\"\n",
    "            },\n",
    "            {\n",
    "                \"url\": \"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1000&q=80\",\n",
    "                \"description\": \"Mountain landscape\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for i, image_info in enumerate(images_to_analyze, 1):\n",
    "            print(f\"\\n{i}. Analyzing: {image_info['description']}\")\n",
    "            print(f\"   URL: {image_info['url']}\")\n",
    "            \n",
    "            try:\n",
    "                # Analysis focused on caption and tags\n",
    "                result = client.analyze_from_url(\n",
    "                    image_url=image_info[\"url\"],\n",
    "                    visual_features=[VisualFeatures.CAPTION, VisualFeatures.TAGS],\n",
    "                    gender_neutral_caption=True\n",
    "                )\n",
    "                \n",
    "                if result.caption:\n",
    "                    print(f\"   üìù Caption: '{result.caption.text}' (Confidence: {result.caption.confidence:.4f})\")\n",
    "                \n",
    "                if result.tags:\n",
    "                    print(\"   üè∑Ô∏è  Top 5 tags:\")\n",
    "                    for tag in result.tags.list[:5]:\n",
    "                        print(f\"      - {tag.name}: {tag.confidence:.4f}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error analyzing image {i}: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"General error in comparative analysis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ffac0",
   "metadata": {},
   "source": [
    "#### Azure AI Vision Features\n",
    "\n",
    "Azure AI Vision offers various functionalities through the `azure-ai-vision-imageanalysis` SDK:\n",
    "\n",
    "**üìù Main Features:**\n",
    "- **CAPTION**: Generates descriptive captions for images\n",
    "- **READ**: Extracts text from images (OCR)\n",
    "- **TAGS**: Identifies objects, concepts, and actions in the image\n",
    "- **OBJECTS**: Detects and locates specific objects\n",
    "- **PEOPLE**: Identifies people in the image\n",
    "- **SMART_CROPS**: Suggests intelligent crops of the image\n",
    "- **FACES**: Detects and analyzes faces (separate functionality)\n",
    "\n",
    "**üìä Analysis Methods:**\n",
    "- `analyze_from_url()`: Analyzes image from a URL\n",
    "- `analyze()`: Analyzes image from binary data (local file)\n",
    "\n",
    "**‚öôÔ∏è Important Parameters:**\n",
    "- `visual_features`: List of features to be analyzed\n",
    "- `gender_neutral_caption`: Generates gender-neutral captions\n",
    "- `language`: Language for results (default: English)\n",
    "- `smart_crops_aspect_ratios`: Aspect ratios for smart crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f4412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Practical Exercise - Test your own images\n",
    "if vision_endpoint and vision_key:\n",
    "    print(\"=== Practical Exercise ===\")\n",
    "    print(\"\\nüìù Instructions:\")\n",
    "    print(\"1. Replace the URL below with an image of your choice\")\n",
    "    print(\"2. Choose the features you want to test\")\n",
    "    print(\"3. Run the cell and analyze the results\")\n",
    "    \n",
    "    # üéØ MODIFY HERE: Paste an image URL to test\n",
    "    your_image_url = \"https://images.unsplash.com/photo-1544947950-fa07a98d237f?ixlib=rb-4.0.3&auto=format&fit=crop&w=800&q=80\"\n",
    "    \n",
    "    # üéØ MODIFY HERE: Choose the features you want to test\n",
    "    selected_features = [\n",
    "        VisualFeatures.CAPTION,\n",
    "        VisualFeatures.TAGS,\n",
    "        VisualFeatures.OBJECTS,\n",
    "        # VisualFeatures.READ,          # Uncomment for OCR\n",
    "        # VisualFeatures.PEOPLE,        # Uncomment to detect people\n",
    "        # VisualFeatures.SMART_CROPS,   # Uncomment for smart crops\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nüîç Analyzing your image...\")\n",
    "        print(f\"URL: {your_image_url}\")\n",
    "        \n",
    "        result = client.analyze_from_url(\n",
    "            image_url=your_image_url,\n",
    "            visual_features=selected_features,\n",
    "            gender_neutral_caption=True\n",
    "        )\n",
    "        \n",
    "        # Results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"üéâ YOUR ANALYSIS RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if result.caption:\n",
    "            print(f\"\\nüìù CAPTION:\")\n",
    "            print(f\"   '{result.caption.text}'\")\n",
    "            print(f\"   Confidence: {result.caption.confidence:.2%}\")\n",
    "        \n",
    "        if result.tags:\n",
    "            print(f\"\\nüè∑Ô∏è  IDENTIFIED TAGS:\")\n",
    "            for i, tag in enumerate(result.tags.list[:10], 1):\n",
    "                print(f\"   {i:2d}. {tag.name:<20} {tag.confidence:.2%}\")\n",
    "        \n",
    "        if result.objects:\n",
    "            print(f\"\\nüì¶ DETECTED OBJECTS:\")\n",
    "            for i, obj in enumerate(result.objects.list, 1):\n",
    "                print(f\"   {i}. {obj.tags[0].name} (Confidence: {obj.tags[0].confidence:.2%})\")\n",
    "        \n",
    "        if result.read:\n",
    "            print(f\"\\nüìñ EXTRACTED TEXT:\")\n",
    "            for block in result.read.blocks:\n",
    "                for line in block.lines:\n",
    "                    print(f\"   '{line.text}'\")\n",
    "        \n",
    "        if result.people:\n",
    "            print(f\"\\nüë• DETECTED PEOPLE: {len(result.people.list)} person(s)\")\n",
    "        \n",
    "        if result.smart_crops:\n",
    "            print(f\"\\n‚úÇÔ∏è  SUGGESTED CROPS: {len(result.smart_crops.list)} option(s)\")\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error analyzing your image: {e}\")\n",
    "        print(\"Check if the image URL is correct and accessible.\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Configure the AZURE_VISION_ENDPOINT and AZURE_VISION_KEY environment variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6a6114",
   "metadata": {},
   "source": [
    "#### üéì Next Steps\n",
    "\n",
    "**üìö Documentation and Resources:**\n",
    "- [Official Azure AI Vision Documentation](https://learn.microsoft.com/azure/ai-services/computer-vision/)\n",
    "- [Complete Guide to Image Analysis API 4.0](https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/call-analyze-image-40)\n",
    "- [Python SDK Reference](https://aka.ms/azsdk/image-analysis/ref-docs/python)\n",
    "- [Code Samples on GitHub](https://aka.ms/azsdk/image-analysis/samples/python)\n",
    "\n",
    "**üõ†Ô∏è Also Try:**\n",
    "- [Azure AI Vision Studio](https://portal.vision.cognitive.azure.com/) - Visual interface to test features\n",
    "- [Azure AI Face Service](https://azure.microsoft.com/services/cognitive-services/face/) - Advanced facial analysis\n",
    "- [Azure AI Custom Vision](https://www.customvision.ai/) - Custom model training\n",
    "- [Azure AI Video Indexer](https://www.videoindexer.ai/) - Video analysis\n",
    "\n",
    "**üíª Suggested Practical Projects:**\n",
    "1. **Image Descriptor for Accessibility**: Create a website that generates automatic alt-text\n",
    "2. **Document Analyzer**: Extract text from invoices and documents\n",
    "3. **Content Moderator**: System to automatically classify images\n",
    "4. **Visual Assistant**: Mobile app that describes the surrounding environment\n",
    "\n",
    "**üîÑ Integration with Other Services:**\n",
    "- Combine with Azure AI Language for sentiment analysis of extracted text\n",
    "- Use with Azure AI Translator for automatic translation of text in images\n",
    "- Integrate with Azure Storage for batch processing\n",
    "- Connect with Power BI for visual insights dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8370bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - Document Intelligence\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Configure Document Intelligence client\n",
    "doc_intelligence_endpoint = os.getenv('DOC_INTELLIGENCE_ENDPOINT')\n",
    "doc_intelligence_key = os.getenv('DOC_INTELLIGENCE_KEY')\n",
    "\n",
    "if doc_intelligence_endpoint and doc_intelligence_key:\n",
    "    try:\n",
    "        print(\"\\n=== Document Intelligence ===\")\n",
    "        \n",
    "        # Create client\n",
    "        document_analysis_client = DocumentAnalysisClient(\n",
    "            endpoint=doc_intelligence_endpoint,\n",
    "            credential=AzureKeyCredential(doc_intelligence_key)\n",
    "        )\n",
    "        \n",
    "        # Analyze generic document\n",
    "        doc_path = \"../../../samples/placa.jpg\"\n",
    "        \n",
    "        with open(doc_path, \"rb\") as f:\n",
    "            # Use pre-built model for general layout\n",
    "            poller = document_analysis_client.begin_analyze_document(\n",
    "                \"prebuilt-layout\", document=f\n",
    "            )\n",
    "            \n",
    "        result = poller.result()\n",
    "        \n",
    "        print(\"Document Layout Analysis:\")\n",
    "        print(f\"Number of pages: {len(result.pages)}\")\n",
    "        \n",
    "        # Extract tables\n",
    "        if result.tables:\n",
    "            print(f\"\\nTables found: {len(result.tables)}\")\n",
    "            for idx, table in enumerate(result.tables):\n",
    "                print(f\"Table {idx + 1}: {table.row_count} rows x {table.column_count} columns\")\n",
    "                \n",
    "        # Extract paragraphs\n",
    "        if result.paragraphs:\n",
    "            print(f\"\\nParagraphs found: {len(result.paragraphs)}\")\n",
    "            for idx, paragraph in enumerate(result.paragraphs[:3]):  # Show only first 3\n",
    "                print(f\"Paragraph {idx + 1}: {paragraph.content[:100]}...\")\n",
    "                \n",
    "        # Extract key-value pairs\n",
    "        if result.key_value_pairs:\n",
    "            print(f\"\\nKey-value pairs found: {len(result.key_value_pairs)}\")\n",
    "            for kv_pair in result.key_value_pairs[:5]:  # Show only first 5\n",
    "                if kv_pair.key and kv_pair.value:\n",
    "                    print(f\"  {kv_pair.key.content}: {kv_pair.value.content}\")\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Document file not found: {doc_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in Document Intelligence: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Define the DOC_INTELLIGENCE_ENDPOINT and DOC_INTELLIGENCE_KEY variables in the .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c116406b",
   "metadata": {},
   "source": [
    "### Exercise 4 - Content Safety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4910dfe0",
   "metadata": {},
   "source": [
    "Azure AI Content Safety detects harmful user-generated and AI-generated content in applications and services. This service provides various types of analysis.\n",
    "\n",
    "**Prompt Shields** - Examines text for risks of user input attacks on a Large Language Model.\n",
    "\n",
    "**Groundedness detection (preview)** - Detects whether text responses from large language models (LLMs) are grounded in the source materials provided by users.\n",
    "\n",
    "**Protected material detection in text** - Examines AI-generated text for known text content (for example, song lyrics, articles, recipes, selected web content).\n",
    "\n",
    "**Custom categories API (standard) (preview)** - Allows you to create and train your own custom content categories and examine text for matches.\n",
    "\n",
    "**Custom categories API (rapid) (preview)** - Allows you to define emerging patterns of harmful content and examine text and images for matches.\n",
    "\n",
    "**Text analysis API** - Examines text for sexual, violence, hate, and self-harm content with multiple severity levels.\n",
    "\n",
    "**Image analysis API** - Examines images for sexual, violence, hate, and self-harm content with multiple severity levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7567647c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - Content Safety (following official quickstart)\n",
    "from azure.ai.contentsafety import ContentSafetyClient\n",
    "from azure.ai.contentsafety.models import AnalyzeTextOptions, TextCategory\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "def analyze_text_content(text_to_analyze):\n",
    "    \"\"\"\n",
    "    Function to analyze text using Azure Content Safety\n",
    "    Based on Microsoft's official quickstart\n",
    "    \"\"\"\n",
    "    # Get credentials from environment variables\n",
    "    key = os.getenv('CONTENT_SAFETY_KEY')\n",
    "    endpoint = os.getenv('CONTENT_SAFETY_ENDPOINT')\n",
    "    \n",
    "    if not key or not endpoint:\n",
    "        print(\"‚ùå Error: Define the CONTENT_SAFETY_KEY and CONTENT_SAFETY_ENDPOINT variables in the .env file\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Create Azure AI Content Safety client\n",
    "        client = ContentSafetyClient(endpoint, AzureKeyCredential(key))\n",
    "        \n",
    "        # Configure request\n",
    "        request = AnalyzeTextOptions(text=text_to_analyze)\n",
    "        \n",
    "        # Analyze text\n",
    "        response = client.analyze_text(request)\n",
    "        \n",
    "        # Extract results by specific category (following quickstart)\n",
    "        hate_result = next((item for item in response.categories_analysis if item.category == TextCategory.HATE), None)\n",
    "        self_harm_result = next((item for item in response.categories_analysis if item.category == TextCategory.SELF_HARM), None)\n",
    "        sexual_result = next((item for item in response.categories_analysis if item.category == TextCategory.SEXUAL), None)\n",
    "        violence_result = next((item for item in response.categories_analysis if item.category == TextCategory.VIOLENCE), None)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"üìù Analyzed text: '{text_to_analyze}'\")\n",
    "        print(\"üîç Analysis results:\")\n",
    "        \n",
    "        if hate_result:\n",
    "            print(f\"  üí¨ Hate: Severity {hate_result.severity}\")\n",
    "        if self_harm_result:\n",
    "            print(f\"  ü©π Self-harm: Severity {self_harm_result.severity}\")\n",
    "        if sexual_result:\n",
    "            print(f\"  üîû Sexual: Severity {sexual_result.severity}\")\n",
    "        if violence_result:\n",
    "            print(f\"  ‚öîÔ∏è  Violence: Severity {violence_result.severity}\")\n",
    "        \n",
    "        # Interpret overall risk level\n",
    "        max_severity = max([\n",
    "            hate_result.severity if hate_result else 0,\n",
    "            self_harm_result.severity if self_harm_result else 0,\n",
    "            sexual_result.severity if sexual_result else 0,\n",
    "            violence_result.severity if violence_result else 0\n",
    "        ])\n",
    "        \n",
    "        if max_severity == 0:\n",
    "            risk_level = \"‚úÖ Safe\"\n",
    "        elif max_severity <= 2:\n",
    "            risk_level = \"‚ö†Ô∏è Low risk\"\n",
    "        elif max_severity <= 4:\n",
    "            risk_level = \"üî∏ Moderate risk\"\n",
    "        else:\n",
    "            risk_level = \"üî¥ High risk\"\n",
    "            \n",
    "        print(f\"üìä Overall assessment: {risk_level}\")\n",
    "        return response\n",
    "        \n",
    "    except HttpResponseError as e:\n",
    "        print(\"‚ùå Text analysis failed.\")\n",
    "        if e.error:\n",
    "            print(f\"Error code: {e.error.code}\")\n",
    "            print(f\"Error message: {e.error.message}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Configure Content Safety client\n",
    "content_safety_endpoint = os.getenv('CONTENT_SAFETY_ENDPOINT')\n",
    "content_safety_key = os.getenv('CONTENT_SAFETY_KEY')\n",
    "\n",
    "if content_safety_endpoint and content_safety_key:\n",
    "    print(\"=== Content Safety - Text Analysis ===\")\n",
    "    print(\"Based on Microsoft's official quickstart\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Sample texts for analysis (including different risk levels)\n",
    "    test_texts = [\n",
    "        \"Hello! How are you today? Have a great day!\",\n",
    "        \"This is neutral text about technology and Python programming.\",\n",
    "        \"I'm very angry about this situation, but I'll resolve it civilly.\",\n",
    "        \"Test text for potentially problematic content moderation.\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"üìã Analyzing {len(test_texts)} sample texts...\\n\")\n",
    "    \n",
    "    for idx, text in enumerate(test_texts, 1):\n",
    "        print(f\"üîç Analysis {idx}/{len(test_texts)}:\")\n",
    "        result = analyze_text_content(text)\n",
    "        \n",
    "        if result:\n",
    "            print(\"‚úÖ Analysis completed successfully\")\n",
    "        else:\n",
    "            print(\"‚ùå Analysis failed\")\n",
    "            \n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Required configuration:\")\n",
    "    print(\"Define the CONTENT_SAFETY_ENDPOINT and CONTENT_SAFETY_KEY variables in the .env file\")\n",
    "    print(\"\\nConfiguration example:\")\n",
    "    print(\"CONTENT_SAFETY_ENDPOINT=https://your-content-safety.cognitiveservices.azure.com/\")\n",
    "    print(\"CONTENT_SAFETY_KEY=your-content-safety-key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dace413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - Content Safety for Images (following official quickstart)\n",
    "from azure.ai.contentsafety.models import AnalyzeImageOptions, ImageData\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "def analyze_image_content(image_path):\n",
    "    \"\"\"\n",
    "    Function to analyze image using Azure Content Safety\n",
    "    Based on Microsoft's official quickstart\n",
    "    \"\"\"\n",
    "    # Get credentials from environment variables\n",
    "    key = os.getenv('CONTENT_SAFETY_KEY')\n",
    "    endpoint = os.getenv('CONTENT_SAFETY_ENDPOINT')\n",
    "    \n",
    "    if not key or not endpoint:\n",
    "        print(\"‚ùå Error: Define the CONTENT_SAFETY_KEY and CONTENT_SAFETY_ENDPOINT variables in the .env file\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Create Azure AI Content Safety client\n",
    "        client = ContentSafetyClient(endpoint, AzureKeyCredential(key))\n",
    "        \n",
    "        # Read image\n",
    "        with open(image_path, \"rb\") as file:\n",
    "            image_data = file.read()\n",
    "            \n",
    "        # Configure request for image analysis\n",
    "        request = AnalyzeImageOptions(image=ImageData(content=image_data))\n",
    "        \n",
    "        # Analyze image\n",
    "        response = client.analyze_image(request)\n",
    "        \n",
    "        # Extract results by specific category\n",
    "        hate_result = next((item for item in response.categories_analysis if item.category == TextCategory.HATE), None)\n",
    "        self_harm_result = next((item for item in response.categories_analysis if item.category == TextCategory.SELF_HARM), None)\n",
    "        sexual_result = next((item for item in response.categories_analysis if item.category == TextCategory.SEXUAL), None)\n",
    "        violence_result = next((item for item in response.categories_analysis if item.category == TextCategory.VIOLENCE), None)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"üñºÔ∏è Analyzed image: {image_path}\")\n",
    "        print(\"üîç Analysis results:\")\n",
    "        \n",
    "        if hate_result:\n",
    "            print(f\"  üí¨ Hate: Severity {hate_result.severity}\")\n",
    "        if self_harm_result:\n",
    "            print(f\"  ü©π Self-harm: Severity {self_harm_result.severity}\")\n",
    "        if sexual_result:\n",
    "            print(f\"  üîû Sexual: Severity {sexual_result.severity}\")\n",
    "        if violence_result:\n",
    "            print(f\"  ‚öîÔ∏è  Violence: Severity {violence_result.severity}\")\n",
    "        \n",
    "        # Interpret overall risk level\n",
    "        max_severity = max([\n",
    "            hate_result.severity if hate_result else 0,\n",
    "            self_harm_result.severity if self_harm_result else 0,\n",
    "            sexual_result.severity if sexual_result else 0,\n",
    "            violence_result.severity if violence_result else 0\n",
    "        ])\n",
    "        \n",
    "        if max_severity == 0:\n",
    "            risk_level = \"‚úÖ Safe\"\n",
    "        elif max_severity <= 2:\n",
    "            risk_level = \"‚ö†Ô∏è Low risk\"\n",
    "        elif max_severity <= 4:\n",
    "            risk_level = \"üî∏ Moderate risk\"\n",
    "        else:\n",
    "            risk_level = \"üî¥ High risk\"\n",
    "            \n",
    "        print(f\"üìä Overall assessment: {risk_level}\")\n",
    "        return response\n",
    "        \n",
    "    except HttpResponseError as e:\n",
    "        print(\"‚ùå Image analysis failed.\")\n",
    "        if e.error:\n",
    "            print(f\"Error code: {e.error.code}\")\n",
    "            print(f\"Error message: {e.error.message}\")\n",
    "        return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: Image file not found: {image_path}\")\n",
    "        print(\"Check if the image path is correct.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "if content_safety_endpoint and content_safety_key:\n",
    "    print(\"\\n=== Content Safety - Image Analysis ===\")\n",
    "    print(\"Based on Microsoft's official quickstart\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Paths for sample images\n",
    "    image_paths = [\n",
    "        \"../../../samples/234039841.jpg\",\n",
    "        \"../../../samples/car-accident.png\"\n",
    "    ]\n",
    "    \n",
    "    for idx, image_path in enumerate(image_paths, 1):\n",
    "        print(f\"\\nüîç Analysis {idx}/{len(image_paths)}:\")\n",
    "        result = analyze_image_content(image_path)\n",
    "        \n",
    "        if result:\n",
    "            print(\"‚úÖ Analysis completed successfully\")\n",
    "        else:\n",
    "            print(\"‚ùå Analysis failed\")\n",
    "            \n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Required configuration:\")\n",
    "    print(\"Define the CONTENT_SAFETY_ENDPOINT and CONTENT_SAFETY_KEY variables in the .env file\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
