{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a53b34b",
   "metadata": {},
   "source": [
    "# Azure AI Foundry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f4a9f",
   "metadata": {},
   "source": [
    "<center><img src=\"../../../images/Azure-AI-Foundry_1600x900.jpg\" alt=\"Azure AI Foundry\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad5ac73",
   "metadata": {},
   "source": [
    "## Lab 5 - RAG (Retrieval-Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b357035",
   "metadata": {},
   "source": [
    "In this lab, we will implement a RAG (Retrieval-Augmented Generation) system using Azure AI Search and Azure OpenAI. RAG is a technique that combines information retrieval with text generation, allowing language models to access specific external knowledge beyond their training.\n",
    "\n",
    "We will explore:\n",
    "1. **Query without RAG**: How the LLM responds without additional knowledge\n",
    "2. **Azure AI Search Setup**: Preparing the vector search index\n",
    "3. **RAG Implementation**: Integrating search with response generation\n",
    "4. **Results Comparison**: Demonstrating the difference between responses with and without RAG\n",
    "\n",
    "The first step is validating the environment variables configuration in the `.env` file present at the repository root.\n",
    "\n",
    "Fill in the variable values as requested, including the Azure AI Search credentials.\n",
    "\n",
    "### Exercise 1 - Configuration and Library Import\n",
    "\n",
    "Let's import the necessary libraries for the RAG lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c121807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-search azure-search-documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1262cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv(dotenv_path=\"../../../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9f938d",
   "metadata": {},
   "source": [
    "Let's load the credentials into variables to facilitate use in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a01bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure OpenAI Configuration\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "api_version = os.getenv(\"API_VERSION\")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "embedding_model = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    "\n",
    "# Azure AI Search Configuration\n",
    "search_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "search_key = os.getenv(\"AZURE_SEARCH_KEY\")\n",
    "search_index = os.getenv(\"AZURE_SEARCH_INDEX\", \"rag-index\")\n",
    "\n",
    "print(\"Configurations loaded:\")\n",
    "print(f\"Azure OpenAI Endpoint: {azure_endpoint}\")\n",
    "print(f\"Azure Search Endpoint: {search_endpoint}\")\n",
    "print(f\"Search Index: {search_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b7e2f1",
   "metadata": {},
   "source": [
    "Now let's initialize the Azure OpenAI and Azure AI Search clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addfb739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI client\n",
    "openai_client = AzureOpenAI(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_key=api_key,\n",
    "    api_version=api_version\n",
    ")\n",
    "\n",
    "# Initialize Azure AI Search client\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=search_index,\n",
    "    credential=AzureKeyCredential(search_key)\n",
    ")\n",
    "\n",
    "print(\"Clients initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1436c5da",
   "metadata": {},
   "source": [
    "### Exercise 2 - Query Without RAG (Baseline)\n",
    "\n",
    "First, let's ask a specific question about a topic that probably isn't in the LLM's base knowledge or is outdated. This will allow us to compare the quality of responses before and after RAG implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f1c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific question about a topic that may not be in the LLM's knowledge\n",
    "question = \"What are the main features of Azure AI Foundry launched in 2024?\"\n",
    "\n",
    "def query_without_rag(question):\n",
    "    \"\"\"Makes a direct query to the model without using RAG\"\"\"\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in Azure technologies.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Query without RAG\n",
    "print(\"=== RESPONSE WITHOUT RAG ===\")\n",
    "response_without_rag = query_without_rag(question)\n",
    "print(response_without_rag)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5960ff40",
   "metadata": {},
   "source": [
    "### Exercise 3 - Data Preparation for RAG\n",
    "\n",
    "Let's create some sample documents about Azure AI Foundry to simulate a knowledge base. In a real scenario, this data would come from official documentation, manuals, or other knowledge repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1faa4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data about Azure AI Foundry\n",
    "sample_documents = [\n",
    "    {\n",
    "        \"id\": \"1\",\n",
    "        \"title\": \"Azure AI Foundry - Overview\",\n",
    "        \"content\": \"\"\"Azure AI Foundry is a unified platform for developing AI applications. \n",
    "        Launched in 2024, it offers integrated tools to build, train and deploy AI models. \n",
    "        Includes native support for RAG, function calling, and integration with Azure AI Search. \n",
    "        The platform enables team collaboration and centralized model governance.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"2\", \n",
    "        \"title\": \"Azure AI Foundry 2024 Features\",\n",
    "        \"content\": \"\"\"The main features of Azure AI Foundry include: \n",
    "        1) Model Catalog with over 200 pre-trained models\n",
    "        2) Prompt Flow for AI workflow orchestration\n",
    "        3) Integrated Azure AI Search for RAG implementations\n",
    "        4) Evaluation tools for quality metrics\n",
    "        5) Responsible AI dashboard for ethical monitoring\n",
    "        6) Multi-cloud deployment support\n",
    "        7) Real-time inference endpoints with auto-scaling\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"3\",\n",
    "        \"title\": \"RAG in Azure AI Foundry\", \n",
    "        \"content\": \"\"\"Azure AI Foundry offers native support for Retrieval-Augmented Generation (RAG). \n",
    "        Allows easy connection with Azure AI Search, Cosmos DB, and other data sources. \n",
    "        Includes visual tools to configure RAG pipelines without code. \n",
    "        Supports custom embeddings and multiple retrieval types like hybrid and semantic.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"4\",\n",
    "        \"title\": \"Azure AI Search Integration\",\n",
    "        \"content\": \"\"\"Integration with Azure AI Search enables advanced vector search with support for:\n",
    "        - Hybrid search (keyword + semantic)\n",
    "        - Metadata filters\n",
    "        - Semantic re-ranking \n",
    "        - Multiple similarity algorithms (cosine, dot product, euclidean)\n",
    "        - Automatic document indexing\n",
    "        - Support for over 300 file formats\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(sample_documents)} sample documents\")\n",
    "for doc in sample_documents:\n",
    "    print(f\"- {doc['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f98731",
   "metadata": {},
   "source": [
    "### Exercise 4 - Embeddings Generation\n",
    "\n",
    "Now let's generate embeddings for our documents using Azure OpenAI. Embeddings are vector representations of documents that enable semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1337f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    \"\"\"Generates embedding for a text using Azure OpenAI\"\"\"\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=text,\n",
    "        model=embedding_model\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Generate embeddings for all documents\n",
    "print(\"Generating embeddings for documents...\")\n",
    "for doc in sample_documents:\n",
    "    # Combine title and content for embedding\n",
    "    full_text = f\"{doc['title']} {doc['content']}\"\n",
    "    doc['embedding'] = get_embedding(full_text)\n",
    "    print(f\"‚úì Embedding generated for: {doc['title']}\")\n",
    "\n",
    "print(f\"\\nEmbeddings generated! Dimension: {len(sample_documents[0]['embedding'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b398fbd2",
   "metadata": {},
   "source": [
    "### Exercise 5 - Semantic Search (Simulated)\n",
    "\n",
    "Since we don't have a real Azure AI Search index configured, let's simulate semantic search by calculating similarity between embeddings. In a real environment, Azure AI Search would do this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab30003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculates cosine similarity between two vectors\"\"\"\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def semantic_search(query, documents, top_k=2):\n",
    "    \"\"\"Performs semantic search on documents\"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    similarities = []\n",
    "    for doc in documents:\n",
    "        similarity = cosine_similarity(query_embedding, doc['embedding'])\n",
    "        similarities.append({\n",
    "            'document': doc,\n",
    "            'similarity': similarity\n",
    "        })\n",
    "    \n",
    "    # Sort by similarity (highest first)\n",
    "    similarities.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Test semantic search\n",
    "print(\"=== SEMANTIC SEARCH ===\")\n",
    "results = semantic_search(question, sample_documents)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    doc = result['document']\n",
    "    similarity = result['similarity']\n",
    "    print(f\"{i}. {doc['title']} (Similarity: {similarity:.3f})\")\n",
    "    print(f\"   Content: {doc['content'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4b7c1a",
   "metadata": {},
   "source": [
    "### Exercise 6 - RAG Implementation\n",
    "\n",
    "Now let's implement the complete RAG system, combining semantic search with response generation by the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2625a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_rag(question, documents):\n",
    "    \"\"\"Implements RAG: search relevant documents and generate response based on context\"\"\"\n",
    "    \n",
    "    # 1. Retrieval: Search relevant documents\n",
    "    search_results = semantic_search(question, documents, top_k=2)\n",
    "    \n",
    "    # 2. Build context with retrieved documents\n",
    "    context_parts = []\n",
    "    for result in search_results:\n",
    "        doc = result['document']\n",
    "        context_parts.append(f\"Document: {doc['title']}\\nContent: {doc['content']}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # 3. Build prompt with context\n",
    "    system_message = \"\"\"You are an assistant specialized in Azure technologies. \n",
    "    Use ONLY the information provided in the context to answer the question. \n",
    "    If the information is not in the context, say you don't have that specific information.\n",
    "    Be precise and cite specific information from the context when possible.\"\"\"\n",
    "    \n",
    "    user_message = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer based on the provided context:\"\"\"\n",
    "    \n",
    "    # 4. Augmented Generation: Generate response with context\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        temperature=0.3  # Lower temperature for more precise responses\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'answer': response.choices[0].message.content,\n",
    "        'sources': [result['document']['title'] for result in search_results],\n",
    "        'context': context\n",
    "    }\n",
    "\n",
    "# Execute RAG with the same question\n",
    "print(\"=== RESPONSE WITH RAG ===\")\n",
    "rag_result = query_with_rag(question, sample_documents)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(rag_result['answer'])\n",
    "print(f\"\\nSources used: {', '.join(rag_result['sources'])}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec2012",
   "metadata": {},
   "source": [
    "### Exercise 7 - Results Comparison\n",
    "\n",
    "Let's directly compare the responses obtained without RAG and with RAG to highlight the differences and benefits of using specific knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaeac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç COMPARATIVE ANALYSIS: RAG vs Without RAG\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìù QUESTION:\")\n",
    "print(f'\"{question}\"')\n",
    "\n",
    "print(\"\\n‚ùå RESPONSE WITHOUT RAG (limited knowledge):\")\n",
    "print(\"-\" * 50)\n",
    "print(response_without_rag)\n",
    "\n",
    "print(\"\\n‚úÖ RESPONSE WITH RAG (augmented knowledge):\")\n",
    "print(\"-\" * 50)\n",
    "print(rag_result['answer'])\n",
    "\n",
    "print(f\"\\nüìö SOURCES USED IN RAG:\")\n",
    "for source in rag_result['sources']:\n",
    "    print(f\"‚Ä¢ {source}\")\n",
    "\n",
    "print(\"\\nüí° OBSERVED BENEFITS OF RAG:\")\n",
    "benefits = [\n",
    "    \"‚úì More specific and up-to-date information\",\n",
    "    \"‚úì Responses based on reliable sources\",\n",
    "    \"‚úì Greater precision in technical details\", \n",
    "    \"‚úì Information traceability (sources)\",\n",
    "    \"‚úì Reduction of model hallucinations\"\n",
    "]\n",
    "\n",
    "for benefit in benefits:\n",
    "    print(benefit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5289624c",
   "metadata": {},
   "source": [
    "### Exercise 8 - RAG with Azure AI Search (Real Example)\n",
    "\n",
    "Although we simulated semantic search, let's show how the real implementation would be using Azure AI Search. This code demonstrates how to connect with a real index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce45378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_azure_ai_search(question, search_client, openai_client):\n",
    "    \"\"\"\n",
    "    Real RAG implementation using Azure AI Search\n",
    "    Note: This code requires a configured index in Azure AI Search\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Generate question embedding\n",
    "        query_embedding = get_embedding(question)\n",
    "        \n",
    "        # 2. Create vectorized query for Azure AI Search\n",
    "        vector_query = VectorizedQuery(\n",
    "            vector=query_embedding,\n",
    "            k_nearest_neighbors=3,  # Top 3 most similar results\n",
    "            fields=\"content_vector\"  # Field containing embeddings\n",
    "        )\n",
    "        \n",
    "        # 3. Execute search in Azure AI Search\n",
    "        search_results = search_client.search(\n",
    "            search_text=question,  # Hybrid search: text + vector\n",
    "            vector_queries=[vector_query],\n",
    "            top=3,\n",
    "            include_total_count=True\n",
    "        )\n",
    "        \n",
    "        # 4. Extract context from results\n",
    "        context_parts = []\n",
    "        sources = []\n",
    "        \n",
    "        for result in search_results:\n",
    "            context_parts.append(f\"Title: {result['title']}\\nContent: {result['content']}\")\n",
    "            sources.append(result['title'])\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # 5. Generate response with context\n",
    "        system_message = \"\"\"You are an assistant specialized in Azure. \n",
    "        Use ONLY the information from the provided context to answer.\"\"\"\n",
    "        \n",
    "        user_message = f\"\"\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "        \n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=deployment_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'answer': response.choices[0].message.content,\n",
    "            'sources': sources,\n",
    "            'context': context\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'answer': f\"Error connecting to Azure AI Search: {str(e)}\",\n",
    "            'sources': [],\n",
    "            'context': \"\"\n",
    "        }\n",
    "\n",
    "# Usage example (commented as it requires configured index)\n",
    "print(\"üìã CODE FOR REAL AZURE AI SEARCH:\")\n",
    "print(\"# To use this code, you need:\")\n",
    "print(\"# 1. Create an index in Azure AI Search\")\n",
    "print(\"# 2. Configure embedding fields\") \n",
    "print(\"# 3. Index your documents\")\n",
    "print(\"# 4. Execute: query_with_azure_ai_search(question, search_client, openai_client)\")\n",
    "\n",
    "# result = query_with_azure_ai_search(question, search_client, openai_client)\n",
    "# print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4059170",
   "metadata": {},
   "source": [
    "### Exercise 9 - Interactive Testing\n",
    "\n",
    "Now you can test the RAG system with your own questions! Try different types of queries to see how RAG behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c3792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your own questions here!\n",
    "test_questions = [\n",
    "    \"How does Azure AI Foundry support RAG?\",\n",
    "    \"How many models are available in the Model Catalog?\",\n",
    "    \"What types of search are supported by Azure AI Search?\",\n",
    "    \"What is Prompt Flow in Azure AI Foundry?\"\n",
    "]\n",
    "\n",
    "print(\"üß™ INTERACTIVE TEST - Try different questions:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for test_q in test_questions:\n",
    "    print(f\"\\n‚ùì Question: {test_q}\")\n",
    "    \n",
    "    # Response without RAG\n",
    "    no_rag = query_without_rag(test_q)\n",
    "    print(f\"‚ùå Without RAG: {no_rag[:150]}...\")\n",
    "    \n",
    "    # Response with RAG\n",
    "    with_rag = query_with_rag(test_q, sample_documents)\n",
    "    print(f\"‚úÖ With RAG: {with_rag['answer'][:150]}...\")\n",
    "    print(f\"üìö Sources: {', '.join(with_rag['sources'])}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nüí° Try creating your own questions by modifying the 'test_questions' list!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b370c0",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this lab, we explored the concept and implementation of RAG (Retrieval-Augmented Generation), demonstrating how:\n",
    "\n",
    "### ‚úÖ What we learned:\n",
    "1. **Fundamental difference**: How RAG significantly improves response quality\n",
    "2. **Embedding Generation**: Converting text into vector representations\n",
    "3. **Semantic Search**: Finding relevant documents using embedding similarity\n",
    "4. **Augmented Generation**: Combining retrieved context with text generation\n",
    "5. **Practical implementation**: Functional code for complete RAG system\n",
    "\n",
    "### üöÄ Demonstrated RAG benefits:\n",
    "- **Updated knowledge**: Access to specific and recent information\n",
    "- **Reduced hallucinations**: Responses based on reliable sources  \n",
    "- **Traceability**: Ability to identify information sources\n",
    "- **Specialization**: More precise responses about specific topics\n",
    "- **Flexibility**: Easy updating of knowledge base\n",
    "\n",
    "### üîß For production implementation:\n",
    "1. **Configure Azure AI Search** with optimized vector indexes\n",
    "2. **Implement chunking** for large documents (512-1024 tokens)\n",
    "3. **Use hybrid search** (keyword + semantic) for better recall\n",
    "4. **Add re-ranking** to improve result relevance\n",
    "5. **Monitor performance** and adjust parameters (top_k, temperature, etc.)\n",
    "6. **Implement caching** for frequent queries\n",
    "7. **Configure governance** for source validation\n",
    "\n",
    "### üìö Additional resources:\n",
    "- [Azure AI Search Vector Search](https://learn.microsoft.com/en-us/azure/search/vector-search-overview)\n",
    "- [Azure OpenAI RAG Patterns](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/use-your-data)\n",
    "- [Azure AI Foundry Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/)\n",
    "- [RAG Best Practices](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-usage)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
