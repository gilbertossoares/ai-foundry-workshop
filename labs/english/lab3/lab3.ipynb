{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce9486c",
   "metadata": {},
   "source": [
    "# Azure AI Foundry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04825252",
   "metadata": {},
   "source": [
    "<center><img src=\"../../../images/Azure-AI-Foundry_1600x900.jpg\" alt=\"Azure AI Foundry\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37212673",
   "metadata": {},
   "source": [
    "## Laboratory 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff23990",
   "metadata": {},
   "source": [
    "### Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f67fac",
   "metadata": {},
   "source": [
    "Prompt Engineering is the art and science of creating effective instructions for large language models (LLMs). It's an essential skill for obtaining accurate, relevant, and useful results when interacting with generative AI.\n",
    "\n",
    "**What is Prompt Engineering?**\n",
    "\n",
    "Prompt Engineering involves the careful design of instructions (prompts) that guide AI model behavior to produce desired outputs. This includes word choice, information structuring, providing context, and defining output formats.\n",
    "\n",
    "**Why is it Important?**\n",
    "\n",
    "- **Accuracy**: Well-crafted prompts produce more accurate results\n",
    "- **Consistency**: Structured techniques ensure predictable results\n",
    "- **Efficiency**: Reduces the need for multiple attempts\n",
    "- **Control**: Enables greater control over style and output format\n",
    "\n",
    "**Components of a Good Prompt:**\n",
    "\n",
    "1. **Context**: Relevant background information\n",
    "2. **Instruction**: What you want the model to do\n",
    "3. **Examples**: Demonstrations of the desired format (when applicable)\n",
    "4. **Constraints**: Specific restrictions or guidelines\n",
    "5. **Output Format**: How the response should be structured\n",
    "\n",
    "In the following sections, we'll explore various advanced Prompt Engineering techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure OpenAI Configuration\n",
    "import json\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=\"../../../.env\")\n",
    "\n",
    "# Settings\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "api_version = os.getenv(\"API_VERSION\")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "\n",
    "# Initialize client\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=azure_endpoint, \n",
    "    api_key=api_key,  \n",
    "    api_version=api_version\n",
    ")\n",
    "\n",
    "print(\"Azure OpenAI client configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936bd1a9",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962a6d4d",
   "metadata": {},
   "source": [
    "Zero-Shot Prompting is a technique where you provide a task to the language model without prior examples. The model must understand and execute the task based solely on its general understanding and pre-trained knowledge.\n",
    "\n",
    "**Characteristics:**\n",
    "- Does not require input/output examples\n",
    "- Relies on the model's pre-trained knowledge\n",
    "- Simple to implement\n",
    "- May not be effective for complex or specific tasks\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Prompt: \"Translate the following sentence to Portuguese: 'Hello, how are you?'\"\n",
    "Response: \"Olá, como você está?\"\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- Simple and well-defined tasks\n",
    "- When you don't have examples available\n",
    "- For initial testing of model capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a7e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example of Zero-Shot Prompting\n",
    "print(\"=== TEST: Zero-Shot Prompting ===\")\n",
    "\n",
    "# Example 1: Translation\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Translate the following sentence to Portuguese: 'Hello, how are you?'\"}\n",
    "    ],\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"Translation:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Example 2: Sentiment classification\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Classify the sentiment of this sentence as positive, negative, or neutral: 'This product is amazing, I highly recommend it!'\"}\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"Sentiment classification:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada70d7c",
   "metadata": {},
   "source": [
    "### Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175313d0",
   "metadata": {},
   "source": [
    "Few-Shot Prompting is a technique where you provide a few examples of input and expected output before presenting the actual task. This helps the model better understand the pattern and desired response format.\n",
    "\n",
    "**Characteristics:**\n",
    "- Includes 2-5 demonstrative examples\n",
    "- Improves accuracy compared to zero-shot\n",
    "- Helps the model understand output format\n",
    "- Effective for tasks that follow specific patterns\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Prompt: \n",
    "\"Classify the sentiment of the following sentences:\n",
    "Sentence: 'I love this product!' → Sentiment: Positive\n",
    "Sentence: 'This service is terrible.' → Sentiment: Negative\n",
    "Sentence: 'The product is ok.' → Sentiment: Neutral\n",
    "Sentence: 'I am very happy with the purchase!' → Sentiment: ?\"\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- Tasks that require specific format\n",
    "- When zero-shot doesn't provide adequate results\n",
    "- For classification or structured tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ae69eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example of Few-Shot Prompting\n",
    "print(\"=== TEST: Few-Shot Prompting ===\")\n",
    "\n",
    "# Example: Sentiment classification with examples\n",
    "few_shot_prompt = \"\"\"Classify the sentiment of the following sentences:\n",
    "\n",
    "Sentence: 'I love this product!' → Sentiment: Positive\n",
    "Sentence: 'This service is terrible.' → Sentiment: Negative  \n",
    "Sentence: 'The product is ok.' → Sentiment: Neutral\n",
    "Sentence: 'I couldn't use the app, very confusing.' → Sentiment: Negative\n",
    "Sentence: 'Worked perfectly, exactly as expected!' → Sentiment: Positive\n",
    "\n",
    "Sentence: 'The service was reasonable, nothing special.' → Sentiment: ?\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": few_shot_prompt}\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"Classification with Few-Shot:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Example 2: Data formatting with examples\n",
    "format_prompt = \"\"\"Convert the data to the specified format:\n",
    "\n",
    "Name: John Silva, Age: 30, City: New York → {\"name\": \"John Silva\", \"age\": 30, \"city\": \"New York\"}\n",
    "Name: Mary Santos, Age: 25, City: Los Angeles → {\"name\": \"Mary Santos\", \"age\": 25, \"city\": \"Los Angeles\"}\n",
    "Name: Peter Costa, Age: 45, City: Chicago → {\"name\": \"Peter Costa\", \"age\": 45, \"city\": \"Chicago\"}\n",
    "\n",
    "Name: Ana Oliveira, Age: 28, City: Miami → ?\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": format_prompt}\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"Formatting with Few-Shot:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de84478",
   "metadata": {},
   "source": [
    "### Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f864b788",
   "metadata": {},
   "source": [
    "Chain-of-Thought (CoT) Prompting is a technique that encourages the model to show its step-by-step reasoning before arriving at the final answer. This significantly improves performance on complex reasoning tasks.\n",
    "\n",
    "**Characteristics:**\n",
    "- Breaks complex problems into smaller steps\n",
    "- Shows the reasoning process\n",
    "- Improves accuracy in mathematical and logical problems\n",
    "- Allows identification of where reasoning might have failed\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Prompt: \"Solve step by step: If a train travels at 60 km/h and needs to cover 180 km, how long will it take?\n",
    "\n",
    "Let's think step by step:\n",
    "1. Speed = 60 km/h\n",
    "2. Distance = 180 km\n",
    "3. Time = Distance ÷ Speed\n",
    "4. Time = 180 ÷ 60 = 3 hours\n",
    "\n",
    "Answer: 3 hours\"\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- Mathematical or logical problems\n",
    "- Tasks requiring multi-step reasoning\n",
    "- When you need to verify the thought process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed02164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example of Chain-of-Thought Prompting\n",
    "print(\"=== TEST: Chain-of-Thought Prompting ===\")\n",
    "\n",
    "# Example 1: Mathematical problem\n",
    "cot_prompt = \"\"\"Solve step by step: \n",
    "\n",
    "In a store, there are 24 t-shirts. 1/3 of them are blue, 1/4 are red, and the rest are white. \n",
    "If the price of each blue t-shirt is $30, each red one $25, and each white one $20, what is the total stock value?\n",
    "\n",
    "Let's think step by step:\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": cot_prompt}\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"Solution with Chain-of-Thought:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Example 2: Logic problem\n",
    "logic_prompt = \"\"\"Solve this logic problem step by step:\n",
    "\n",
    "Ana, Bruno, and Carlos are in a line. \n",
    "- Ana is not in front\n",
    "- Bruno is not in the middle\n",
    "- Carlos is not behind\n",
    "\n",
    "What is the order of the line?\n",
    "\n",
    "Let's analyze each clue:\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": logic_prompt}\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"Logic solution with Chain-of-Thought:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3aa53f",
   "metadata": {},
   "source": [
    "### Meta Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab1ad69",
   "metadata": {},
   "source": [
    "Meta Prompting is an advanced technique where the model is instructed to generate or improve prompts. Essentially, you use the model to create better prompts for itself or other tasks.\n",
    "\n",
    "**Characteristics:**\n",
    "- The model helps create more effective prompts\n",
    "- Can iteratively improve prompt quality\n",
    "- Useful for automatic prompt optimization\n",
    "- Requires knowledge about prompting techniques\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Prompt: \"Create an effective prompt to make an AI model explain complex scientific concepts to 10-year-old children. The prompt should include:\n",
    "- Simple language\n",
    "- Age-appropriate analogies\n",
    "- Clear structure\n",
    "- Interactive elements\"\n",
    "\n",
    "Response: \"Explain [scientific concept] to a 10-year-old child. Use simple words, compare with things they know from daily life, organize into 3 main parts, and ask questions to maintain interest.\"\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- Optimization of existing prompts\n",
    "- Creating prompts for specific tasks\n",
    "- When you need multiple variations of a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73508d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example of Meta Prompting\n",
    "print(\"=== TEST: Meta Prompting ===\")\n",
    "\n",
    "# Example 1: Creating a prompt for teaching\n",
    "meta_prompt = \"\"\"Create an effective prompt to make an AI model explain programming concepts to beginners. The prompt should include:\n",
    "- Simple and accessible language\n",
    "- Real-world analogies\n",
    "- Clear and didactic structure  \n",
    "- Practical examples\n",
    "- Comprehension verification\n",
    "\n",
    "Provide only the optimized prompt, without additional explanations.\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": meta_prompt}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "generated_prompt = response.choices[0].message.content\n",
    "print(\"Prompt generated by Meta Prompting:\")\n",
    "print(generated_prompt)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Example 2: Testing the generated prompt\n",
    "test_content = generated_prompt + \"\\n\\nConcept: Variables in programming\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": test_content}\n",
    "    ],\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "print(\"Test of the generated prompt:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d347554",
   "metadata": {},
   "source": [
    "### Prompt Chaining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ea7135",
   "metadata": {},
   "source": [
    "Prompt Chaining is a technique where you break a complex task into multiple sequential prompts, where the output of one prompt serves as input for the next.\n",
    "\n",
    "**Characteristics:**\n",
    "- Divides complex tasks into smaller steps\n",
    "- The output of one prompt feeds the next\n",
    "- Allows greater control over each step\n",
    "- Reduces the chance of errors in complex tasks\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Prompt 1: \"Analyze this text and identify the main points: [text]\"\n",
    "Output 1: \"Main points: A, B, C\"\n",
    "\n",
    "Prompt 2: \"Based on the main points: A, B, C, create a 100-word executive summary\"\n",
    "Output 2: [Executive summary]\n",
    "\n",
    "Prompt 3: \"Transform this summary into a 3-slide presentation: [summary]\"\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- Tasks involving multiple processing steps\n",
    "- When you need granular control over each phase\n",
    "- For data transformation into multiple formats\n",
    "- Complex analysis requiring validation at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15238b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example of Prompt Chaining\n",
    "print(\"=== TEST: Prompt Chaining ===\")\n",
    "\n",
    "# Sample text for analysis\n",
    "sample_text = \"\"\"\n",
    "Artificial intelligence is rapidly transforming various sectors of the economy. \n",
    "In healthcare, machine learning algorithms are being used for more accurate diagnoses \n",
    "and discovery of new medicines. In the financial sector, AI helps in fraud detection \n",
    "and risk analysis. In education, intelligent systems personalize learning for each \n",
    "student. However, ethical challenges and questions about the future of work also arise.\n",
    "\"\"\"\n",
    "\n",
    "# Prompt 1: Identify main points\n",
    "print(\"1. Identifying main points...\")\n",
    "response1 = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Analyze this text and identify the 3 main points in list format:\\n\\n{sample_text}\"}\n",
    "    ],\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "main_points = response1.choices[0].message.content\n",
    "print(\"Main points identified:\")\n",
    "print(main_points)\n",
    "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "\n",
    "# Prompt 2: Create executive summary\n",
    "print(\"2. Creating executive summary...\")\n",
    "response2 = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Based on the main points identified:\\n{main_points}\\n\\nCreate an executive summary of exactly 50 words.\"}\n",
    "    ],\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "executive_summary = response2.choices[0].message.content\n",
    "print(\"Executive summary:\")\n",
    "print(executive_summary)\n",
    "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "\n",
    "# Prompt 3: Create presentation\n",
    "print(\"3. Creating presentation structure...\")\n",
    "response3 = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Transform this summary into a 3-slide presentation structure with titles and bullet points:\\n\\n{executive_summary}\"}\n",
    "    ],\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "print(\"Presentation structure:\")\n",
    "print(response3.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a0cb3",
   "metadata": {},
   "source": [
    "### Tree of Thoughts (ToT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc0f89b",
   "metadata": {},
   "source": [
    "Tree of Thoughts (ToT) is an advanced technique that allows the model to explore multiple reasoning paths simultaneously, like a decision tree, evaluating and choosing the best paths.\n",
    "\n",
    "**Characteristics:**\n",
    "- Explores multiple approaches simultaneously\n",
    "- Allows backtracking when a path doesn't work\n",
    "- Evaluates the quality of each \"thought\" or step\n",
    "- More robust than linear Chain-of-Thought\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Problem: \"How to resolve conflict between teams?\"\n",
    "\n",
    "Thought 1: Direct mediation\n",
    "├── Evaluation: May be confrontational\n",
    "├── Next step: Formal meeting\n",
    "└── Expected result: Quick resolution\n",
    "\n",
    "Thought 2: Indirect facilitation\n",
    "├── Evaluation: Less stressful\n",
    "├── Next step: Individual conversations\n",
    "└── Expected result: Gradual understanding\n",
    "\n",
    "Thought 3: Process restructuring\n",
    "├── Evaluation: Systematic solution\n",
    "├── Next step: Workflow analysis\n",
    "└── Expected result: Future prevention\n",
    "\n",
    "Best path: Combination of 2 and 3\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- Complex problems with multiple possible solutions\n",
    "- When you need to explore alternatives\n",
    "- Strategic planning and decision making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd905994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example of Tree of Thoughts (ToT)\n",
    "print(\"=== TEST: Tree of Thoughts (ToT) ===\")\n",
    "\n",
    "# Complex problem for analysis\n",
    "tot_prompt = \"\"\"Problem: A technology company is facing high employee turnover (30% per year). \n",
    "As CEO, you need to solve this quickly.\n",
    "\n",
    "Explore 3 different approaches simultaneously:\n",
    "\n",
    "APPROACH 1 - Salary Improvement:\n",
    "- Evaluate: Pros, cons, and feasibility\n",
    "- Specific next steps\n",
    "- Expected outcome\n",
    "\n",
    "APPROACH 2 - Work Environment Improvement:\n",
    "- Evaluate: Pros, cons, and feasibility  \n",
    "- Specific next steps\n",
    "- Expected outcome\n",
    "\n",
    "APPROACH 3 - Development Program:\n",
    "- Evaluate: Pros, cons, and feasibility\n",
    "- Specific next steps\n",
    "- Expected outcome\n",
    "\n",
    "FINAL ANALYSIS:\n",
    "- Compare the 3 approaches\n",
    "- Recommend the best strategy or combination\n",
    "- Justify your choice\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": tot_prompt}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "print(\"Tree of Thoughts analysis:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Example 2: Planning problem\n",
    "planning_prompt = \"\"\"You need to plan the launch of a new mobile app product in 6 months.\n",
    "Explore simultaneously 3 go-to-market strategies:\n",
    "\n",
    "STRATEGY A - Gradual Launch:\n",
    "- Feasibility analysis: \n",
    "- Required resources:\n",
    "- Risks and mitigations:\n",
    "\n",
    "STRATEGY B - Massive Launch:\n",
    "- Feasibility analysis:\n",
    "- Required resources: \n",
    "- Risks and mitigations:\n",
    "\n",
    "STRATEGY C - Niche Launch:\n",
    "- Feasibility analysis:\n",
    "- Required resources:\n",
    "- Risks and mitigations:\n",
    "\n",
    "DECISION: Choose the best strategy based on the analysis.\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": planning_prompt}\n",
    "    ],\n",
    "    temperature=0.6,\n",
    "    max_tokens=800\n",
    ")\n",
    "\n",
    "print(\"Planning with Tree of Thoughts:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e23694",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6947f90e",
   "metadata": {},
   "source": [
    "Retrieval Augmented Generation (RAG) is a technique that combines text generation from the model with information retrieval from an external knowledge base, enabling more accurate and up-to-date responses.\n",
    "\n",
    "**Characteristics:**\n",
    "- Combines generation with information retrieval\n",
    "- Accesses external and updated knowledge\n",
    "- Reduces model hallucinations\n",
    "- Allows citing specific sources\n",
    "\n",
    "**RAG Components:**\n",
    "1. **Knowledge Base**: Documents, articles, databases\n",
    "2. **Retrieval System**: Searches for relevant information\n",
    "3. **Generator**: Language model that creates the response\n",
    "4. **Integration**: Combines retrieved information with generation\n",
    "\n",
    "**Process Example:**\n",
    "```\n",
    "Question: \"What are the Azure AI updates in 2024?\"\n",
    "\n",
    "1. Retrieval: Search recent documents about Azure AI\n",
    "2. Context: \"Azure AI Foundry was launched in 2024...\"\n",
    "3. Prompt: \"Based on the information: [context], answer: [question]\"\n",
    "4. Response: Generated based on retrieved context\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- When you need updated information\n",
    "- To reduce hallucinations\n",
    "- In corporate Q&A systems\n",
    "- When the model needs to cite specific sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906919de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example of RAG (Simulated)\n",
    "print(\"=== TEST: Retrieval Augmented Generation (RAG) ===\")\n",
    "\n",
    "# Simulating a knowledge base (normally would be retrieved from a vector database)\n",
    "knowledge_base = \"\"\"\n",
    "DOCUMENT 1 - Azure AI Foundry (2024):\n",
    "Azure AI Foundry is a unified platform for developing generative AI applications. \n",
    "Launched in 2024, it offers integrated tools for training, evaluating, and deploying AI models.\n",
    "Includes features like prompt flow, automated evaluation, and model monitoring.\n",
    "\n",
    "DOCUMENT 2 - Azure OpenAI Service:\n",
    "Azure OpenAI Service provides access to GPT-4, GPT-3.5-turbo, DALL-E, and Codex models through REST APIs.\n",
    "Offers enterprise features like virtual networks, customer-managed keys, and compliance.\n",
    "Available in multiple regions with different models and capabilities.\n",
    "\n",
    "DOCUMENT 3 - Prompt Engineering Best Practices:\n",
    "Essential techniques include few-shot learning, chain-of-thought, and prompt chaining.\n",
    "Important to be specific, use clear examples, and structure instructions well.\n",
    "The order of information in the prompt can significantly affect results.\n",
    "\"\"\"\n",
    "\n",
    "# User question\n",
    "question = \"What are the main features of Azure AI Foundry launched in 2024?\"\n",
    "\n",
    "# RAG prompt: Combining retrieved context with the question\n",
    "rag_prompt = f\"\"\"Based on the information provided below, answer the user's question accurately and cite relevant sources.\n",
    "\n",
    "RETRIEVED CONTEXT:\n",
    "{knowledge_base}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Use only the information provided in the context\n",
    "- Cite the specific document when relevant\n",
    "- If information is not available in the context, clearly indicate so\n",
    "- Be precise and objective in your response\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": rag_prompt}\n",
    "    ],\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "print(\"RAG Response:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Example 2: Question not in context\n",
    "question2 = \"What is the price of Azure AI Foundry?\"\n",
    "\n",
    "rag_prompt2 = f\"\"\"Based on the information provided below, answer the user's question.\n",
    "\n",
    "RETRIEVED CONTEXT:\n",
    "{knowledge_base}\n",
    "\n",
    "QUESTION: {question2}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Use only the information provided in the context\n",
    "- If information is not available, respond: \"Information not found in the provided context\"\n",
    "\"\"\"\n",
    "\n",
    "response2 = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": rag_prompt2}\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"Test with unavailable information:\")\n",
    "print(response2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e2ed31",
   "metadata": {},
   "source": [
    "### Active-Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f68321",
   "metadata": {},
   "source": [
    "Active-Prompt is a technique that automatically selects the most useful and informative examples to use in few-shot prompting, based on the model's uncertainty about certain questions.\n",
    "\n",
    "**Characteristics:**\n",
    "- Automatic selection of most useful examples\n",
    "- Based on model uncertainty\n",
    "- Improves few-shot learning efficiency\n",
    "- Reduces need for manual example curation\n",
    "\n",
    "**How it Works:**\n",
    "1. **Uncertainty Analysis**: Identifies where the model has the most doubts\n",
    "2. **Example Selection**: Chooses examples that address these uncertainties\n",
    "3. **Adaptive Prompting**: Uses the most relevant examples for each query\n",
    "4. **Continuous Refinement**: Improves based on feedback\n",
    "\n",
    "**Conceptual Example:**\n",
    "```\n",
    "Task: Sentiment classification\n",
    "\n",
    "Model identifies uncertainty in:\n",
    "- Sarcasm\n",
    "- Ambiguous language\n",
    "- Cultural expressions\n",
    "\n",
    "Active-Prompt selects examples that specifically address:\n",
    "- \"What a wonderful day, it rained at my wedding!\" (Sarcasm → Negative)\n",
    "- \"Not sure if I liked it...\" (Ambiguous → Neutral)\n",
    "- \"It's all good!\" (Cultural → Positive)\n",
    "```\n",
    "\n",
    "**When to use:**\n",
    "- When you have many examples available\n",
    "- To automatically optimize few-shot prompts\n",
    "- In domains where uncertainty varies by topic\n",
    "- For adaptive systems that improve with use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f4bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example of Active-Prompt (Simulated)\n",
    "print(\"=== TEST: Active-Prompt ===\")\n",
    "\n",
    "# Simulating the process of selecting examples based on uncertainty\n",
    "def simulate_active_prompt():\n",
    "    # Pool of available examples\n",
    "    example_pool = [\n",
    "        (\"What a wonderful day, it rained at my wedding!\", \"Negative\", \"sarcasm\"),\n",
    "        (\"This product is ok, nothing special.\", \"Neutral\", \"ambiguity\"),\n",
    "        (\"I loved the experience!\", \"Positive\", \"direct\"),\n",
    "        (\"This app is pretty cool!\", \"Positive\", \"informal_language\"),\n",
    "        (\"Not sure if I really liked it...\", \"Neutral\", \"uncertainty\"),\n",
    "        (\"Simply fantastic!\", \"Positive\", \"enthusiasm\"),\n",
    "        (\"Could be better, right?\", \"Negative\", \"indirect_criticism\")\n",
    "    ]\n",
    "    \n",
    "    # Question that generates uncertainty (informal language + potential sarcasm)\n",
    "    query = \"This thing is pretty awesome, dude!\"\n",
    "    \n",
    "    # Active-Prompt selects most relevant examples for similar cases\n",
    "    selected_examples = [\n",
    "        (\"What a wonderful day, it rained at my wedding!\", \"Negative\"),  # sarcasm\n",
    "        (\"This app is pretty cool!\", \"Positive\"),  # informal language\n",
    "        (\"Could be better, right?\", \"Negative\")  # ambiguous tone\n",
    "    ]\n",
    "    \n",
    "    return query, selected_examples\n",
    "\n",
    "# Running simulation\n",
    "query, selected_examples = simulate_active_prompt()\n",
    "\n",
    "print(\"Example of Active-Prompt in action:\")\n",
    "print(f\"Sentence to classify: '{query}'\")\n",
    "print(\"\\nExamples automatically selected based on uncertainty:\")\n",
    "for i, (sentence, sentiment) in enumerate(selected_examples, 1):\n",
    "    print(f\"{i}. '{sentence}' → {sentiment}\")\n",
    "\n",
    "# Building prompt with dynamically selected examples\n",
    "active_prompt = \"Classify the sentiment (Positive, Negative, Neutral) considering cultural context and possible sarcasm:\\n\\n\"\n",
    "\n",
    "for sentence, sentiment in selected_examples:\n",
    "    active_prompt += f\"Sentence: '{sentence}' → Sentiment: {sentiment}\\n\"\n",
    "\n",
    "active_prompt += f\"\\nSentence: '{query}' → Sentiment: ?\"\n",
    "\n",
    "print(f\"\\n{'-'*50}\")\n",
    "print(\"Dynamically constructed prompt:\")\n",
    "print(active_prompt)\n",
    "\n",
    "# Running classification\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": active_prompt}\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(f\"\\n{'-'*50}\")\n",
    "print(\"Active-Prompt result:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Comparison: same prompt without Active-Prompt (generic examples)\n",
    "generic_prompt = \"\"\"Classify the sentiment (Positive, Negative, Neutral):\n",
    "\n",
    "Sentence: 'I love this product!' → Sentiment: Positive\n",
    "Sentence: 'This service is terrible.' → Sentiment: Negative\n",
    "Sentence: 'The product is ok.' → Sentiment: Neutral\n",
    "\n",
    "Sentence: 'This thing is pretty awesome, dude!' → Sentiment: ?\"\"\"\n",
    "\n",
    "response_generic = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": generic_prompt}\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"Comparison with generic Few-Shot:\")\n",
    "print(response_generic.choices[0].message.content)\n",
    "print(\"\\nNote: Active-Prompt selected more relevant examples to handle informal language and possible sarcasm.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6ce52",
   "metadata": {},
   "source": [
    "### Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f6960",
   "metadata": {},
   "source": [
    "**Be Specific.** Leave as little as possible to interpretation. Constrain the operating space.\n",
    "\n",
    "**Be Descriptive.** Use analogies to make instructions clearer.\n",
    "\n",
    "**Reinforce Instructions.** Sometimes it may be necessary to repeat yourself for the model. Provide instructions before and after your main content, use an instruction and a cue, etc.\n",
    "\n",
    "**Order Matters.** The order in which you present information to the model can impact the outcome. Whether you place instructions before your content (\"summarize the following...\") or after (\"summarize the text above...\") can make a difference in the result. Even the order of few-shot examples can matter. This is known as recency bias.\n",
    "\n",
    "**Offer an Alternative to the Model.** Sometimes it can be useful to give the model an alternative path if it cannot complete the assigned task. For example, when asking a question about a text, you might include something like \"answer with 'not found' if the answer is not present.\" This can help the model avoid generating false responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370145a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 EXPERIMENTATION AREA\n",
    "# Use this space to test Prompt Engineering techniques\n",
    "\n",
    "# Example: Test your own technique here\n",
    "# response = client.chat.completions.create(\n",
    "#     model=deployment_name,\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": \"Your prompt here...\"}\n",
    "#     ],\n",
    "#     temperature=0.5\n",
    "# )\n",
    "# \n",
    "# print(response.choices[0].message.content)\n",
    "\n",
    "# Your code here..."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
