{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b6954ca",
   "metadata": {},
   "source": [
    "# Azure AI Foundry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc5272",
   "metadata": {},
   "source": [
    "<center><img src=\"../../../images/Azure-AI-Foundry_1600x900.jpg\" alt=\"Azure AI Foundry\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548c1279",
   "metadata": {},
   "source": [
    "## Laborat√≥rio 5 - RAG (Retrieval-Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d37a39",
   "metadata": {},
   "source": [
    "Neste laborat√≥rio iremos implementar um sistema RAG (Retrieval-Augmented Generation) usando Azure AI Search e Azure OpenAI. O RAG √© uma t√©cnica que combina recupera√ß√£o de informa√ß√µes com gera√ß√£o de texto, permitindo que os modelos de linguagem acessem conhecimentos espec√≠ficos externos ao seu treinamento.\n",
    "\n",
    "Vamos explorar:\n",
    "1. **Consulta sem RAG**: Como a LLM responde sem conhecimento adicional\n",
    "2. **Configura√ß√£o do Azure AI Search**: Prepara√ß√£o do √≠ndice de busca vetorial\n",
    "3. **Implementa√ß√£o do RAG**: Integra√ß√£o da busca com a gera√ß√£o de respostas\n",
    "4. **Compara√ß√£o de resultados**: Demonstra√ß√£o da diferen√ßa entre respostas com e sem RAG\n",
    "\n",
    "O primeiro passo √© a valida√ß√£o da configura√ß√£o das vari√°veis de ambiente no arquivo `.env` presente na raiz do reposit√≥rio.\n",
    "\n",
    "Preencha os valores das vari√°veis de acordo com o solicitado, incluindo as credenciais do Azure AI Search.\n",
    "\n",
    "### Exerc√≠cio 1 - Configura√ß√£o e Importa√ß√£o de Bibliotecas\n",
    "\n",
    "Vamos realizar a importa√ß√£o das bibliotecas necess√°rias para o laborat√≥rio de RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05cff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv(dotenv_path=\"../../../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f828b863",
   "metadata": {},
   "source": [
    "Vamos carregar as credenciais em vari√°veis para facilitar o uso no laborat√≥rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b55a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure OpenAI Configuration\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "api_version = os.getenv(\"API_VERSION\")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "embedding_model = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    "\n",
    "# Azure AI Search Configuration\n",
    "search_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "search_key = os.getenv(\"AZURE_SEARCH_KEY\")\n",
    "search_index = os.getenv(\"AZURE_SEARCH_INDEX\", \"rag-index\")\n",
    "\n",
    "print(\"Configura√ß√µes carregadas:\")\n",
    "print(f\"Azure OpenAI Endpoint: {azure_endpoint}\")\n",
    "print(f\"Azure Search Endpoint: {search_endpoint}\")\n",
    "print(f\"Search Index: {search_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1c1ac",
   "metadata": {},
   "source": [
    "Agora vamos inicializar os clientes do Azure OpenAI e Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f51258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI client\n",
    "openai_client = AzureOpenAI(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_key=api_key,\n",
    "    api_version=api_version\n",
    ")\n",
    "\n",
    "# Initialize Azure AI Search client\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=search_index,\n",
    "    credential=AzureKeyCredential(search_key)\n",
    ")\n",
    "\n",
    "print(\"Clientes inicializados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b31a9",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 2 - Consulta Sem RAG (Baseline)\n",
    "\n",
    "Primeiro, vamos fazer uma pergunta espec√≠fica sobre um t√≥pico que provavelmente n√£o est√° no conhecimento base da LLM ou est√° desatualizado. Isso nos permitir√° comparar a qualidade das respostas antes e depois da implementa√ß√£o do RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded74446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pergunta espec√≠fica sobre um t√≥pico que pode n√£o estar no conhecimento da LLM\n",
    "question = \"Quais s√£o as principais funcionalidades do Azure AI Foundry lan√ßadas em 2024?\"\n",
    "\n",
    "def query_without_rag(question):\n",
    "    \"\"\"Faz uma consulta direta ao modelo sem usar RAG\"\"\"\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Voc√™ √© um assistente √∫til especializado em tecnologias Azure.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Consulta sem RAG\n",
    "print(\"=== RESPOSTA SEM RAG ===\")\n",
    "response_without_rag = query_without_rag(question)\n",
    "print(response_without_rag)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb0f939",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 3 - Prepara√ß√£o de Dados para RAG\n",
    "\n",
    "Vamos criar alguns documentos de exemplo sobre Azure AI Foundry para simular uma base de conhecimento. Em um cen√°rio real, estes dados viriam de documenta√ß√£o oficial, manuais, ou outros reposit√≥rios de conhecimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0c5adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de exemplo sobre Azure AI Foundry\n",
    "sample_documents = [\n",
    "    {\n",
    "        \"id\": \"1\",\n",
    "        \"title\": \"Azure AI Foundry - Vis√£o Geral\",\n",
    "        \"content\": \"\"\"Azure AI Foundry √© uma plataforma unificada para desenvolvimento de aplica√ß√µes de IA. \n",
    "        Lan√ßado em 2024, oferece ferramentas integradas para construir, treinar e implementar modelos de IA. \n",
    "        Inclui suporte nativo para RAG, function calling, e integra√ß√£o com Azure AI Search. \n",
    "        A plataforma permite colabora√ß√£o entre equipes e governan√ßa centralizada de modelos.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"2\", \n",
    "        \"title\": \"Funcionalidades do Azure AI Foundry 2024\",\n",
    "        \"content\": \"\"\"As principais funcionalidades do Azure AI Foundry incluem: \n",
    "        1) Model Catalog com mais de 200 modelos pr√©-treinados\n",
    "        2) Prompt Flow para orquestra√ß√£o de workflows de IA\n",
    "        3) Azure AI Search integrado para implementa√ß√µes RAG\n",
    "        4) Evaluation tools para m√©tricas de qualidade\n",
    "        5) Responsible AI dashboard para monitoramento √©tico\n",
    "        6) Multi-cloud deployment support\n",
    "        7) Real-time inference endpoints com auto-scaling\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"3\",\n",
    "        \"title\": \"RAG no Azure AI Foundry\", \n",
    "        \"content\": \"\"\"O Azure AI Foundry oferece suporte nativo para Retrieval-Augmented Generation (RAG). \n",
    "        Permite conectar facilmente com Azure AI Search, Cosmos DB, e outras fontes de dados. \n",
    "        Inclui ferramentas visuais para configurar pipelines RAG sem c√≥digo. \n",
    "        Suporta embeddings personalizados e m√∫ltiplos tipos de retrieval como h√≠brido e sem√¢ntico.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"4\",\n",
    "        \"title\": \"Azure AI Search Integration\",\n",
    "        \"content\": \"\"\"A integra√ß√£o com Azure AI Search permite busca vetorial avan√ßada com suporte a:\n",
    "        - Busca h√≠brida (keyword + sem√¢ntica)\n",
    "        - Filtros de metadados\n",
    "        - Re-ranking sem√¢ntico \n",
    "        - M√∫ltiplos algoritmos de similaridade (cosine, dot product, euclidean)\n",
    "        - Indexa√ß√£o autom√°tica de documentos\n",
    "        - Suporte a mais de 300 formatos de arquivo\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Preparados {len(sample_documents)} documentos de exemplo\")\n",
    "for doc in sample_documents:\n",
    "    print(f\"- {doc['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a33d36c",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 4 - Gera√ß√£o de Embeddings\n",
    "\n",
    "Agora vamos gerar embeddings para nossos documentos usando o Azure OpenAI. Os embeddings s√£o representa√ß√µes vetoriais dos documentos que permitem busca sem√¢ntica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb805ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    \"\"\"Gera embedding para um texto usando Azure OpenAI\"\"\"\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=text,\n",
    "        model=embedding_model\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Gerar embeddings para todos os documentos\n",
    "print(\"Gerando embeddings para os documentos...\")\n",
    "for doc in sample_documents:\n",
    "    # Combinar t√≠tulo e conte√∫do para o embedding\n",
    "    full_text = f\"{doc['title']} {doc['content']}\"\n",
    "    doc['embedding'] = get_embedding(full_text)\n",
    "    print(f\"‚úì Embedding gerado para: {doc['title']}\")\n",
    "\n",
    "print(f\"\\nEmbeddings gerados! Dimens√£o: {len(sample_documents[0]['embedding'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9580072",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 5 - Busca Sem√¢ntica (Simulada)\n",
    "\n",
    "Como n√£o temos um √≠ndice real do Azure AI Search configurado, vamos simular a busca sem√¢ntica calculando a similaridade entre embeddings. Em um ambiente real, o Azure AI Search faria isso automaticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3c61ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calcula a similaridade coseno entre dois vetores\"\"\"\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def semantic_search(query, documents, top_k=2):\n",
    "    \"\"\"Realiza busca sem√¢ntica nos documentos\"\"\"\n",
    "    # Gerar embedding da consulta\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Calcular similaridades\n",
    "    similarities = []\n",
    "    for doc in documents:\n",
    "        similarity = cosine_similarity(query_embedding, doc['embedding'])\n",
    "        similarities.append({\n",
    "            'document': doc,\n",
    "            'similarity': similarity\n",
    "        })\n",
    "    \n",
    "    # Ordenar por similaridade (maior primeiro)\n",
    "    similarities.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Teste da busca sem√¢ntica\n",
    "print(\"=== BUSCA SEM√ÇNTICA ===\")\n",
    "results = semantic_search(question, sample_documents)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    doc = result['document']\n",
    "    similarity = result['similarity']\n",
    "    print(f\"{i}. {doc['title']} (Similaridade: {similarity:.3f})\")\n",
    "    print(f\"   Conte√∫do: {doc['content'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c69f8",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 6 - Implementa√ß√£o do RAG\n",
    "\n",
    "Agora vamos implementar o sistema RAG completo, combinando a busca sem√¢ntica com a gera√ß√£o de respostas pelo modelo de linguagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4abb6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_rag(question, documents):\n",
    "    \"\"\"Implementa RAG: busca documentos relevantes e gera resposta baseada no contexto\"\"\"\n",
    "    \n",
    "    # 1. Recupera√ß√£o (Retrieval): Buscar documentos relevantes\n",
    "    search_results = semantic_search(question, documents, top_k=2)\n",
    "    \n",
    "    # 2. Construir contexto com os documentos recuperados\n",
    "    context_parts = []\n",
    "    for result in search_results:\n",
    "        doc = result['document']\n",
    "        context_parts.append(f\"Documento: {doc['title']}\\nConte√∫do: {doc['content']}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # 3. Construir prompt com contexto\n",
    "    system_message = \"\"\"Voc√™ √© um assistente especializado em tecnologias Azure. \n",
    "    Use APENAS as informa√ß√µes fornecidas no contexto para responder √† pergunta. \n",
    "    Se a informa√ß√£o n√£o estiver no contexto, diga que n√£o possui essa informa√ß√£o espec√≠fica.\n",
    "    Seja preciso e cite informa√ß√µes espec√≠ficas do contexto quando poss√≠vel.\"\"\"\n",
    "    \n",
    "    user_message = f\"\"\"Contexto:\n",
    "{context}\n",
    "\n",
    "Pergunta: {question}\n",
    "\n",
    "Resposta baseada no contexto fornecido:\"\"\"\n",
    "    \n",
    "    # 4. Gera√ß√£o (Augmented Generation): Gerar resposta com contexto\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        temperature=0.3  # Menor temperatura para respostas mais precisas\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'answer': response.choices[0].message.content,\n",
    "        'sources': [result['document']['title'] for result in search_results],\n",
    "        'context': context\n",
    "    }\n",
    "\n",
    "# Executar RAG com a mesma pergunta\n",
    "print(\"=== RESPOSTA COM RAG ===\")\n",
    "rag_result = query_with_rag(question, sample_documents)\n",
    "\n",
    "print(\"Resposta:\")\n",
    "print(rag_result['answer'])\n",
    "print(f\"\\nFontes utilizadas: {', '.join(rag_result['sources'])}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a2df98",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 7 - Compara√ß√£o de Resultados\n",
    "\n",
    "Vamos comparar diretamente as respostas obtidas sem RAG e com RAG para evidenciar as diferen√ßas e benef√≠cios do uso de conhecimento espec√≠fico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43508797",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç AN√ÅLISE COMPARATIVA: RAG vs Sem RAG\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìù PERGUNTA:\")\n",
    "print(f'\"{question}\"')\n",
    "\n",
    "print(\"\\n‚ùå RESPOSTA SEM RAG (conhecimento limitado):\")\n",
    "print(\"-\" * 50)\n",
    "print(response_without_rag)\n",
    "\n",
    "print(\"\\n‚úÖ RESPOSTA COM RAG (conhecimento aumentado):\")\n",
    "print(\"-\" * 50)\n",
    "print(rag_result['answer'])\n",
    "\n",
    "print(f\"\\nüìö FONTES UTILIZADAS NO RAG:\")\n",
    "for source in rag_result['sources']:\n",
    "    print(f\"‚Ä¢ {source}\")\n",
    "\n",
    "print(\"\\nüí° BENEF√çCIOS OBSERVADOS DO RAG:\")\n",
    "benefits = [\n",
    "    \"‚úì Informa√ß√µes mais espec√≠ficas e atualizadas\",\n",
    "    \"‚úì Respostas baseadas em fontes confi√°veis\",\n",
    "    \"‚úì Maior precis√£o em detalhes t√©cnicos\", \n",
    "    \"‚úì Rastreabilidade das informa√ß√µes (sources)\",\n",
    "    \"‚úì Redu√ß√£o de alucina√ß√µes do modelo\"\n",
    "]\n",
    "\n",
    "for benefit in benefits:\n",
    "    print(benefit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185254c1",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 8 - RAG com Azure AI Search (Exemplo Real)\n",
    "\n",
    "Embora tenhamos simulado a busca sem√¢ntica, vamos mostrar como seria a implementa√ß√£o real usando Azure AI Search. Este c√≥digo demonstra como conectar com um √≠ndice real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a6a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_azure_ai_search(question, search_client, openai_client):\n",
    "    \"\"\"\n",
    "    Implementa√ß√£o real de RAG usando Azure AI Search\n",
    "    Nota: Este c√≥digo requer um √≠ndice configurado no Azure AI Search\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Gerar embedding da pergunta\n",
    "        query_embedding = get_embedding(question)\n",
    "        \n",
    "        # 2. Criar consulta vetorizada para Azure AI Search\n",
    "        vector_query = VectorizedQuery(\n",
    "            vector=query_embedding,\n",
    "            k_nearest_neighbors=3,  # Top 3 resultados mais similares\n",
    "            fields=\"content_vector\"  # Campo que cont√©m os embeddings\n",
    "        )\n",
    "        \n",
    "        # 3. Executar busca no Azure AI Search\n",
    "        search_results = search_client.search(\n",
    "            search_text=question,  # Busca h√≠brida: texto + vetorial\n",
    "            vector_queries=[vector_query],\n",
    "            top=3,\n",
    "            include_total_count=True\n",
    "        )\n",
    "        \n",
    "        # 4. Extrair contexto dos resultados\n",
    "        context_parts = []\n",
    "        sources = []\n",
    "        \n",
    "        for result in search_results:\n",
    "            context_parts.append(f\"T√≠tulo: {result['title']}\\\\nConte√∫do: {result['content']}\")\n",
    "            sources.append(result['title'])\n",
    "        \n",
    "        context = \"\\\\n\\\\n\".join(context_parts)\n",
    "        \n",
    "        # 5. Gerar resposta com contexto\n",
    "        system_message = \"\"\"Voc√™ √© um assistente especializado em Azure. \n",
    "        Use APENAS as informa√ß√µes do contexto fornecido para responder.\"\"\"\n",
    "        \n",
    "        user_message = f\"\"\"Contexto:\\\\n{context}\\\\n\\\\nPergunta: {question}\\\\n\\\\nResposta:\"\"\"\n",
    "        \n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=deployment_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'answer': response.choices[0].message.content,\n",
    "            'sources': sources,\n",
    "            'context': context\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'answer': f\"Erro ao conectar com Azure AI Search: {str(e)}\",\n",
    "            'sources': [],\n",
    "            'context': \"\"\n",
    "        }\n",
    "\n",
    "# Exemplo de uso (comentado pois requer √≠ndice configurado)\n",
    "print(\"üìã C√ìDIGO PARA AZURE AI SEARCH REAL:\")\n",
    "print(\"# Para usar este c√≥digo, voc√™ precisa:\")\n",
    "print(\"# 1. Criar um √≠ndice no Azure AI Search\")\n",
    "print(\"# 2. Configurar campos de embedding\") \n",
    "print(\"# 3. Indexar seus documentos\")\n",
    "print(\"# 4. Executar: query_with_azure_ai_search(question, search_client, openai_client)\")\n",
    "\n",
    "# result = query_with_azure_ai_search(question, search_client, openai_client)\n",
    "# print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5712cc",
   "metadata": {},
   "source": [
    "### Exerc√≠cio 9 - Teste Interativo\n",
    "\n",
    "Agora voc√™ pode testar o sistema RAG com suas pr√≥prias perguntas! Experimente diferentes tipos de consultas para ver como o RAG se comporta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f25f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste suas pr√≥prias perguntas aqui!\n",
    "test_questions = [\n",
    "    \"Como o Azure AI Foundry suporta RAG?\",\n",
    "    \"Quantos modelos est√£o dispon√≠veis no Model Catalog?\",\n",
    "    \"Quais s√£o os tipos de busca suportados pelo Azure AI Search?\",\n",
    "    \"O que √© o Prompt Flow no Azure AI Foundry?\"\n",
    "]\n",
    "\n",
    "print(\"üß™ TESTE INTERATIVO - Experimente diferentes perguntas:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for test_q in test_questions:\n",
    "    print(f\"\\n‚ùì Pergunta: {test_q}\")\n",
    "    \n",
    "    # Resposta sem RAG\n",
    "    no_rag = query_without_rag(test_q)\n",
    "    print(f\"‚ùå Sem RAG: {no_rag[:150]}...\")\n",
    "    \n",
    "    # Resposta com RAG\n",
    "    with_rag = query_with_rag(test_q, sample_documents)\n",
    "    print(f\"‚úÖ Com RAG: {with_rag['answer'][:150]}...\")\n",
    "    print(f\"üìö Fontes: {', '.join(with_rag['sources'])}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nüí° Experimente criar suas pr√≥prias perguntas modificando a lista 'test_questions'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78809d4e",
   "metadata": {},
   "source": [
    "## Conclus√£o e Pr√≥ximos Passos\n",
    "\n",
    "Neste laborat√≥rio exploramos o conceito e implementa√ß√£o de RAG (Retrieval-Augmented Generation), demonstrando como:\n",
    "\n",
    "### ‚úÖ O que aprendemos:\n",
    "1. **Diferen√ßa fundamental**: Como RAG melhora significativamente a qualidade das respostas\n",
    "2. **Embedding Generation**: Convers√£o de texto em representa√ß√µes vetoriais\n",
    "3. **Busca Sem√¢ntica**: Encontrar documentos relevantes usando similaridade de embeddings\n",
    "4. **Augmented Generation**: Combinar contexto recuperado com gera√ß√£o de texto\n",
    "5. **Implementa√ß√£o pr√°tica**: C√≥digo funcional para sistema RAG completo\n",
    "\n",
    "### üöÄ Benef√≠cios do RAG demonstrados:\n",
    "- **Conhecimento atualizado**: Acesso a informa√ß√µes espec√≠ficas e recentes\n",
    "- **Redu√ß√£o de alucina√ß√µes**: Respostas baseadas em fontes confi√°veis  \n",
    "- **Rastreabilidade**: Capacidade de identificar fontes das informa√ß√µes\n",
    "- **Especializa√ß√£o**: Respostas mais precisas sobre t√≥picos espec√≠ficos\n",
    "- **Flexibilidade**: F√°cil atualiza√ß√£o da base de conhecimento\n",
    "\n",
    "### üîß Para implementa√ß√£o em produ√ß√£o:\n",
    "1. **Configure Azure AI Search** com √≠ndices vetoriais otimizados\n",
    "2. **Implemente chunking** para documentos grandes (512-1024 tokens)\n",
    "3. **Use busca h√≠brida** (keyword + sem√¢ntica) para melhor recall\n",
    "4. **Adicione re-ranking** para melhorar a relev√¢ncia dos resultados\n",
    "5. **Monitore performance** e ajuste par√¢metros (top_k, temperature, etc.)\n",
    "6. **Implemente cache** para consultas frequentes\n",
    "7. **Configure governan√ßa** para valida√ß√£o de fontes\n",
    "\n",
    "### üìö Recursos adicionais:\n",
    "- [Azure AI Search Vector Search](https://learn.microsoft.com/en-us/azure/search/vector-search-overview)\n",
    "- [Azure OpenAI RAG Patterns](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/use-your-data)\n",
    "- [Azure AI Foundry Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/)\n",
    "- [RAG Best Practices](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-usage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
