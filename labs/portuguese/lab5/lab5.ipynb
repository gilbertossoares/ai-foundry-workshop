{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b6954ca",
   "metadata": {},
   "source": [
    "# Azure AI Foundry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bc5272",
   "metadata": {},
   "source": [
    "<center><img src=\"../../../images/Azure-AI-Foundry_1600x900.jpg\" alt=\"Azure AI Foundry\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548c1279",
   "metadata": {},
   "source": [
    "## Laboratório 5 - RAG (Retrieval-Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d37a39",
   "metadata": {},
   "source": [
    "Neste laboratório iremos implementar um sistema RAG (Retrieval-Augmented Generation) usando Azure AI Search e Azure OpenAI. O RAG é uma técnica que combina recuperação de informações com geração de texto, permitindo que os modelos de linguagem acessem conhecimentos específicos externos ao seu treinamento.\n",
    "\n",
    "Vamos explorar:\n",
    "1. **Consulta sem RAG**: Como a LLM responde sem conhecimento adicional\n",
    "2. **Configuração do Azure AI Search**: Preparação do índice de busca vetorial\n",
    "3. **Implementação do RAG**: Integração da busca com a geração de respostas\n",
    "4. **Comparação de resultados**: Demonstração da diferença entre respostas com e sem RAG\n",
    "\n",
    "O primeiro passo é a validação da configuração das variáveis de ambiente no arquivo `.env` presente na raiz do repositório.\n",
    "\n",
    "Preencha os valores das variáveis de acordo com o solicitado, incluindo as credenciais do Azure AI Search.\n",
    "\n",
    "### Exercício 1 - Configuração e Importação de Bibliotecas\n",
    "\n",
    "Vamos realizar a importação das bibliotecas necessárias para o laboratório de RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05cff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv(dotenv_path=\"../../../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f828b863",
   "metadata": {},
   "source": [
    "Vamos carregar as credenciais em variáveis para facilitar o uso no laboratório."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b55a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure OpenAI Configuration\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "api_version = os.getenv(\"API_VERSION\")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "embedding_model = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    "\n",
    "# Azure AI Search Configuration\n",
    "search_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "search_key = os.getenv(\"AZURE_SEARCH_KEY\")\n",
    "search_index = os.getenv(\"AZURE_SEARCH_INDEX\", \"rag-index\")\n",
    "\n",
    "print(\"Configurações carregadas:\")\n",
    "print(f\"Azure OpenAI Endpoint: {azure_endpoint}\")\n",
    "print(f\"Azure Search Endpoint: {search_endpoint}\")\n",
    "print(f\"Search Index: {search_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1c1ac",
   "metadata": {},
   "source": [
    "Agora vamos inicializar os clientes do Azure OpenAI e Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f51258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI client\n",
    "openai_client = AzureOpenAI(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_key=api_key,\n",
    "    api_version=api_version\n",
    ")\n",
    "\n",
    "# Initialize Azure AI Search client\n",
    "search_client = SearchClient(\n",
    "    endpoint=search_endpoint,\n",
    "    index_name=search_index,\n",
    "    credential=AzureKeyCredential(search_key)\n",
    ")\n",
    "\n",
    "print(\"Clientes inicializados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b31a9",
   "metadata": {},
   "source": [
    "### Exercício 2 - Consulta Sem RAG (Baseline)\n",
    "\n",
    "Primeiro, vamos fazer uma pergunta específica sobre um tópico que provavelmente não está no conhecimento base da LLM ou está desatualizado. Isso nos permitirá comparar a qualidade das respostas antes e depois da implementação do RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded74446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pergunta específica sobre um tópico que pode não estar no conhecimento da LLM\n",
    "question = \"Quais são as principais funcionalidades do Azure AI Foundry lançadas em 2024?\"\n",
    "\n",
    "def query_without_rag(question):\n",
    "    \"\"\"Faz uma consulta direta ao modelo sem usar RAG\"\"\"\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Você é um assistente útil especializado em tecnologias Azure.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Consulta sem RAG\n",
    "print(\"=== RESPOSTA SEM RAG ===\")\n",
    "response_without_rag = query_without_rag(question)\n",
    "print(response_without_rag)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb0f939",
   "metadata": {},
   "source": [
    "### Exercício 3 - Preparação de Dados para RAG\n",
    "\n",
    "Vamos criar alguns documentos de exemplo sobre Azure AI Foundry para simular uma base de conhecimento. Em um cenário real, estes dados viriam de documentação oficial, manuais, ou outros repositórios de conhecimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0c5adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de exemplo sobre Azure AI Foundry\n",
    "sample_documents = [\n",
    "    {\n",
    "        \"id\": \"1\",\n",
    "        \"title\": \"Azure AI Foundry - Visão Geral\",\n",
    "        \"content\": \"\"\"Azure AI Foundry é uma plataforma unificada para desenvolvimento de aplicações de IA. \n",
    "        Lançado em 2024, oferece ferramentas integradas para construir, treinar e implementar modelos de IA. \n",
    "        Inclui suporte nativo para RAG, function calling, e integração com Azure AI Search. \n",
    "        A plataforma permite colaboração entre equipes e governança centralizada de modelos.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"2\", \n",
    "        \"title\": \"Funcionalidades do Azure AI Foundry 2024\",\n",
    "        \"content\": \"\"\"As principais funcionalidades do Azure AI Foundry incluem: \n",
    "        1) Model Catalog com mais de 200 modelos pré-treinados\n",
    "        2) Prompt Flow para orquestração de workflows de IA\n",
    "        3) Azure AI Search integrado para implementações RAG\n",
    "        4) Evaluation tools para métricas de qualidade\n",
    "        5) Responsible AI dashboard para monitoramento ético\n",
    "        6) Multi-cloud deployment support\n",
    "        7) Real-time inference endpoints com auto-scaling\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"3\",\n",
    "        \"title\": \"RAG no Azure AI Foundry\", \n",
    "        \"content\": \"\"\"O Azure AI Foundry oferece suporte nativo para Retrieval-Augmented Generation (RAG). \n",
    "        Permite conectar facilmente com Azure AI Search, Cosmos DB, e outras fontes de dados. \n",
    "        Inclui ferramentas visuais para configurar pipelines RAG sem código. \n",
    "        Suporta embeddings personalizados e múltiplos tipos de retrieval como híbrido e semântico.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"4\",\n",
    "        \"title\": \"Azure AI Search Integration\",\n",
    "        \"content\": \"\"\"A integração com Azure AI Search permite busca vetorial avançada com suporte a:\n",
    "        - Busca híbrida (keyword + semântica)\n",
    "        - Filtros de metadados\n",
    "        - Re-ranking semântico \n",
    "        - Múltiplos algoritmos de similaridade (cosine, dot product, euclidean)\n",
    "        - Indexação automática de documentos\n",
    "        - Suporte a mais de 300 formatos de arquivo\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Preparados {len(sample_documents)} documentos de exemplo\")\n",
    "for doc in sample_documents:\n",
    "    print(f\"- {doc['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a33d36c",
   "metadata": {},
   "source": [
    "### Exercício 4 - Geração de Embeddings\n",
    "\n",
    "Agora vamos gerar embeddings para nossos documentos usando o Azure OpenAI. Os embeddings são representações vetoriais dos documentos que permitem busca semântica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb805ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    \"\"\"Gera embedding para um texto usando Azure OpenAI\"\"\"\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=text,\n",
    "        model=embedding_model\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Gerar embeddings para todos os documentos\n",
    "print(\"Gerando embeddings para os documentos...\")\n",
    "for doc in sample_documents:\n",
    "    # Combinar título e conteúdo para o embedding\n",
    "    full_text = f\"{doc['title']} {doc['content']}\"\n",
    "    doc['embedding'] = get_embedding(full_text)\n",
    "    print(f\"✓ Embedding gerado para: {doc['title']}\")\n",
    "\n",
    "print(f\"\\nEmbeddings gerados! Dimensão: {len(sample_documents[0]['embedding'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9580072",
   "metadata": {},
   "source": [
    "### Exercício 5 - Busca Semântica (Simulada)\n",
    "\n",
    "Como não temos um índice real do Azure AI Search configurado, vamos simular a busca semântica calculando a similaridade entre embeddings. Em um ambiente real, o Azure AI Search faria isso automaticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3c61ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calcula a similaridade coseno entre dois vetores\"\"\"\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def semantic_search(query, documents, top_k=2):\n",
    "    \"\"\"Realiza busca semântica nos documentos\"\"\"\n",
    "    # Gerar embedding da consulta\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Calcular similaridades\n",
    "    similarities = []\n",
    "    for doc in documents:\n",
    "        similarity = cosine_similarity(query_embedding, doc['embedding'])\n",
    "        similarities.append({\n",
    "            'document': doc,\n",
    "            'similarity': similarity\n",
    "        })\n",
    "    \n",
    "    # Ordenar por similaridade (maior primeiro)\n",
    "    similarities.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Teste da busca semântica\n",
    "print(\"=== BUSCA SEMÂNTICA ===\")\n",
    "results = semantic_search(question, sample_documents)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    doc = result['document']\n",
    "    similarity = result['similarity']\n",
    "    print(f\"{i}. {doc['title']} (Similaridade: {similarity:.3f})\")\n",
    "    print(f\"   Conteúdo: {doc['content'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c69f8",
   "metadata": {},
   "source": [
    "### Exercício 6 - Implementação do RAG\n",
    "\n",
    "Agora vamos implementar o sistema RAG completo, combinando a busca semântica com a geração de respostas pelo modelo de linguagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4abb6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_rag(question, documents):\n",
    "    \"\"\"Implementa RAG: busca documentos relevantes e gera resposta baseada no contexto\"\"\"\n",
    "    \n",
    "    # 1. Recuperação (Retrieval): Buscar documentos relevantes\n",
    "    search_results = semantic_search(question, documents, top_k=2)\n",
    "    \n",
    "    # 2. Construir contexto com os documentos recuperados\n",
    "    context_parts = []\n",
    "    for result in search_results:\n",
    "        doc = result['document']\n",
    "        context_parts.append(f\"Documento: {doc['title']}\\nConteúdo: {doc['content']}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # 3. Construir prompt com contexto\n",
    "    system_message = \"\"\"Você é um assistente especializado em tecnologias Azure. \n",
    "    Use APENAS as informações fornecidas no contexto para responder à pergunta. \n",
    "    Se a informação não estiver no contexto, diga que não possui essa informação específica.\n",
    "    Seja preciso e cite informações específicas do contexto quando possível.\"\"\"\n",
    "    \n",
    "    user_message = f\"\"\"Contexto:\n",
    "{context}\n",
    "\n",
    "Pergunta: {question}\n",
    "\n",
    "Resposta baseada no contexto fornecido:\"\"\"\n",
    "    \n",
    "    # 4. Geração (Augmented Generation): Gerar resposta com contexto\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        temperature=0.3  # Menor temperatura para respostas mais precisas\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'answer': response.choices[0].message.content,\n",
    "        'sources': [result['document']['title'] for result in search_results],\n",
    "        'context': context\n",
    "    }\n",
    "\n",
    "# Executar RAG com a mesma pergunta\n",
    "print(\"=== RESPOSTA COM RAG ===\")\n",
    "rag_result = query_with_rag(question, sample_documents)\n",
    "\n",
    "print(\"Resposta:\")\n",
    "print(rag_result['answer'])\n",
    "print(f\"\\nFontes utilizadas: {', '.join(rag_result['sources'])}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a2df98",
   "metadata": {},
   "source": [
    "### Exercício 7 - Comparação de Resultados\n",
    "\n",
    "Vamos comparar diretamente as respostas obtidas sem RAG e com RAG para evidenciar as diferenças e benefícios do uso de conhecimento específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43508797",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 ANÁLISE COMPARATIVA: RAG vs Sem RAG\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n📝 PERGUNTA:\")\n",
    "print(f'\"{question}\"')\n",
    "\n",
    "print(\"\\n❌ RESPOSTA SEM RAG (conhecimento limitado):\")\n",
    "print(\"-\" * 50)\n",
    "print(response_without_rag)\n",
    "\n",
    "print(\"\\n✅ RESPOSTA COM RAG (conhecimento aumentado):\")\n",
    "print(\"-\" * 50)\n",
    "print(rag_result['answer'])\n",
    "\n",
    "print(f\"\\n📚 FONTES UTILIZADAS NO RAG:\")\n",
    "for source in rag_result['sources']:\n",
    "    print(f\"• {source}\")\n",
    "\n",
    "print(\"\\n💡 BENEFÍCIOS OBSERVADOS DO RAG:\")\n",
    "benefits = [\n",
    "    \"✓ Informações mais específicas e atualizadas\",\n",
    "    \"✓ Respostas baseadas em fontes confiáveis\",\n",
    "    \"✓ Maior precisão em detalhes técnicos\", \n",
    "    \"✓ Rastreabilidade das informações (sources)\",\n",
    "    \"✓ Redução de alucinações do modelo\"\n",
    "]\n",
    "\n",
    "for benefit in benefits:\n",
    "    print(benefit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185254c1",
   "metadata": {},
   "source": [
    "### Exercício 8 - RAG com Azure AI Search (Exemplo Real)\n",
    "\n",
    "Embora tenhamos simulado a busca semântica, vamos mostrar como seria a implementação real usando Azure AI Search. Este código demonstra como conectar com um índice real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a6a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_azure_ai_search(question, search_client, openai_client):\n",
    "    \"\"\"\n",
    "    Implementação real de RAG usando Azure AI Search\n",
    "    Nota: Este código requer um índice configurado no Azure AI Search\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Gerar embedding da pergunta\n",
    "        query_embedding = get_embedding(question)\n",
    "        \n",
    "        # 2. Criar consulta vetorizada para Azure AI Search\n",
    "        vector_query = VectorizedQuery(\n",
    "            vector=query_embedding,\n",
    "            k_nearest_neighbors=3,  # Top 3 resultados mais similares\n",
    "            fields=\"content_vector\"  # Campo que contém os embeddings\n",
    "        )\n",
    "        \n",
    "        # 3. Executar busca no Azure AI Search\n",
    "        search_results = search_client.search(\n",
    "            search_text=question,  # Busca híbrida: texto + vetorial\n",
    "            vector_queries=[vector_query],\n",
    "            top=3,\n",
    "            include_total_count=True\n",
    "        )\n",
    "        \n",
    "        # 4. Extrair contexto dos resultados\n",
    "        context_parts = []\n",
    "        sources = []\n",
    "        \n",
    "        for result in search_results:\n",
    "            context_parts.append(f\"Título: {result['title']}\\\\nConteúdo: {result['content']}\")\n",
    "            sources.append(result['title'])\n",
    "        \n",
    "        context = \"\\\\n\\\\n\".join(context_parts)\n",
    "        \n",
    "        # 5. Gerar resposta com contexto\n",
    "        system_message = \"\"\"Você é um assistente especializado em Azure. \n",
    "        Use APENAS as informações do contexto fornecido para responder.\"\"\"\n",
    "        \n",
    "        user_message = f\"\"\"Contexto:\\\\n{context}\\\\n\\\\nPergunta: {question}\\\\n\\\\nResposta:\"\"\"\n",
    "        \n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=deployment_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'answer': response.choices[0].message.content,\n",
    "            'sources': sources,\n",
    "            'context': context\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'answer': f\"Erro ao conectar com Azure AI Search: {str(e)}\",\n",
    "            'sources': [],\n",
    "            'context': \"\"\n",
    "        }\n",
    "\n",
    "# Exemplo de uso (comentado pois requer índice configurado)\n",
    "print(\"📋 CÓDIGO PARA AZURE AI SEARCH REAL:\")\n",
    "print(\"# Para usar este código, você precisa:\")\n",
    "print(\"# 1. Criar um índice no Azure AI Search\")\n",
    "print(\"# 2. Configurar campos de embedding\") \n",
    "print(\"# 3. Indexar seus documentos\")\n",
    "print(\"# 4. Executar: query_with_azure_ai_search(question, search_client, openai_client)\")\n",
    "\n",
    "# result = query_with_azure_ai_search(question, search_client, openai_client)\n",
    "# print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5712cc",
   "metadata": {},
   "source": [
    "### Exercício 9 - Teste Interativo\n",
    "\n",
    "Agora você pode testar o sistema RAG com suas próprias perguntas! Experimente diferentes tipos de consultas para ver como o RAG se comporta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f25f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste suas próprias perguntas aqui!\n",
    "test_questions = [\n",
    "    \"Como o Azure AI Foundry suporta RAG?\",\n",
    "    \"Quantos modelos estão disponíveis no Model Catalog?\",\n",
    "    \"Quais são os tipos de busca suportados pelo Azure AI Search?\",\n",
    "    \"O que é o Prompt Flow no Azure AI Foundry?\"\n",
    "]\n",
    "\n",
    "print(\"🧪 TESTE INTERATIVO - Experimente diferentes perguntas:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for test_q in test_questions:\n",
    "    print(f\"\\n❓ Pergunta: {test_q}\")\n",
    "    \n",
    "    # Resposta sem RAG\n",
    "    no_rag = query_without_rag(test_q)\n",
    "    print(f\"❌ Sem RAG: {no_rag[:150]}...\")\n",
    "    \n",
    "    # Resposta com RAG\n",
    "    with_rag = query_with_rag(test_q, sample_documents)\n",
    "    print(f\"✅ Com RAG: {with_rag['answer'][:150]}...\")\n",
    "    print(f\"📚 Fontes: {', '.join(with_rag['sources'])}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\n💡 Experimente criar suas próprias perguntas modificando a lista 'test_questions'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78809d4e",
   "metadata": {},
   "source": [
    "## Conclusão e Próximos Passos\n",
    "\n",
    "Neste laboratório exploramos o conceito e implementação de RAG (Retrieval-Augmented Generation), demonstrando como:\n",
    "\n",
    "### ✅ O que aprendemos:\n",
    "1. **Diferença fundamental**: Como RAG melhora significativamente a qualidade das respostas\n",
    "2. **Embedding Generation**: Conversão de texto em representações vetoriais\n",
    "3. **Busca Semântica**: Encontrar documentos relevantes usando similaridade de embeddings\n",
    "4. **Augmented Generation**: Combinar contexto recuperado com geração de texto\n",
    "5. **Implementação prática**: Código funcional para sistema RAG completo\n",
    "\n",
    "### 🚀 Benefícios do RAG demonstrados:\n",
    "- **Conhecimento atualizado**: Acesso a informações específicas e recentes\n",
    "- **Redução de alucinações**: Respostas baseadas em fontes confiáveis  \n",
    "- **Rastreabilidade**: Capacidade de identificar fontes das informações\n",
    "- **Especialização**: Respostas mais precisas sobre tópicos específicos\n",
    "- **Flexibilidade**: Fácil atualização da base de conhecimento\n",
    "\n",
    "### 🔧 Para implementação em produção:\n",
    "1. **Configure Azure AI Search** com índices vetoriais otimizados\n",
    "2. **Implemente chunking** para documentos grandes (512-1024 tokens)\n",
    "3. **Use busca híbrida** (keyword + semântica) para melhor recall\n",
    "4. **Adicione re-ranking** para melhorar a relevância dos resultados\n",
    "5. **Monitore performance** e ajuste parâmetros (top_k, temperature, etc.)\n",
    "6. **Implemente cache** para consultas frequentes\n",
    "7. **Configure governança** para validação de fontes\n",
    "\n",
    "### 📚 Recursos adicionais:\n",
    "- [Azure AI Search Vector Search](https://learn.microsoft.com/en-us/azure/search/vector-search-overview)\n",
    "- [Azure OpenAI RAG Patterns](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/use-your-data)\n",
    "- [Azure AI Foundry Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/)\n",
    "- [RAG Best Practices](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-usage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
