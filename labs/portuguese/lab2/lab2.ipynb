{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2364a810",
   "metadata": {},
   "source": [
    "# Azure AI Foundry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a02bae",
   "metadata": {},
   "source": [
    "<center><img src=\"../../../images/Azure-AI-Foundry_1600x900.jpg\" alt=\"Azure AI Foundry\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a15a5",
   "metadata": {},
   "source": [
    "## Laboratório 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048dc4e5",
   "metadata": {},
   "source": [
    "Neste laboratório é explorar os serviços de AI presentes no Azure Foundry, este laboratório vai cobrir os seguintes serviços:\n",
    "- Speech\n",
    "- Language + Translator\n",
    "- Vision + Document \n",
    "- Content Safety\n",
    "\n",
    "Entendendo estes serviços podemos adicionar mais habilidades à nossas aplicações."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7617fed7",
   "metadata": {},
   "source": [
    "### Exercício 1 - Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f46c67f",
   "metadata": {},
   "source": [
    "O serviço Speech fornece recursos de conversão de fala para texto e texto para fala com um recurso Speech. Você pode transcrever fala para texto com alta precisão, produzir vozes naturais de texto para fala, traduzir áudio falado e usar reconhecimento de locutor durante conversas. Crie vozes personalizadas, adicione palavras específicas ao seu vocabulário base ou construa seus próprios modelos. Execute o Speech em qualquer lugar, na nuvem ou na borda em contêineres. É fácil habilitar fala em suas aplicações, ferramentas e dispositivos com a CLI do Speech, SDK do Speech e APIs REST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e176c",
   "metadata": {},
   "source": [
    "Cenários comunis para uso do speech:\n",
    "\n",
    "**Geração de legenda:** Aprenda como sincronizar legendas com seu áudio de entrada, aplicar filtros de palavrões, obter resultados parciais, aplicar personalizações e identificar idiomas falados para cenários multilíngues.\n",
    "\n",
    "**Criação de Conteúdo de Áudio:** Você pode usar vozes neurais para tornar as interações com chatbots e assistentes de voz mais naturais e envolventes, converter textos digitais como e-books em audiolivros e aprimorar sistemas de navegação automotiva.\n",
    "\n",
    "**Central de Atendimento:** Transcreva chamadas em tempo real ou processe um lote de chamadas, remova informações de identificação pessoal e extraia insights como análise de sentimento para auxiliar no seu caso de uso de central de atendimento.\n",
    "\n",
    "**Aprendizado de Idiomas:** Forneça feedback de avaliação de pronúncia para estudantes de idiomas, ofereça suporte à transcrição em tempo real para conversas de aprendizado remoto e leia materiais didáticos em voz alta usando vozes neurais.\n",
    "\n",
    "**Assistentes de Voz:** Crie interfaces conversacionais naturais e semelhantes às humanas para suas aplicações e experiências. O recurso de assistente de voz oferece interação rápida e confiável entre um dispositivo e uma implementação de assistente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dea14b",
   "metadata": {},
   "source": [
    "Para realizar este exercício verifique se no seu arquivo `.env` possui as seguintes variaveis preenchidas:\n",
    "- SPEECH_ENDPOINT \n",
    "- SPEECH_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0237903",
   "metadata": {},
   "source": [
    "Após verificar vamos iniciar carregando as bibliotecas necessárias, iniciando o cliente e realizando uma chamada para converter audio em texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63856ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856a708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependências necessárias\n",
    "# Execute esta célula apenas uma vez para instalar as bibliotecas necessárias\n",
    "\n",
    "# %pip install azure-cognitiveservices-speech\n",
    "# %pip install azure-ai-textanalytics\n",
    "# %pip install azure-cognitiveservices-vision-computervision\n",
    "# %pip install azure-ai-formrecognizer\n",
    "# %pip install azure-ai-contentsafety\n",
    "# %pip install requests\n",
    "# %pip install python-dotenv\n",
    "# %pip install msrest\n",
    "\n",
    "print(\"Para instalar as dependências, descomente as linhas acima e execute a célula.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75304711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "# Configurar o cliente Speech\n",
    "speech_key = os.getenv('SPEECH_KEY')\n",
    "speech_endpoint = os.getenv('SPEECH_ENDPOINT')\n",
    "speech_region = os.getenv('SPEECH_REGION')\n",
    "\n",
    "# Configurar o Speech SDK\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "speech_config.speech_recognition_language = \"pt-BR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35da9a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_config = speechsdk.audio.AudioConfig(filename=\"../../samples/audio001.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71278663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando transcrição do áudio...\n"
     ]
    }
   ],
   "source": [
    "speech_recognizer = None\n",
    "\n",
    "# Criar o SpeechRecognizer usando endpoint (solução para SPXERR_INVALID_HEADER)\n",
    "speech_config_endpoint = speechsdk.SpeechConfig(endpoint=speech_endpoint, subscription=speech_key)\n",
    "speech_config_endpoint.speech_recognition_language = \"pt-BR\"\n",
    "\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config_endpoint, audio_config=audio_config)\n",
    "\n",
    "print(\"Iniciando transcrição do áudio...\")\n",
    "\n",
    "result = speech_recognizer.recognize_once()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37caf3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nenhuma fala foi detectada no áudio.\n"
     ]
    }
   ],
   "source": [
    "if result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "    print(f\"Transcrição: {result.text}\")\n",
    "elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "    print(\"Nenhuma fala foi detectada no áudio.\")\n",
    "elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "    cancellation_details = result.cancellation_details\n",
    "    print(f\"Reconhecimento cancelado: {cancellation_details.reason}\")\n",
    "    if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "        print(f\"Erro: {cancellation_details.error_details}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d0478c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85f8f57",
   "metadata": {},
   "source": [
    "### Exercício 2 - Language + Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8423f00",
   "metadata": {},
   "source": [
    "Integre linguagem natural em aplicativos, bots e dispositivos IoT. Por exemplo, este serviço pode remover dados sensíveis, segmentar reuniões longas em capítulos, analisar registros de saúde e orquestrar bots conversacionais com suas intenções personalizadas usando respostas factuais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464ad576",
   "metadata": {},
   "source": [
    "Este serviço de Linguagem unifica os seguintes serviços do Azure AI anteriormente disponíveis: Text Analytics, QnA Maker e LUIS.\n",
    "\n",
    "O Azure AI Foundry permite que você use a maioria dos seguintes recursos de serviço sem precisar escrever código.\n",
    "\n",
    "**Reconhecimento de Entidade Nomeada (NER)** - O reconhecimento de entidade nomeada identifica diferentes entradas no texto e as categoriza em tipos predefinidos.\n",
    "\n",
    "**Detecção de informações pessoais e de saúde** - A detecção de PII identifica entidades em texto e conversas (chat ou transcrições) que estão associadas a indivíduos.\n",
    "\n",
    "**Detecção de idioma** - A detecção de idioma avalia o texto e detecta uma ampla gama de idiomas e dialetos variantes.\n",
    "\n",
    "**Análise de sentimento e mineração de opinião** - A análise de sentimento e mineração de opinião são recursos pré-configurados que ajudam você a entender a percepção pública da sua marca ou tópico. Esses recursos analisam o texto para identificar sentimentos positivos ou negativos e podem vinculá-los a elementos específicos dentro do texto.\n",
    "\n",
    "**Sumarização** - A sumarização condensa informações para texto e conversas (chat e transcrições). A sumarização de texto gera um resumo, suportando duas abordagens: A sumarização extrativa cria um resumo selecionando frases-chave do documento e preservando suas posições originais. Em contraste, a sumarização abstrativa gera um resumo produzindo sentenças ou frases novas, concisas e coerentes que não são copiadas diretamente do documento original. A sumarização de conversa recapitula e segmenta reuniões longas em capítulos com marcação de tempo. A sumarização de call center resume problemas do cliente e suas resoluções.\n",
    "\n",
    "**Extração de frases-chave** - A extração de frases-chave é um recurso pré-configurado que avalia e retorna os principais conceitos em texto não estruturado, retornando-os como uma lista.\n",
    "\n",
    "**Vinculação de entidades** - A vinculação de entidades é um recurso pré-configurado que desambigua a identidade de entidades (palavras ou frases) encontradas em texto não estruturado e retorna links para a Wikipedia.\n",
    "\n",
    "**Análise de texto para saúde** - A análise de texto para saúde extrai e rotula informações relevantes de saúde de texto não estruturado.\n",
    "\n",
    "**Classificação de texto personalizada** - A classificação de texto personalizada permite que você construa modelos de IA personalizados para classificar documentos de texto não estruturado em classes personalizadas que você define.\n",
    "\n",
    "**Reconhecimento de Entidade Nomeada Personalizada (NER Personalizado)** - O NER personalizado permite que você construa modelos de IA personalizados para extrair categorias de entidades personalizadas (rótulos para palavras ou frases), usando texto não estruturado que você fornece.\n",
    "\n",
    "**Compreensão de linguagem conversacional** - A compreensão de linguagem conversacional (CLU) permite que os usuários construam modelos personalizados de compreensão de linguagem natural para prever a intenção geral de uma declaração recebida e extrair informações importantes dela.\n",
    "\n",
    "**Fluxo de trabalho de orquestração** - O fluxo de trabalho de orquestração é um recurso personalizado que permite conectar aplicações de Compreensão de Linguagem Conversacional (CLU), resposta a perguntas e LUIS.\n",
    "\n",
    "**Resposta a perguntas** - A resposta a perguntas é um recurso personalizado que identifica a resposta mais adequada para entradas do usuário. Este recurso é tipicamente utilizado para desenvolver aplicações cliente conversacionais, incluindo plataformas de mídia social, chat bots e aplicações desktop habilitadas por voz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca10f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo - Language Service\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Configurar o cliente Language\n",
    "language_endpoint = os.getenv('LANGUAGE_ENDPOINT')\n",
    "language_key = os.getenv('LANGUAGE_KEY')\n",
    "\n",
    "if language_endpoint and language_key:\n",
    "    # Criar cliente\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=language_endpoint,\n",
    "        credential=AzureKeyCredential(language_key)\n",
    "    )\n",
    "    \n",
    "    # Texto de exemplo\n",
    "    documents = [\n",
    "        \"Eu amo este produto! É incrível e funciona perfeitamente.\",\n",
    "        \"Este serviço é terrível, não funcionou como esperado.\",\n",
    "        \"O atendimento foi adequado, nada excepcional.\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Análise de sentimento\n",
    "        response = text_analytics_client.analyze_sentiment(documents=documents, language=\"pt\")\n",
    "        \n",
    "        print(\"=== Análise de Sentimento ===\")\n",
    "        for idx, doc in enumerate(response):\n",
    "            if not doc.is_error:\n",
    "                print(f\"Documento {idx + 1}:\")\n",
    "                print(f\"  Texto: {documents[idx]}\")\n",
    "                print(f\"  Sentimento: {doc.sentiment}\")\n",
    "                print(f\"  Confiança: Positivo={doc.confidence_scores.positive:.2f}, \"\n",
    "                      f\"Neutro={doc.confidence_scores.neutral:.2f}, \"\n",
    "                      f\"Negativo={doc.confidence_scores.negative:.2f}\")\n",
    "                print()\n",
    "            else:\n",
    "                print(f\"Erro no documento {idx + 1}: {doc.error}\")\n",
    "                \n",
    "        # Extração de frases-chave\n",
    "        key_phrases_response = text_analytics_client.extract_key_phrases(documents=documents, language=\"pt\")\n",
    "        \n",
    "        print(\"=== Extração de Frases-Chave ===\")\n",
    "        for idx, doc in enumerate(key_phrases_response):\n",
    "            if not doc.is_error:\n",
    "                print(f\"Documento {idx + 1}:\")\n",
    "                print(f\"  Frases-chave: {', '.join(doc.key_phrases)}\")\n",
    "                print()\n",
    "            else:\n",
    "                print(f\"Erro no documento {idx + 1}: {doc.error}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Erro: {e}\")\n",
    "else:\n",
    "    print(\"Defina as variáveis LANGUAGE_ENDPOINT e LANGUAGE_KEY no arquivo .env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f754d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo - Translator Service\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Configurar o cliente Translator\n",
    "translator_endpoint = os.getenv('TRANSLATOR_ENDPOINT')\n",
    "translator_key = os.getenv('TRANSLATOR_KEY')\n",
    "translator_region = os.getenv('TRANSLATOR_REGION')\n",
    "\n",
    "if translator_endpoint and translator_key:\n",
    "    # URL para tradução\n",
    "    translate_url = f\"{translator_endpoint}/translate\"\n",
    "    \n",
    "    # Parâmetros da requisição\n",
    "    params = {\n",
    "        'api-version': '3.0',\n",
    "        'from': 'pt',\n",
    "        'to': ['en', 'es', 'fr']\n",
    "    }\n",
    "    \n",
    "    # Headers\n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': translator_key,\n",
    "        'Ocp-Apim-Subscription-Region': translator_region,\n",
    "        'Content-type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    # Texto para traduzir\n",
    "    body = [{\n",
    "        'text': 'Olá! Como você está hoje? Espero que esteja tendo um ótimo dia!'\n",
    "    }]\n",
    "    \n",
    "    try:\n",
    "        # Fazer a requisição de tradução\n",
    "        response = requests.post(translate_url, params=params, headers=headers, json=body)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        result = response.json()\n",
    "        \n",
    "        print(\"=== Tradução de Texto ===\")\n",
    "        print(f\"Texto original (pt): {body[0]['text']}\")\n",
    "        print()\n",
    "        \n",
    "        for translation in result[0]['translations']:\n",
    "            print(f\"Tradução ({translation['to']}): {translation['text']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na tradução: {e}\")\n",
    "        \n",
    "    # Detecção de idioma\n",
    "    detect_url = f\"{translator_endpoint}/detect\"\n",
    "    detect_params = {'api-version': '3.0'}\n",
    "    \n",
    "    texts_to_detect = [\n",
    "        {'text': 'Hello, how are you?'},\n",
    "        {'text': 'Bonjour, comment allez-vous?'},\n",
    "        {'text': 'Hola, ¿cómo estás?'},\n",
    "        {'text': 'Olá, como você está?'}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        detect_response = requests.post(detect_url, params=detect_params, headers=headers, json=texts_to_detect)\n",
    "        detect_response.raise_for_status()\n",
    "        \n",
    "        detect_result = detect_response.json()\n",
    "        \n",
    "        print(\"\\n=== Detecção de Idioma ===\")\n",
    "        for idx, detection in enumerate(detect_result):\n",
    "            print(f\"Texto: {texts_to_detect[idx]['text']}\")\n",
    "            print(f\"Idioma detectado: {detection['language']} (confiança: {detection['score']:.2f})\")\n",
    "            print()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na detecção de idioma: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Defina as variáveis TRANSLATOR_ENDPOINT, TRANSLATOR_KEY e TRANSLATOR_REGION no arquivo .env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6d532d",
   "metadata": {},
   "source": [
    "### Exercício 3 - Vision + Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76352a97",
   "metadata": {},
   "source": [
    "Dê aos seus aplicativos a capacidade de ler texto, analisar imagens, processar documentos e detectar rostos com tecnologias como reconhecimento óptico de caracteres (OCR) e aprendizado de máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c8d42",
   "metadata": {},
   "source": [
    "O serviço Azure AI Vision oferece acesso a algoritmos avançados que processam imagens e retornam informações baseadas nas características visuais de seu interesse. A tabela a seguir lista as principais categorias de produtos.\n",
    "\n",
    "**Reconhecimento Óptico de Caracteres (OCR)** - O serviço de Reconhecimento Óptico de Caracteres (OCR) extrai texto de imagens. Você pode usar a API Read para extrair texto impresso e manuscrito de fotos e documentos. Ele utiliza modelos baseados em aprendizado profundo e funciona com texto em várias superfícies e fundos. Isso inclui documentos comerciais, faturas, recibos, cartazes, cartões de visita, cartas e quadros brancos. As APIs de OCR suportam a extração de texto impresso em vários idiomas.\n",
    "\n",
    "**Análise de Imagem** - O serviço de Análise de Imagem extrai muitas características visuais de imagens, como objetos, rostos, conteúdo adulto e descrições de texto geradas automaticamente.\n",
    "\n",
    "**Face** - O serviço Face fornece algoritmos de IA que detectam, reconhecem e analisam rostos humanos em imagens. O software de reconhecimento facial é importante em muitos cenários diferentes, como identificação, controle de acesso sem toque e desfoque facial para privacidade.\n",
    "\n",
    "**Recuperação de Vídeo** - A Recuperação de Vídeo permite criar um índice de vídeos que você pode pesquisar com linguagem natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ef1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo - Computer Vision Service\n",
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "import time\n",
    "\n",
    "# Configurar o cliente Computer Vision\n",
    "vision_endpoint = os.getenv('VISION_ENDPOINT')\n",
    "vision_key = os.getenv('VISION_KEY')\n",
    "\n",
    "if vision_endpoint and vision_key:\n",
    "    # Criar cliente\n",
    "    computervision_client = ComputerVisionClient(\n",
    "        vision_endpoint, \n",
    "        CognitiveServicesCredentials(vision_key)\n",
    "    )\n",
    "    \n",
    "    # Caminho para a imagem de exemplo\n",
    "    image_path = \"../../samples/234039841.jpg\"\n",
    "    \n",
    "    try:\n",
    "        # Análise de imagem\n",
    "        print(\"=== Análise de Imagem ===\")\n",
    "        \n",
    "        # Abrir imagem local\n",
    "        with open(image_path, \"rb\") as image_stream:\n",
    "            # Análise da imagem\n",
    "            analysis = computervision_client.analyze_image_in_stream(\n",
    "                image_stream,\n",
    "                visual_features=['Categories', 'Description', 'Tags', 'Objects', 'Faces', 'Color']\n",
    "            )\n",
    "        \n",
    "        # Descrição da imagem\n",
    "        if analysis.description.captions:\n",
    "            print(f\"Descrição: {analysis.description.captions[0].text}\")\n",
    "            print(f\"Confiança: {analysis.description.captions[0].confidence:.2f}\")\n",
    "        \n",
    "        # Tags identificadas\n",
    "        print(f\"\\nTags identificadas:\")\n",
    "        for tag in analysis.tags:\n",
    "            print(f\"  - {tag.name} (confiança: {tag.confidence:.2f})\")\n",
    "        \n",
    "        # Objetos detectados\n",
    "        if analysis.objects:\n",
    "            print(f\"\\nObjetos detectados:\")\n",
    "            for obj in analysis.objects:\n",
    "                print(f\"  - {obj.object_property} em ({obj.rectangle.x}, {obj.rectangle.y}, \"\n",
    "                      f\"{obj.rectangle.w}, {obj.rectangle.h}) - confiança: {obj.confidence:.2f}\")\n",
    "        \n",
    "        # Faces detectadas\n",
    "        if analysis.faces:\n",
    "            print(f\"\\nFaces detectadas: {len(analysis.faces)}\")\n",
    "            for face in analysis.faces:\n",
    "                print(f\"  - Idade: {face.age}, Gênero: {face.gender}\")\n",
    "        \n",
    "        # Cores dominantes\n",
    "        print(f\"\\nCores dominantes: {', '.join(analysis.color.dominant_colors)}\")\n",
    "        print(f\"Cor de acento: {analysis.color.accent_color}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Arquivo de imagem não encontrado: {image_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na análise de imagem: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Defina as variáveis VISION_ENDPOINT e VISION_KEY no arquivo .env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo - OCR (Optical Character Recognition)\n",
    "if vision_endpoint and vision_key:\n",
    "    try:\n",
    "        print(\"\\n=== OCR - Extração de Texto ===\")\n",
    "        \n",
    "        # Usar uma imagem que contenha texto (por exemplo, car-accident.png)\n",
    "        ocr_image_path = \"../../samples/car-accident.png\"\n",
    "        \n",
    "        # Abrir imagem para OCR\n",
    "        with open(ocr_image_path, \"rb\") as image_stream:\n",
    "            # Usar a API Read para extrair texto\n",
    "            read_response = computervision_client.read_in_stream(image_stream, raw=True)\n",
    "            \n",
    "        # Obter operation ID da resposta\n",
    "        operation_location = read_response.headers[\"Operation-Location\"]\n",
    "        operation_id = operation_location.split(\"/\")[-1]\n",
    "        \n",
    "        # Aguardar processamento\n",
    "        print(\"Processando extração de texto...\")\n",
    "        while True:\n",
    "            read_result = computervision_client.get_read_result(operation_id)\n",
    "            if read_result.status not in ['notStarted', 'running']:\n",
    "                break\n",
    "            time.sleep(1)\n",
    "        \n",
    "        # Exibir resultados\n",
    "        if read_result.status == OperationStatusCodes.succeeded:\n",
    "            print(\"Texto extraído da imagem:\")\n",
    "            for text_result in read_result.analyze_result.read_results:\n",
    "                for line in text_result.lines:\n",
    "                    print(f\"  - {line.text}\")\n",
    "                    print(f\"    Caixa delimitadora: {line.bounding_box}\")\n",
    "        else:\n",
    "            print(\"Falha na extração de texto\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Arquivo de imagem não encontrado: {ocr_image_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no OCR: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf1ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo - Document Intelligence\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Configurar cliente Document Intelligence\n",
    "doc_intelligence_endpoint = os.getenv('DOC_INTELLIGENCE_ENDPOINT')\n",
    "doc_intelligence_key = os.getenv('DOC_INTELLIGENCE_KEY')\n",
    "\n",
    "if doc_intelligence_endpoint and doc_intelligence_key:\n",
    "    try:\n",
    "        print(\"\\n=== Document Intelligence ===\")\n",
    "        \n",
    "        # Criar cliente\n",
    "        document_analysis_client = DocumentAnalysisClient(\n",
    "            endpoint=doc_intelligence_endpoint,\n",
    "            credential=AzureKeyCredential(doc_intelligence_key)\n",
    "        )\n",
    "        \n",
    "        # Analisar documento genérico\n",
    "        doc_path = \"../../samples/car-accident.png\"\n",
    "        \n",
    "        with open(doc_path, \"rb\") as f:\n",
    "            # Usar modelo pré-construído para layout geral\n",
    "            poller = document_analysis_client.begin_analyze_document(\n",
    "                \"prebuilt-layout\", document=f\n",
    "            )\n",
    "            \n",
    "        result = poller.result()\n",
    "        \n",
    "        print(\"Análise de Layout do Documento:\")\n",
    "        print(f\"Número de páginas: {len(result.pages)}\")\n",
    "        \n",
    "        # Extrair tabelas\n",
    "        if result.tables:\n",
    "            print(f\"\\nTabelas encontradas: {len(result.tables)}\")\n",
    "            for idx, table in enumerate(result.tables):\n",
    "                print(f\"Tabela {idx + 1}: {table.row_count} linhas x {table.column_count} colunas\")\n",
    "                \n",
    "        # Extrair parágrafos\n",
    "        if result.paragraphs:\n",
    "            print(f\"\\nParágrafos encontrados: {len(result.paragraphs)}\")\n",
    "            for idx, paragraph in enumerate(result.paragraphs[:3]):  # Mostrar apenas os 3 primeiros\n",
    "                print(f\"Parágrafo {idx + 1}: {paragraph.content[:100]}...\")\n",
    "                \n",
    "        # Extrair pares chave-valor\n",
    "        if result.key_value_pairs:\n",
    "            print(f\"\\nPares chave-valor encontrados: {len(result.key_value_pairs)}\")\n",
    "            for kv_pair in result.key_value_pairs[:5]:  # Mostrar apenas os 5 primeiros\n",
    "                if kv_pair.key and kv_pair.value:\n",
    "                    print(f\"  {kv_pair.key.content}: {kv_pair.value.content}\")\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Arquivo de documento não encontrado: {doc_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no Document Intelligence: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Defina as variáveis DOC_INTELLIGENCE_ENDPOINT e DOC_INTELLIGENCE_KEY no arquivo .env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf190fab",
   "metadata": {},
   "source": [
    "### Exercício 4 - Content Safety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded113b8",
   "metadata": {},
   "source": [
    "A segurança de conteúdo do Azure AI detecta conteúdo prejudicial gerado por usuários e por IA em aplicativos e serviços. Este serviço disponibiliza vários tipos diferentes de análise.\n",
    "\n",
    "**Escudos de Prompt** - Examina texto em busca de riscos de ataques de entrada do usuário em um Modelo de Linguagem Grande.\n",
    "\n",
    "**Detecção de fundamentação (preview)** - Detecta se as respostas de texto de modelos de linguagem grandes (LLMs) estão fundamentadas nos materiais fonte fornecidos pelos usuários.\n",
    "\n",
    "**Detecção de material protegido em texto** - Examina texto gerado por IA em busca de conteúdo de texto conhecido (por exemplo, letras de música, artigos, receitas, conteúdo web selecionado).\n",
    "\n",
    "**API de categorias personalizadas (padrão) (preview)** - Permite criar e treinar suas próprias categorias de conteúdo personalizadas e examinar texto em busca de correspondências.\n",
    "\n",
    "**API de categorias personalizadas (rápida) (preview)** - Permite definir padrões emergentes de conteúdo prejudicial e examinar texto e imagens em busca de correspondências.\n",
    "\n",
    "**API de análise de texto** - Examina texto em busca de conteúdo sexual, violência, ódio e autolesão com múltiplos níveis de severidade.\n",
    "\n",
    "**API de análise de imagem** - Examina imagens em busca de conteúdo sexual, violência, ódio e autolesão com múltiplos níveis de severidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa9045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo - Content Safety\n",
    "from azure.ai.contentsafety import ContentSafetyClient\n",
    "from azure.ai.contentsafety.models import AnalyzeTextOptions, TextCategory\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Configurar cliente Content Safety\n",
    "content_safety_endpoint = os.getenv('CONTENT_SAFETY_ENDPOINT')\n",
    "content_safety_key = os.getenv('CONTENT_SAFETY_KEY')\n",
    "\n",
    "if content_safety_endpoint and content_safety_key:\n",
    "    try:\n",
    "        print(\"=== Content Safety - Análise de Texto ===\")\n",
    "        \n",
    "        # Criar cliente\n",
    "        client = ContentSafetyClient(\n",
    "            endpoint=content_safety_endpoint,\n",
    "            credential=AzureKeyCredential(content_safety_key)\n",
    "        )\n",
    "        \n",
    "        # Textos de exemplo para análise\n",
    "        test_texts = [\n",
    "            \"Olá! Como você está hoje? Tenha um ótimo dia!\",\n",
    "            \"Este é um texto neutro sobre tecnologia e programação.\",\n",
    "            \"Texto com conteúdo potencialmente problemático para teste de moderação.\"\n",
    "        ]\n",
    "        \n",
    "        for idx, text in enumerate(test_texts):\n",
    "            print(f\"\\nTexto {idx + 1}: {text}\")\n",
    "            \n",
    "            # Configurar opções de análise\n",
    "            request = AnalyzeTextOptions(text=text)\n",
    "            \n",
    "            try:\n",
    "                # Analisar texto\n",
    "                response = client.analyze_text(request)\n",
    "                \n",
    "                print(\"Resultados da análise:\")\n",
    "                \n",
    "                # Verificar cada categoria\n",
    "                for category_result in response.categories_analysis:\n",
    "                    category_name = category_result.category.value\n",
    "                    severity = category_result.severity\n",
    "                    \n",
    "                    print(f\"  - {category_name}: Severidade {severity}\")\n",
    "                    \n",
    "                    # Interpretar severidade\n",
    "                    if severity == 0:\n",
    "                        risk_level = \"Seguro\"\n",
    "                    elif severity <= 2:\n",
    "                        risk_level = \"Baixo risco\"\n",
    "                    elif severity <= 4:\n",
    "                        risk_level = \"Risco moderado\"\n",
    "                    else:\n",
    "                        risk_level = \"Alto risco\"\n",
    "                        \n",
    "                    print(f\"    Nível de risco: {risk_level}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Erro na análise: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Erro geral no Content Safety: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Defina as variáveis CONTENT_SAFETY_ENDPOINT e CONTENT_SAFETY_KEY no arquivo .env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b195fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo - Content Safety para Imagens\n",
    "from azure.ai.contentsafety.models import AnalyzeImageOptions, ImageData\n",
    "\n",
    "if content_safety_endpoint and content_safety_key:\n",
    "    try:\n",
    "        print(\"\\n=== Content Safety - Análise de Imagem ===\")\n",
    "        \n",
    "        # Caminho para imagem de exemplo\n",
    "        image_path = \"../../samples/234039841.jpg\"\n",
    "        \n",
    "        with open(image_path, \"rb\") as file:\n",
    "            image_data = file.read()\n",
    "            \n",
    "        # Configurar análise de imagem\n",
    "        request = AnalyzeImageOptions(image=ImageData(content=image_data))\n",
    "        \n",
    "        try:\n",
    "            # Analisar imagem\n",
    "            response = client.analyze_image(request)\n",
    "            \n",
    "            print(f\"Analisando imagem: {image_path}\")\n",
    "            print(\"Resultados da análise:\")\n",
    "            \n",
    "            # Verificar cada categoria\n",
    "            for category_result in response.categories_analysis:\n",
    "                category_name = category_result.category.value\n",
    "                severity = category_result.severity\n",
    "                \n",
    "                print(f\"  - {category_name}: Severidade {severity}\")\n",
    "                \n",
    "                # Interpretar severidade\n",
    "                if severity == 0:\n",
    "                    risk_level = \"Seguro\"\n",
    "                elif severity <= 2:\n",
    "                    risk_level = \"Baixo risco\"\n",
    "                elif severity <= 4:\n",
    "                    risk_level = \"Risco moderado\"\n",
    "                else:\n",
    "                    risk_level = \"Alto risco\"\n",
    "                    \n",
    "                print(f\"    Nível de risco: {risk_level}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Erro na análise de imagem: {e}\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Arquivo de imagem não encontrado: {image_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro geral: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4dfe4a",
   "metadata": {},
   "source": [
    "### Variáveis de Ambiente Necessárias\n",
    "\n",
    "Para executar todos os exemplos deste laboratório, certifique-se de que seu arquivo `.env` contenha as seguintes variáveis:\n",
    "\n",
    "```env\n",
    "# Speech Service\n",
    "SPEECH_ENDPOINT=https://your-speech-service.cognitiveservices.azure.com/\n",
    "SPEECH_KEY=your-speech-key\n",
    "SPEECH_REGION=your-region\n",
    "\n",
    "# Language Service\n",
    "LANGUAGE_ENDPOINT=https://your-language-service.cognitiveservices.azure.com/\n",
    "LANGUAGE_KEY=your-language-key\n",
    "\n",
    "# Translator Service\n",
    "TRANSLATOR_ENDPOINT=https://api.cognitive.microsofttranslator.com/\n",
    "TRANSLATOR_KEY=your-translator-key\n",
    "TRANSLATOR_REGION=your-region\n",
    "\n",
    "# Computer Vision Service\n",
    "VISION_ENDPOINT=https://your-vision-service.cognitiveservices.azure.com/\n",
    "VISION_KEY=your-vision-key\n",
    "\n",
    "# Document Intelligence\n",
    "DOC_INTELLIGENCE_ENDPOINT=https://your-doc-intelligence.cognitiveservices.azure.com/\n",
    "DOC_INTELLIGENCE_KEY=your-doc-intelligence-key\n",
    "\n",
    "# Content Safety\n",
    "CONTENT_SAFETY_ENDPOINT=https://your-content-safety.cognitiveservices.azure.com/\n",
    "CONTENT_SAFETY_KEY=your-content-safety-key\n",
    "```\n",
    "\n",
    "**Nota:** Substitua os valores pelos endpoints e chaves reais dos seus recursos Azure AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
