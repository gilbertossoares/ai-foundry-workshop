{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2364a810",
   "metadata": {},
   "source": [
    "# Azure AI Foundry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a02bae",
   "metadata": {},
   "source": [
    "<center><img src=\"../../../images/Azure-AI-Foundry_1600x900.jpg\" alt=\"Azure AI Foundry\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a15a5",
   "metadata": {},
   "source": [
    "## Laboratório 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048dc4e5",
   "metadata": {},
   "source": [
    "Neste laboratório é explorar os serviços de AI presentes no Azure Foundry, este laboratório vai cobrir os seguintes serviços:\n",
    "- Speech\n",
    "- Language + Translator\n",
    "- Vision + Document \n",
    "- Content Safety\n",
    "\n",
    "Entendendo estes serviços podemos adicionar mais habilidades à nossas aplicações."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7617fed7",
   "metadata": {},
   "source": [
    "### Exercício 1 - Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f46c67f",
   "metadata": {},
   "source": [
    "O serviço Speech fornece recursos de conversão de fala para texto e texto para fala com um recurso Speech. Você pode transcrever fala para texto com alta precisão, produzir vozes naturais de texto para fala, traduzir áudio falado e usar reconhecimento de locutor durante conversas. Crie vozes personalizadas, adicione palavras específicas ao seu vocabulário base ou construa seus próprios modelos. Execute o Speech em qualquer lugar, na nuvem ou na borda em contêineres. É fácil habilitar fala em suas aplicações, ferramentas e dispositivos com a CLI do Speech, SDK do Speech e APIs REST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e176c",
   "metadata": {},
   "source": [
    "Cenários comunis para uso do speech:\n",
    "\n",
    "**Geração de legenda:** Aprenda como sincronizar legendas com seu áudio de entrada, aplicar filtros de palavrões, obter resultados parciais, aplicar personalizações e identificar idiomas falados para cenários multilíngues.\n",
    "\n",
    "**Criação de Conteúdo de Áudio:** Você pode usar vozes neurais para tornar as interações com chatbots e assistentes de voz mais naturais e envolventes, converter textos digitais como e-books em audiolivros e aprimorar sistemas de navegação automotiva.\n",
    "\n",
    "**Central de Atendimento:** Transcreva chamadas em tempo real ou processe um lote de chamadas, remova informações de identificação pessoal e extraia insights como análise de sentimento para auxiliar no seu caso de uso de central de atendimento.\n",
    "\n",
    "**Aprendizado de Idiomas:** Forneça feedback de avaliação de pronúncia para estudantes de idiomas, ofereça suporte à transcrição em tempo real para conversas de aprendizado remoto e leia materiais didáticos em voz alta usando vozes neurais.\n",
    "\n",
    "**Assistentes de Voz:** Crie interfaces conversacionais naturais e semelhantes às humanas para suas aplicações e experiências. O recurso de assistente de voz oferece interação rápida e confiável entre um dispositivo e uma implementação de assistente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dea14b",
   "metadata": {},
   "source": [
    "Para realizar este exercício verifique se no seu arquivo `.env` possui as seguintes variaveis preenchidas:\n",
    "- SPEECH_ENDPOINT \n",
    "- SPEECH_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0237903",
   "metadata": {},
   "source": [
    "Após verificar vamos iniciar carregando as bibliotecas necessárias, iniciando o cliente e realizando uma chamada para converter audio em texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea88da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-cognitiveservices-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a68310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconhecimento contínuo de fala para processar todo o áudio, mesmo com silêncio inicial\n",
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "speech_key = os.getenv('SPEECH_KEY')\n",
    "speech_region = os.getenv('SPEECH_REGION')\n",
    "audio_file = '../../../samples/audio001.wav'\n",
    "if speech_key and speech_region:\n",
    "    try:\n",
    "        speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)\n",
    "        speech_config.speech_recognition_language = \"pt-BR\"\n",
    "        audio_config = speechsdk.audio.AudioConfig(filename=audio_file)\n",
    "        speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "        recognized_texts = []\n",
    "        def recognized_cb(evt):\n",
    "            if evt.result.text:\n",
    "                print('Reconhecido:', evt.result.text)\n",
    "                recognized_texts.append(evt.result.text)\n",
    "\n",
    "        speech_recognizer.recognized.connect(recognized_cb)\n",
    "\n",
    "        print(\"Iniciando reconhecimento contínuo...\")\n",
    "        speech_recognizer.start_continuous_recognition()\n",
    "\n",
    "        # Aguarda o reconhecimento terminar (ajuste o tempo conforme o tamanho do áudio)\n",
    "        time.sleep(10)\n",
    "        speech_recognizer.stop_continuous_recognition()\n",
    "\n",
    "        print(\"Reconhecimento finalizado. Texto completo:\")\n",
    "        print(' '.join(recognized_texts))\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no reconhecimento contínuo: {e}\")\n",
    "else:\n",
    "    print(\"Por favor, configure as variáveis de ambiente SPEECH_KEY e SPEECH_REGION.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85f8f57",
   "metadata": {},
   "source": [
    "### Exercício 2 - Language + Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8423f00",
   "metadata": {},
   "source": [
    "Integre linguagem natural em aplicativos, bots e dispositivos IoT. Por exemplo, este serviço pode remover dados sensíveis, segmentar reuniões longas em capítulos, analisar registros de saúde e orquestrar bots conversacionais com suas intenções personalizadas usando respostas factuais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464ad576",
   "metadata": {},
   "source": [
    "Este serviço de Linguagem unifica os seguintes serviços do Azure AI anteriormente disponíveis: Text Analytics, QnA Maker e LUIS.\n",
    "\n",
    "O Azure AI Foundry permite que você use a maioria dos seguintes recursos de serviço sem precisar escrever código.\n",
    "\n",
    "**Reconhecimento de Entidade Nomeada (NER)** - O reconhecimento de entidade nomeada identifica diferentes entradas no texto e as categoriza em tipos predefinidos.\n",
    "\n",
    "**Detecção de informações pessoais e de saúde** - A detecção de PII identifica entidades em texto e conversas (chat ou transcrições) que estão associadas a indivíduos.\n",
    "\n",
    "**Detecção de idioma** - A detecção de idioma avalia o texto e detecta uma ampla gama de idiomas e dialetos variantes.\n",
    "\n",
    "**Análise de sentimento e mineração de opinião** - A análise de sentimento e mineração de opinião são recursos pré-configurados que ajudam você a entender a percepção pública da sua marca ou tópico. Esses recursos analisam o texto para identificar sentimentos positivos ou negativos e podem vinculá-los a elementos específicos dentro do texto.\n",
    "\n",
    "**Sumarização** - A sumarização condensa informações para texto e conversas (chat e transcrições). A sumarização de texto gera um resumo, suportando duas abordagens: A sumarização extrativa cria um resumo selecionando frases-chave do documento e preservando suas posições originais. Em contraste, a sumarização abstrativa gera um resumo produzindo sentenças ou frases novas, concisas e coerentes que não são copiadas diretamente do documento original. A sumarização de conversa recapitula e segmenta reuniões longas em capítulos com marcação de tempo. A sumarização de call center resume problemas do cliente e suas resoluções.\n",
    "\n",
    "**Extração de frases-chave** - A extração de frases-chave é um recurso pré-configurado que avalia e retorna os principais conceitos em texto não estruturado, retornando-os como uma lista.\n",
    "\n",
    "**Vinculação de entidades** - A vinculação de entidades é um recurso pré-configurado que desambigua a identidade de entidades (palavras ou frases) encontradas em texto não estruturado e retorna links para a Wikipedia.\n",
    "\n",
    "**Análise de texto para saúde** - A análise de texto para saúde extrai e rotula informações relevantes de saúde de texto não estruturado.\n",
    "\n",
    "**Classificação de texto personalizada** - A classificação de texto personalizada permite que você construa modelos de IA personalizados para classificar documentos de texto não estruturado em classes personalizadas que você define.\n",
    "\n",
    "**Reconhecimento de Entidade Nomeada Personalizada (NER Personalizado)** - O NER personalizado permite que você construa modelos de IA personalizados para extrair categorias de entidades personalizadas (rótulos para palavras ou frases), usando texto não estruturado que você fornece.\n",
    "\n",
    "**Compreensão de linguagem conversacional** - A compreensão de linguagem conversacional (CLU) permite que os usuários construam modelos personalizados de compreensão de linguagem natural para prever a intenção geral de uma declaração recebida e extrair informações importantes dela.\n",
    "\n",
    "**Fluxo de trabalho de orquestração** - O fluxo de trabalho de orquestração é um recurso personalizado que permite conectar aplicações de Compreensão de Linguagem Conversacional (CLU), resposta a perguntas e LUIS.\n",
    "\n",
    "**Resposta a perguntas** - A resposta a perguntas é um recurso personalizado que identifica a resposta mais adequada para entradas do usuário. Este recurso é tipicamente utilizado para desenvolver aplicações cliente conversacionais, incluindo plataformas de mídia social, chat bots e aplicações desktop habilitadas por voz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b72b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure.ai.textanalytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca10f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo - Language Service\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Configurar o cliente Language\n",
    "language_endpoint = os.getenv('AZURE_LANGUAGE_ENDPOINT')\n",
    "language_key = os.getenv('AZURE_LANGUAGE_KEY')\n",
    "\n",
    "if language_endpoint and language_key:\n",
    "    # Criar cliente\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=language_endpoint,\n",
    "        credential=AzureKeyCredential(language_key)\n",
    "    )\n",
    "    \n",
    "    # Texto de exemplo\n",
    "    documents = [\n",
    "        \"Eu amo este produto! É incrível e funciona perfeitamente.\",\n",
    "        \"Este serviço é terrível, não funcionou como esperado.\",\n",
    "        \"O atendimento foi adequado, nada excepcional.\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Análise de sentimento\n",
    "        response = text_analytics_client.analyze_sentiment(documents=documents, language=\"pt\")\n",
    "        \n",
    "        print(\"=== Análise de Sentimento ===\")\n",
    "        for idx, doc in enumerate(response):\n",
    "            if not doc.is_error:\n",
    "                print(f\"Documento {idx + 1}:\")\n",
    "                print(f\"  Texto: {documents[idx]}\")\n",
    "                print(f\"  Sentimento: {doc.sentiment}\")\n",
    "                print(f\"  Confiança: Positivo={doc.confidence_scores.positive:.2f}, \"\n",
    "                      f\"Neutro={doc.confidence_scores.neutral:.2f}, \"\n",
    "                      f\"Negativo={doc.confidence_scores.negative:.2f}\")\n",
    "                print()\n",
    "            else:\n",
    "                print(f\"Erro no documento {idx + 1}: {doc.error}\")\n",
    "                \n",
    "        # Extração de frases-chave\n",
    "        key_phrases_response = text_analytics_client.extract_key_phrases(documents=documents, language=\"pt\")\n",
    "        \n",
    "        print(\"=== Extração de Frases-Chave ===\")\n",
    "        for idx, doc in enumerate(key_phrases_response):\n",
    "            if not doc.is_error:\n",
    "                print(f\"Documento {idx + 1}:\")\n",
    "                print(f\"  Frases-chave: {', '.join(doc.key_phrases)}\")\n",
    "                print()\n",
    "            else:\n",
    "                print(f\"Erro no documento {idx + 1}: {doc.error}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Erro: {e}\")\n",
    "else:\n",
    "    print(\"Defina as variáveis LANGUAGE_ENDPOINT e LANGUAGE_KEY no arquivo .env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6d532d",
   "metadata": {},
   "source": [
    "### Exercício 3 - Vision + Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76352a97",
   "metadata": {},
   "source": [
    "Dê aos seus aplicativos a capacidade de ler texto, analisar imagens, processar documentos e detectar rostos com tecnologias como reconhecimento óptico de caracteres (OCR) e aprendizado de máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c8d42",
   "metadata": {},
   "source": [
    "O serviço Azure AI Vision oferece acesso a algoritmos avançados que processam imagens e retornam informações baseadas nas características visuais de seu interesse. A tabela a seguir lista as principais categorias de produtos.\n",
    "\n",
    "**Reconhecimento Óptico de Caracteres (OCR)** - O serviço de Reconhecimento Óptico de Caracteres (OCR) extrai texto de imagens. Você pode usar a API Read para extrair texto impresso e manuscrito de fotos e documentos. Ele utiliza modelos baseados em aprendizado profundo e funciona com texto em várias superfícies e fundos. Isso inclui documentos comerciais, faturas, recibos, cartazes, cartões de visita, cartas e quadros brancos. As APIs de OCR suportam a extração de texto impresso em vários idiomas.\n",
    "\n",
    "**Análise de Imagem** - O serviço de Análise de Imagem extrai muitas características visuais de imagens, como objetos, rostos, conteúdo adulto e descrições de texto geradas automaticamente.\n",
    "\n",
    "**Face** - O serviço Face fornece algoritmos de IA que detectam, reconhecem e analisam rostos humanos em imagens. O software de reconhecimento facial é importante em muitos cenários diferentes, como identificação, controle de acesso sem toque e desfoque facial para privacidade.\n",
    "\n",
    "**Recuperação de Vídeo** - A Recuperação de Vídeo permite criar um índice de vídeos que você pode pesquisar com linguagem natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ef1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Configurar o cliente Vision\n",
    "vision_endpoint = os.getenv('AZURE_VISION_ENDPOINT')\n",
    "vision_key = os.getenv('AZURE_VISION_KEY')\n",
    "\n",
    "if vision_endpoint and vision_key:\n",
    "    try:\n",
    "        print(\"=== Azure AI Vision - Análise de Imagem ===\")\n",
    "        \n",
    "        # Criar cliente\n",
    "        client = ImageAnalysisClient(\n",
    "            endpoint=vision_endpoint,\n",
    "            credential=AzureKeyCredential(vision_key)\n",
    "        )\n",
    "        \n",
    "        # Analisar imagem remota\n",
    "        print(\"\\n1. Analisando imagem remota...\")\n",
    "        result = client.analyze_from_url(\n",
    "            image_url=\"https://learn.microsoft.com/azure/ai-services/computer-vision/media/quickstarts/presentation.png\",\n",
    "            visual_features=[VisualFeatures.CAPTION, VisualFeatures.READ],\n",
    "            gender_neutral_caption=True,  # Opcional (default é False)\n",
    "        )\n",
    "        \n",
    "        print(\"Resultados da análise:\")\n",
    "        \n",
    "        # Exibir resultados de legenda\n",
    "        print(\"\\n📝 Legenda:\")\n",
    "        if result.caption is not None:\n",
    "            print(f\"   '{result.caption.text}', Confiança: {result.caption.confidence:.4f}\")\n",
    "        else:\n",
    "            print(\"   Nenhuma legenda encontrada\")\n",
    "\n",
    "        # Exibir resultados de OCR (texto extraído)\n",
    "        print(\"\\n📖 Texto extraído (OCR):\")\n",
    "        if result.read is not None:\n",
    "            for block in result.read.blocks:\n",
    "                for line in block.lines:\n",
    "                    print(f\"   Linha: '{line.text}', Caixa delimitadora: {line.bounding_polygon}\")\n",
    "                    for word in line.words:\n",
    "                        print(f\"     Palavra: '{word.text}', Confiança: {word.confidence:.4f}\")\n",
    "        else:\n",
    "            print(\"   Nenhum texto encontrado\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na análise da imagem: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Defina as variáveis AZURE_VISION_ENDPOINT e AZURE_VISION_KEY no arquivo .env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf98a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo avançado - Análise completa de imagem local\n",
    "if vision_endpoint and vision_key:\n",
    "    try:\n",
    "        print(\"\\n=== Análise Avançada de Imagem ===\")\n",
    "        \n",
    "        # Analisar imagem local\n",
    "        image_path = \"../../../samples/car-accident.png\"\n",
    "        \n",
    "        print(f\"\\n2. Analisando imagem local: {image_path}\")\n",
    "        \n",
    "        # Abrir imagem local\n",
    "        with open(image_path, \"rb\") as image_data:\n",
    "            # Análise completa com múltiplas funcionalidades\n",
    "            result = client.analyze(\n",
    "                image_data=image_data.read(),\n",
    "                visual_features=[\n",
    "                    VisualFeatures.CAPTION,\n",
    "                    VisualFeatures.READ,\n",
    "                    VisualFeatures.TAGS,\n",
    "                    VisualFeatures.OBJECTS,\n",
    "                    VisualFeatures.PEOPLE,\n",
    "                    VisualFeatures.SMART_CROPS\n",
    "                ],\n",
    "                gender_neutral_caption=True\n",
    "            )\n",
    "            \n",
    "            print(\"\\n📝 Legenda:\")\n",
    "            if result.caption:\n",
    "                print(f\"   '{result.caption.text}', Confiança: {result.caption.confidence:.4f}\")\n",
    "            \n",
    "            print(\"\\n🏷️  Tags identificadas:\")\n",
    "            if result.tags:\n",
    "                for tag in result.tags.list:\n",
    "                    print(f\"   - {tag.name}: {tag.confidence:.4f}\")\n",
    "            \n",
    "            print(\"\\n📦 Objetos detectados:\")\n",
    "            if result.objects:\n",
    "                for obj in result.objects.list:\n",
    "                    print(f\"   - {obj.tags[0].name}: {obj.tags[0].confidence:.4f}\")\n",
    "                    print(f\"     Localização: {obj.bounding_box}\")\n",
    "            \n",
    "            print(\"\\n👥 Pessoas detectadas:\")\n",
    "            if result.people:\n",
    "                for person in result.people.list:\n",
    "                    print(f\"   - Pessoa detectada com confiança: {person.confidence:.4f}\")\n",
    "                    print(f\"     Localização: {person.bounding_box}\")\n",
    "            \n",
    "            print(\"\\n✂️  Recortes inteligentes:\")\n",
    "            if result.smart_crops:\n",
    "                for crop in result.smart_crops.list:\n",
    "                    print(f\"   - Aspecto {crop.aspect_ratio}: {crop.bounding_box}\")\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Arquivo de imagem não encontrado: {image_path}\")\n",
    "        print(\"Certifique-se de que o arquivo existe no caminho especificado.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na análise avançada: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo - Análise de diferentes tipos de imagens\n",
    "if vision_endpoint and vision_key:\n",
    "    try:\n",
    "        print(\"\\n=== Comparação de Análises de Diferentes Imagens ===\")\n",
    "        \n",
    "        # Lista de imagens para analisar\n",
    "        images_to_analyze = [\n",
    "            {\n",
    "                \"url\": \"https://learn.microsoft.com/azure/ai-services/computer-vision/media/quickstarts/presentation.png\",\n",
    "                \"description\": \"Apresentação de calendário\"\n",
    "            },\n",
    "            {\n",
    "                \"url\": \"https://images.unsplash.com/photo-1518791841217-8f162f1e1131?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1000&q=80\",\n",
    "                \"description\": \"Gato\"\n",
    "            },\n",
    "            {\n",
    "                \"url\": \"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1000&q=80\",\n",
    "                \"description\": \"Paisagem montanhosa\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for i, image_info in enumerate(images_to_analyze, 1):\n",
    "            print(f\"\\n{i}. Analisando: {image_info['description']}\")\n",
    "            print(f\"   URL: {image_info['url']}\")\n",
    "            \n",
    "            try:\n",
    "                # Análise focada em legenda e tags\n",
    "                result = client.analyze_from_url(\n",
    "                    image_url=image_info[\"url\"],\n",
    "                    visual_features=[VisualFeatures.CAPTION, VisualFeatures.TAGS],\n",
    "                    gender_neutral_caption=True\n",
    "                )\n",
    "                \n",
    "                if result.caption:\n",
    "                    print(f\"   📝 Legenda: '{result.caption.text}' (Confiança: {result.caption.confidence:.4f})\")\n",
    "                \n",
    "                if result.tags:\n",
    "                    print(\"   🏷️  Top 5 tags:\")\n",
    "                    for tag in result.tags.list[:5]:\n",
    "                        print(f\"      - {tag.name}: {tag.confidence:.4f}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Erro ao analisar imagem {i}: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Erro geral na análise comparativa: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1808bb",
   "metadata": {},
   "source": [
    "#### Funcionalidades do Azure AI Vision\n",
    "\n",
    "O Azure AI Vision oferece várias funcionalidades através do SDK `azure-ai-vision-imageanalysis`:\n",
    "\n",
    "**📝 Funcionalidades Principais:**\n",
    "- **CAPTION**: Gera legendas descritivas para imagens\n",
    "- **READ**: Extrai texto de imagens (OCR)\n",
    "- **TAGS**: Identifica objetos, conceitos e ações na imagem\n",
    "- **OBJECTS**: Detecta e localiza objetos específicos\n",
    "- **PEOPLE**: Identifica pessoas na imagem\n",
    "- **SMART_CROPS**: Sugere recortes inteligentes da imagem\n",
    "- **FACES**: Detecta e analisa rostos (funcionalidade separada)\n",
    "\n",
    "**📊 Métodos de Análise:**\n",
    "- `analyze_from_url()`: Analisa imagem de uma URL\n",
    "- `analyze()`: Analisa imagem de dados binários (arquivo local)\n",
    "\n",
    "**⚙️ Parâmetros Importantes:**\n",
    "- `visual_features`: Lista de funcionalidades a serem analisadas\n",
    "- `gender_neutral_caption`: Gera legendas neutras em gênero\n",
    "- `language`: Idioma para os resultados (padrão: inglês)\n",
    "- `smart_crops_aspect_ratios`: Proporções para recortes inteligentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4cd641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Exercício Prático - Teste suas próprias imagens\n",
    "if vision_endpoint and vision_key:\n",
    "    print(\"=== Exercício Prático ===\")\n",
    "    print(\"\\n📝 Instruções:\")\n",
    "    print(\"1. Substitua a URL abaixo por uma imagem de sua escolha\")\n",
    "    print(\"2. Escolha as funcionalidades que deseja testar\")\n",
    "    print(\"3. Execute a célula e analise os resultados\")\n",
    "    \n",
    "    # 🎯 MODIFIQUE AQUI: Cole a URL de uma imagem para testar\n",
    "    your_image_url = \"https://images.unsplash.com/photo-1544947950-fa07a98d237f?ixlib=rb-4.0.3&auto=format&fit=crop&w=800&q=80\"\n",
    "    \n",
    "    # 🎯 MODIFIQUE AQUI: Escolha as funcionalidades que deseja testar\n",
    "    selected_features = [\n",
    "        VisualFeatures.CAPTION,\n",
    "        VisualFeatures.TAGS,\n",
    "        VisualFeatures.OBJECTS,\n",
    "        # VisualFeatures.READ,          # Descomente para OCR\n",
    "        # VisualFeatures.PEOPLE,        # Descomente para detectar pessoas\n",
    "        # VisualFeatures.SMART_CROPS,   # Descomente para recortes inteligentes\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n🔍 Analisando sua imagem...\")\n",
    "        print(f\"URL: {your_image_url}\")\n",
    "        \n",
    "        result = client.analyze_from_url(\n",
    "            image_url=your_image_url,\n",
    "            visual_features=selected_features,\n",
    "            gender_neutral_caption=True\n",
    "        )\n",
    "        \n",
    "        # Resultados\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"🎉 RESULTADOS DA SUA ANÁLISE\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if result.caption:\n",
    "            print(f\"\\n📝 LEGENDA:\")\n",
    "            print(f\"   '{result.caption.text}'\")\n",
    "            print(f\"   Confiança: {result.caption.confidence:.2%}\")\n",
    "        \n",
    "        if result.tags:\n",
    "            print(f\"\\n🏷️  TAGS IDENTIFICADAS:\")\n",
    "            for i, tag in enumerate(result.tags.list[:10], 1):\n",
    "                print(f\"   {i:2d}. {tag.name:<20} {tag.confidence:.2%}\")\n",
    "        \n",
    "        if result.objects:\n",
    "            print(f\"\\n📦 OBJETOS DETECTADOS:\")\n",
    "            for i, obj in enumerate(result.objects.list, 1):\n",
    "                print(f\"   {i}. {obj.tags[0].name} (Confiança: {obj.tags[0].confidence:.2%})\")\n",
    "        \n",
    "        if result.read:\n",
    "            print(f\"\\n📖 TEXTO EXTRAÍDO:\")\n",
    "            for block in result.read.blocks:\n",
    "                for line in block.lines:\n",
    "                    print(f\"   '{line.text}'\")\n",
    "        \n",
    "        if result.people:\n",
    "            print(f\"\\n👥 PESSOAS DETECTADAS: {len(result.people.list)} pessoa(s)\")\n",
    "        \n",
    "        if result.smart_crops:\n",
    "            print(f\"\\n✂️  RECORTES SUGERIDOS: {len(result.smart_crops.list)} opção(ões)\")\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Erro ao analisar sua imagem: {e}\")\n",
    "        print(\"Verifique se a URL da imagem está correta e acessível.\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Configure as variáveis de ambiente AZURE_VISION_ENDPOINT e AZURE_VISION_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4065675",
   "metadata": {},
   "source": [
    "#### 🎓 Próximos Passos\n",
    "\n",
    "**📚 Documentação e Recursos:**\n",
    "- [Documentação Oficial do Azure AI Vision](https://learn.microsoft.com/azure/ai-services/computer-vision/)\n",
    "- [Guia Completo da API de Análise de Imagem 4.0](https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/call-analyze-image-40)\n",
    "- [Referência do SDK Python](https://aka.ms/azsdk/image-analysis/ref-docs/python)\n",
    "- [Amostras de Código no GitHub](https://aka.ms/azsdk/image-analysis/samples/python)\n",
    "\n",
    "**🛠️ Experimente Também:**\n",
    "- [Azure AI Vision Studio](https://portal.vision.cognitive.azure.com/) - Interface visual para testar recursos\n",
    "- [Azure AI Face Service](https://azure.microsoft.com/services/cognitive-services/face/) - Análise facial avançada\n",
    "- [Azure AI Custom Vision](https://www.customvision.ai/) - Treinamento de modelos personalizados\n",
    "- [Azure AI Video Indexer](https://www.videoindexer.ai/) - Análise de vídeos\n",
    "\n",
    "**💻 Projetos Práticos Sugeridos:**\n",
    "1. **Descritor de Imagens para Acessibilidade**: Crie um site que gera alt-text automático\n",
    "2. **Analisador de Documentos**: Extraia texto de faturas e documentos\n",
    "3. **Moderador de Conteúdo**: Sistema para classificar imagens automaticamente\n",
    "4. **Assistente Visual**: Aplicativo mobile que descreve o ambiente ao redor\n",
    "\n",
    "**🔄 Integração com Outros Serviços:**\n",
    "- Combine com Azure AI Language para análise de sentimento do texto extraído\n",
    "- Use com Azure AI Translator para tradução automática de texto em imagens\n",
    "- Integre com Azure Storage para processamento em lote\n",
    "- Conecte com Power BI para dashboards de insights visuais\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf1ee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo - Document Intelligence\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Configurar cliente Document Intelligence\n",
    "doc_intelligence_endpoint = os.getenv('DOC_INTELLIGENCE_ENDPOINT')\n",
    "doc_intelligence_key = os.getenv('DOC_INTELLIGENCE_KEY')\n",
    "\n",
    "if doc_intelligence_endpoint and doc_intelligence_key:\n",
    "    try:\n",
    "        print(\"\\n=== Document Intelligence ===\")\n",
    "        \n",
    "        # Criar cliente\n",
    "        document_analysis_client = DocumentAnalysisClient(\n",
    "            endpoint=doc_intelligence_endpoint,\n",
    "            credential=AzureKeyCredential(doc_intelligence_key)\n",
    "        )\n",
    "        \n",
    "        # Analisar documento genérico\n",
    "        doc_path = \"../../../samples/placa.jpg\"\n",
    "        \n",
    "        with open(doc_path, \"rb\") as f:\n",
    "            # Usar modelo pré-construído para layout geral\n",
    "            poller = document_analysis_client.begin_analyze_document(\n",
    "                \"prebuilt-layout\", document=f\n",
    "            )\n",
    "            \n",
    "        result = poller.result()\n",
    "        \n",
    "        print(\"Análise de Layout do Documento:\")\n",
    "        print(f\"Número de páginas: {len(result.pages)}\")\n",
    "        \n",
    "        # Extrair tabelas\n",
    "        if result.tables:\n",
    "            print(f\"\\nTabelas encontradas: {len(result.tables)}\")\n",
    "            for idx, table in enumerate(result.tables):\n",
    "                print(f\"Tabela {idx + 1}: {table.row_count} linhas x {table.column_count} colunas\")\n",
    "                \n",
    "        # Extrair parágrafos\n",
    "        if result.paragraphs:\n",
    "            print(f\"\\nParágrafos encontrados: {len(result.paragraphs)}\")\n",
    "            for idx, paragraph in enumerate(result.paragraphs[:3]):  # Mostrar apenas os 3 primeiros\n",
    "                print(f\"Parágrafo {idx + 1}: {paragraph.content[:100]}...\")\n",
    "                \n",
    "        # Extrair pares chave-valor\n",
    "        if result.key_value_pairs:\n",
    "            print(f\"\\nPares chave-valor encontrados: {len(result.key_value_pairs)}\")\n",
    "            for kv_pair in result.key_value_pairs[:5]:  # Mostrar apenas os 5 primeiros\n",
    "                if kv_pair.key and kv_pair.value:\n",
    "                    print(f\"  {kv_pair.key.content}: {kv_pair.value.content}\")\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Arquivo de documento não encontrado: {doc_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no Document Intelligence: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"Defina as variáveis DOC_INTELLIGENCE_ENDPOINT e DOC_INTELLIGENCE_KEY no arquivo .env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf190fab",
   "metadata": {},
   "source": [
    "### Exercício 4 - Content Safety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded113b8",
   "metadata": {},
   "source": [
    "A segurança de conteúdo do Azure AI detecta conteúdo prejudicial gerado por usuários e por IA em aplicativos e serviços. Este serviço disponibiliza vários tipos diferentes de análise.\n",
    "\n",
    "**Escudos de Prompt** - Examina texto em busca de riscos de ataques de entrada do usuário em um Modelo de Linguagem Grande.\n",
    "\n",
    "**Detecção de fundamentação (preview)** - Detecta se as respostas de texto de modelos de linguagem grandes (LLMs) estão fundamentadas nos materiais fonte fornecidos pelos usuários.\n",
    "\n",
    "**Detecção de material protegido em texto** - Examina texto gerado por IA em busca de conteúdo de texto conhecido (por exemplo, letras de música, artigos, receitas, conteúdo web selecionado).\n",
    "\n",
    "**API de categorias personalizadas (padrão) (preview)** - Permite criar e treinar suas próprias categorias de conteúdo personalizadas e examinar texto em busca de correspondências.\n",
    "\n",
    "**API de categorias personalizadas (rápida) (preview)** - Permite definir padrões emergentes de conteúdo prejudicial e examinar texto e imagens em busca de correspondências.\n",
    "\n",
    "**API de análise de texto** - Examina texto em busca de conteúdo sexual, violência, ódio e autolesão com múltiplos níveis de severidade.\n",
    "\n",
    "**API de análise de imagem** - Examina imagens em busca de conteúdo sexual, violência, ódio e autolesão com múltiplos níveis de severidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa9045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo - Content Safety (seguindo quickstart oficial)\n",
    "from azure.ai.contentsafety import ContentSafetyClient\n",
    "from azure.ai.contentsafety.models import AnalyzeTextOptions, TextCategory\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "def analyze_text_content(text_to_analyze):\n",
    "    \"\"\"\n",
    "    Função para analisar texto usando Azure Content Safety\n",
    "    Baseada no quickstart oficial da Microsoft\n",
    "    \"\"\"\n",
    "    # Obter credenciais das variáveis de ambiente\n",
    "    key = os.getenv('CONTENT_SAFETY_KEY')\n",
    "    endpoint = os.getenv('CONTENT_SAFETY_ENDPOINT')\n",
    "    \n",
    "    if not key or not endpoint:\n",
    "        print(\"❌ Erro: Defina as variáveis CONTENT_SAFETY_KEY e CONTENT_SAFETY_ENDPOINT no arquivo .env\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Criar cliente Azure AI Content Safety\n",
    "        client = ContentSafetyClient(endpoint, AzureKeyCredential(key))\n",
    "        \n",
    "        # Configurar requisição\n",
    "        request = AnalyzeTextOptions(text=text_to_analyze)\n",
    "        \n",
    "        # Analisar texto\n",
    "        response = client.analyze_text(request)\n",
    "        \n",
    "        # Extrair resultados por categoria específica (seguindo quickstart)\n",
    "        hate_result = next((item for item in response.categories_analysis if item.category == TextCategory.HATE), None)\n",
    "        self_harm_result = next((item for item in response.categories_analysis if item.category == TextCategory.SELF_HARM), None)\n",
    "        sexual_result = next((item for item in response.categories_analysis if item.category == TextCategory.SEXUAL), None)\n",
    "        violence_result = next((item for item in response.categories_analysis if item.category == TextCategory.VIOLENCE), None)\n",
    "        \n",
    "        # Exibir resultados\n",
    "        print(f\"📝 Texto analisado: '{text_to_analyze}'\")\n",
    "        print(\"🔍 Resultados da análise:\")\n",
    "        \n",
    "        if hate_result:\n",
    "            print(f\"  💬 Ódio (Hate): Severidade {hate_result.severity}\")\n",
    "        if self_harm_result:\n",
    "            print(f\"  🩹 Autolesão (Self-harm): Severidade {self_harm_result.severity}\")\n",
    "        if sexual_result:\n",
    "            print(f\"  🔞 Sexual: Severidade {sexual_result.severity}\")\n",
    "        if violence_result:\n",
    "            print(f\"  ⚔️  Violência (Violence): Severidade {violence_result.severity}\")\n",
    "        \n",
    "        # Interpretar nível de risco geral\n",
    "        max_severity = max([\n",
    "            hate_result.severity if hate_result else 0,\n",
    "            self_harm_result.severity if self_harm_result else 0,\n",
    "            sexual_result.severity if sexual_result else 0,\n",
    "            violence_result.severity if violence_result else 0\n",
    "        ])\n",
    "        \n",
    "        if max_severity == 0:\n",
    "            risk_level = \"✅ Seguro\"\n",
    "        elif max_severity <= 2:\n",
    "            risk_level = \"⚠️ Baixo risco\"\n",
    "        elif max_severity <= 4:\n",
    "            risk_level = \"🔸 Risco moderado\"\n",
    "        else:\n",
    "            risk_level = \"🔴 Alto risco\"\n",
    "            \n",
    "        print(f\"📊 Avaliação geral: {risk_level}\")\n",
    "        return response\n",
    "        \n",
    "    except HttpResponseError as e:\n",
    "        print(\"❌ Falha na análise de texto.\")\n",
    "        if e.error:\n",
    "            print(f\"Código do erro: {e.error.code}\")\n",
    "            print(f\"Mensagem do erro: {e.error.message}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro inesperado: {e}\")\n",
    "        return None\n",
    "\n",
    "# Configurar cliente Content Safety\n",
    "content_safety_endpoint = os.getenv('CONTENT_SAFETY_ENDPOINT')\n",
    "content_safety_key = os.getenv('CONTENT_SAFETY_KEY')\n",
    "\n",
    "if content_safety_endpoint and content_safety_key:\n",
    "    print(\"=== Content Safety - Análise de Texto ===\")\n",
    "    print(\"Baseado no quickstart oficial da Microsoft\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Textos de exemplo para análise (incluindo diferentes níveis de risco)\n",
    "    test_texts = [\n",
    "        \"Olá! Como você está hoje? Tenha um ótimo dia!\",\n",
    "        \"Este é um texto neutro sobre tecnologia e programação em Python.\",\n",
    "        \"Estou muito bravo com esta situação, mas vou resolver de forma civilizada.\",\n",
    "        \"Texto de teste para moderação de conteúdo potencialmente problemático.\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"📋 Analisando {len(test_texts)} textos de exemplo...\\n\")\n",
    "    \n",
    "    for idx, text in enumerate(test_texts, 1):\n",
    "        print(f\"🔍 Análise {idx}/{len(test_texts)}:\")\n",
    "        result = analyze_text_content(text)\n",
    "        \n",
    "        if result:\n",
    "            print(\"✅ Análise concluída com sucesso\")\n",
    "        else:\n",
    "            print(\"❌ Falha na análise\")\n",
    "            \n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Configuração necessária:\")\n",
    "    print(\"Defina as variáveis CONTENT_SAFETY_ENDPOINT e CONTENT_SAFETY_KEY no arquivo .env\")\n",
    "    print(\"\\nExemplo de configuração:\")\n",
    "    print(\"CONTENT_SAFETY_ENDPOINT=https://your-content-safety.cognitiveservices.azure.com/\")\n",
    "    print(\"CONTENT_SAFETY_KEY=your-content-safety-key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b195fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo - Content Safety para Imagens (seguindo quickstart oficial)\n",
    "from azure.ai.contentsafety.models import AnalyzeImageOptions, ImageData\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "def analyze_image_content(image_path):\n",
    "    \"\"\"\n",
    "    Função para analisar imagem usando Azure Content Safety\n",
    "    Baseada no quickstart oficial da Microsoft\n",
    "    \"\"\"\n",
    "    # Obter credenciais das variáveis de ambiente\n",
    "    key = os.getenv('CONTENT_SAFETY_KEY')\n",
    "    endpoint = os.getenv('CONTENT_SAFETY_ENDPOINT')\n",
    "    \n",
    "    if not key or not endpoint:\n",
    "        print(\"❌ Erro: Defina as variáveis CONTENT_SAFETY_KEY e CONTENT_SAFETY_ENDPOINT no arquivo .env\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Criar cliente Azure AI Content Safety\n",
    "        client = ContentSafetyClient(endpoint, AzureKeyCredential(key))\n",
    "        \n",
    "        # Ler imagem\n",
    "        with open(image_path, \"rb\") as file:\n",
    "            image_data = file.read()\n",
    "            \n",
    "        # Configurar requisição para análise de imagem\n",
    "        request = AnalyzeImageOptions(image=ImageData(content=image_data))\n",
    "        \n",
    "        # Analisar imagem\n",
    "        response = client.analyze_image(request)\n",
    "        \n",
    "        # Extrair resultados por categoria específica\n",
    "        hate_result = next((item for item in response.categories_analysis if item.category == TextCategory.HATE), None)\n",
    "        self_harm_result = next((item for item in response.categories_analysis if item.category == TextCategory.SELF_HARM), None)\n",
    "        sexual_result = next((item for item in response.categories_analysis if item.category == TextCategory.SEXUAL), None)\n",
    "        violence_result = next((item for item in response.categories_analysis if item.category == TextCategory.VIOLENCE), None)\n",
    "        \n",
    "        # Exibir resultados\n",
    "        print(f\"🖼️ Imagem analisada: {image_path}\")\n",
    "        print(\"🔍 Resultados da análise:\")\n",
    "        \n",
    "        if hate_result:\n",
    "            print(f\"  💬 Ódio (Hate): Severidade {hate_result.severity}\")\n",
    "        if self_harm_result:\n",
    "            print(f\"  🩹 Autolesão (Self-harm): Severidade {self_harm_result.severity}\")\n",
    "        if sexual_result:\n",
    "            print(f\"  🔞 Sexual: Severidade {sexual_result.severity}\")\n",
    "        if violence_result:\n",
    "            print(f\"  ⚔️  Violência (Violence): Severidade {violence_result.severity}\")\n",
    "        \n",
    "        # Interpretar nível de risco geral\n",
    "        max_severity = max([\n",
    "            hate_result.severity if hate_result else 0,\n",
    "            self_harm_result.severity if self_harm_result else 0,\n",
    "            sexual_result.severity if sexual_result else 0,\n",
    "            violence_result.severity if violence_result else 0\n",
    "        ])\n",
    "        \n",
    "        if max_severity == 0:\n",
    "            risk_level = \"✅ Seguro\"\n",
    "        elif max_severity <= 2:\n",
    "            risk_level = \"⚠️ Baixo risco\"\n",
    "        elif max_severity <= 4:\n",
    "            risk_level = \"🔸 Risco moderado\"\n",
    "        else:\n",
    "            risk_level = \"🔴 Alto risco\"\n",
    "            \n",
    "        print(f\"📊 Avaliação geral: {risk_level}\")\n",
    "        return response\n",
    "        \n",
    "    except HttpResponseError as e:\n",
    "        print(\"❌ Falha na análise de imagem.\")\n",
    "        if e.error:\n",
    "            print(f\"Código do erro: {e.error.code}\")\n",
    "            print(f\"Mensagem do erro: {e.error.message}\")\n",
    "        return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Erro: Arquivo de imagem não encontrado: {image_path}\")\n",
    "        print(\"Verifique se o caminho da imagem está correto.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro inesperado: {e}\")\n",
    "        return None\n",
    "\n",
    "if content_safety_endpoint and content_safety_key:\n",
    "    print(\"\\n=== Content Safety - Análise de Imagem ===\")\n",
    "    print(\"Baseado no quickstart oficial da Microsoft\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Caminhos para imagens de exemplo\n",
    "    image_paths = [\n",
    "        \"../../../samples/234039841.jpg\",\n",
    "        \"../../../samples/car-accident.png\"\n",
    "    ]\n",
    "    \n",
    "    for idx, image_path in enumerate(image_paths, 1):\n",
    "        print(f\"\\n🔍 Análise {idx}/{len(image_paths)}:\")\n",
    "        result = analyze_image_content(image_path)\n",
    "        \n",
    "        if result:\n",
    "            print(\"✅ Análise concluída com sucesso\")\n",
    "        else:\n",
    "            print(\"❌ Falha na análise\")\n",
    "            \n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Configuração necessária:\")\n",
    "    print(\"Defina as variáveis CONTENT_SAFETY_ENDPOINT e CONTENT_SAFETY_KEY no arquivo .env\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
