{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec4fbfc",
   "metadata": {},
   "source": [
    "# Azure AI Foundry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d0647",
   "metadata": {},
   "source": [
    "<center><img src=\"../../../images/Azure-AI-Foundry_1600x900.jpg\" alt=\"Azure AI Foundry\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0694e061",
   "metadata": {},
   "source": [
    "## Laboratório 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78012a8",
   "metadata": {},
   "source": [
    "### Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d455425",
   "metadata": {},
   "source": [
    "Prompt Engineering é a arte e ciência de criar instruções eficazes para modelos de linguagem de grande escala (LLMs). É uma habilidade essencial para obter resultados precisos, relevantes e úteis ao interagir com IA generativa.\n",
    "\n",
    "**O que é Prompt Engineering?**\n",
    "\n",
    "Prompt Engineering envolve o design cuidadoso de instruções (prompts) que orientam o comportamento de modelos de IA para produzir saídas desejadas. Isso inclui a escolha de palavras, estruturação de informações, fornecimento de contexto e definição de formatos de saída.\n",
    "\n",
    "**Por que é Importante?**\n",
    "\n",
    "- **Precisão**: Prompts bem elaborados produzem resultados mais precisos\n",
    "- **Consistência**: Técnicas estruturadas garantem resultados previsíveis\n",
    "- **Eficiência**: Reduz a necessidade de múltiplas tentativas\n",
    "- **Controle**: Permite maior controle sobre o estilo e formato da saída\n",
    "\n",
    "**Componentes de um Bom Prompt:**\n",
    "\n",
    "1. **Contexto**: Informações de fundo relevantes\n",
    "2. **Instrução**: O que você quer que o modelo faça\n",
    "3. **Exemplos**: Demonstrações do formato desejado (quando aplicável)\n",
    "4. **Limitações**: Restrições ou diretrizes específicas\n",
    "5. **Formato de Saída**: Como a resposta deve ser estruturada\n",
    "\n",
    "Nas seções seguintes, exploraremos várias técnicas avançadas de Prompt Engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa8df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração do Azure OpenAI\n",
    "import json\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carrega as variáveis de ambiente\n",
    "load_dotenv(dotenv_path=\"../../../.env\")\n",
    "\n",
    "# Configurações\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "api_version = os.getenv(\"API_VERSION\")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "\n",
    "# Inicializa o cliente\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=azure_endpoint, \n",
    "    api_key=api_key,  \n",
    "    api_version=api_version\n",
    ")\n",
    "\n",
    "print(\"Cliente Azure OpenAI configurado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09db70a",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e70e2c7",
   "metadata": {},
   "source": [
    "Zero-Shot Prompting é uma técnica onde você fornece uma tarefa ao modelo de linguagem sem exemplos prévios. O modelo deve entender e executar a tarefa baseado apenas em sua compreensão geral e conhecimento pré-treinado.\n",
    "\n",
    "**Características:**\n",
    "- Não requer exemplos de entrada/saída\n",
    "- Depende do conhecimento pré-treinado do modelo\n",
    "- Simples de implementar\n",
    "- Pode não ser eficaz para tarefas complexas ou específicas\n",
    "\n",
    "**Exemplo:**\n",
    "```\n",
    "Prompt: \"Traduza a seguinte frase para o inglês: 'Olá, como você está?'\"\n",
    "Resposta: \"Hello, how are you?\"\n",
    "```\n",
    "\n",
    "**Quando usar:**\n",
    "- Tarefas simples e bem definidas\n",
    "- Quando você não tem exemplos disponíveis\n",
    "- Para teste inicial de capacidades do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1207fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo prático de Zero-Shot Prompting\n",
    "print(\"=== TESTE: Zero-Shot Prompting ===\")\n",
    "\n",
    "# Exemplo 1: Tradução\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Traduza a seguinte frase para o inglês: 'Olá, como você está?'\"}\n",
    "    ],\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"Tradução:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Exemplo 2: Classificação de sentimento\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Classifique o sentimento desta frase como positivo, negativo ou neutro: 'Este produto é incrível, recomendo muito!'\"}\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"Classificação de sentimento:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b7f0c2",
   "metadata": {},
   "source": [
    "### Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66942a38",
   "metadata": {},
   "source": [
    "Few-Shot Prompting é uma técnica onde você fornece alguns exemplos de entrada e saída esperada antes de apresentar a tarefa real. Isso ajuda o modelo a entender melhor o padrão e formato desejado da resposta.\n",
    "\n",
    "**Características:**\n",
    "- Inclui 2-5 exemplos demonstrativos\n",
    "- Melhora a precisão em comparação ao zero-shot\n",
    "- Ajuda o modelo a entender o formato de saída\n",
    "- Eficaz para tarefas que seguem padrões específicos\n",
    "\n",
    "**Exemplo:**\n",
    "```\n",
    "Prompt: \n",
    "\"Classifique o sentimento das seguintes frases:\n",
    "Frase: 'Eu amo este produto!' → Sentimento: Positivo\n",
    "Frase: 'Este serviço é terrível.' → Sentimento: Negativo\n",
    "Frase: 'O produto está ok.' → Sentimento: Neutro\n",
    "Frase: 'Estou muito feliz com a compra!' → Sentimento: ?\"\n",
    "```\n",
    "\n",
    "**Quando usar:**\n",
    "- Tarefas que requerem formato específico\n",
    "- Quando zero-shot não fornece resultados adequados\n",
    "- Para classificação ou tarefas estruturadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158edfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo prático de Few-Shot Prompting\n",
    "print(\"=== TESTE: Few-Shot Prompting ===\")\n",
    "\n",
    "# Exemplo: Classificação de sentimentos com exemplos\n",
    "few_shot_prompt = \"\"\"Classifique o sentimento das seguintes frases:\n",
    "\n",
    "Frase: 'Eu amo este produto!' → Sentimento: Positivo\n",
    "Frase: 'Este serviço é terrível.' → Sentimento: Negativo  \n",
    "Frase: 'O produto está ok.' → Sentimento: Neutro\n",
    "Frase: 'Não consegui usar o aplicativo, muito confuso.' → Sentimento: Negativo\n",
    "Frase: 'Funcionou perfeitamente, exatamente como esperado!' → Sentimento: Positivo\n",
    "\n",
    "Frase: 'O atendimento foi razoável, nada demais.' → Sentimento: ?\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": few_shot_prompt}\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"Classificação com Few-Shot:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Exemplo 2: Formatação de dados com exemplos\n",
    "format_prompt = \"\"\"Converta os dados para o formato especificado:\n",
    "\n",
    "Nome: João Silva, Idade: 30, Cidade: São Paulo → {\"nome\": \"João Silva\", \"idade\": 30, \"cidade\": \"São Paulo\"}\n",
    "Nome: Maria Santos, Idade: 25, Cidade: Rio de Janeiro → {\"nome\": \"Maria Santos\", \"idade\": 25, \"cidade\": \"Rio de Janeiro\"}\n",
    "Nome: Pedro Costa, Idade: 45, Cidade: Belo Horizonte → {\"nome\": \"Pedro Costa\", \"idade\": 45, \"cidade\": \"Belo Horizonte\"}\n",
    "\n",
    "Nome: Ana Oliveira, Idade: 28, Cidade: Brasília → ?\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": format_prompt}\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"Formatação com Few-Shot:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71159cc",
   "metadata": {},
   "source": [
    "### Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332abde8",
   "metadata": {},
   "source": [
    "Chain-of-Thought (CoT) Prompting é uma técnica que encoraja o modelo a mostrar seu raciocínio passo a passo antes de chegar à resposta final. Isso melhora significativamente a performance em tarefas de raciocínio complexo.\n",
    "\n",
    "**Características:**\n",
    "- Quebra problemas complexos em etapas menores\n",
    "- Mostra o processo de raciocínio\n",
    "- Melhora a precisão em problemas matemáticos e lógicos\n",
    "- Permite identificar onde o raciocínio pode ter falhado\n",
    "\n",
    "**Exemplo:**\n",
    "```\n",
    "Prompt: \"Resolva passo a passo: Se um trem viaja a 60 km/h e precisa percorrer 180 km, quanto tempo levará?\n",
    "\n",
    "Vamos pensar passo a passo:\n",
    "1. Velocidade = 60 km/h\n",
    "2. Distância = 180 km\n",
    "3. Tempo = Distância ÷ Velocidade\n",
    "4. Tempo = 180 ÷ 60 = 3 horas\n",
    "\n",
    "Resposta: 3 horas\"\n",
    "```\n",
    "\n",
    "**Quando usar:**\n",
    "- Problemas matemáticos ou lógicos\n",
    "- Tarefas que requerem raciocínio multi-etapas\n",
    "- Quando você precisa verificar o processo de pensamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64065cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo prático de Chain-of-Thought Prompting\n",
    "print(\"=== TESTE: Chain-of-Thought Prompting ===\")\n",
    "\n",
    "# Exemplo 1: Problema matemático\n",
    "cot_prompt = \"\"\"Resolva passo a passo: \n",
    "\n",
    "Em uma loja, há 24 camisetas. 1/3 delas são azuis, 1/4 são vermelhas e o restante são brancas. \n",
    "Se o preço de cada camiseta azul é R$ 30, cada vermelha R$ 25 e cada branca R$ 20, qual é o valor total do estoque?\n",
    "\n",
    "Vamos pensar passo a passo:\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": cot_prompt}\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"Solução com Chain-of-Thought:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Exemplo 2: Problema lógico\n",
    "logic_prompt = \"\"\"Resolva este problema de lógica passo a passo:\n",
    "\n",
    "Ana, Bruno e Carlos estão em uma fila. \n",
    "- Ana não está na frente\n",
    "- Bruno não está no meio\n",
    "- Carlos não está atrás\n",
    "\n",
    "Qual é a ordem da fila?\n",
    "\n",
    "Vamos analisar cada pista:\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": logic_prompt}\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"Solução lógica com Chain-of-Thought:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e37d85a",
   "metadata": {},
   "source": [
    "### Meta Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94905a3d",
   "metadata": {},
   "source": [
    "Meta Prompting é uma técnica avançada onde o modelo é instruído a gerar ou melhorar prompts. Essencialmente, você usa o modelo para criar prompts melhores para ele mesmo ou para outras tarefas.\n",
    "\n",
    "**Características:**\n",
    "- O modelo ajuda a criar prompts mais eficazes\n",
    "- Pode melhorar iterativamente a qualidade dos prompts\n",
    "- Útil para otimização automática de prompts\n",
    "- Requer conhecimento sobre técnicas de prompting\n",
    "\n",
    "**Exemplo:**\n",
    "```\n",
    "Prompt: \"Crie um prompt eficaz para fazer um modelo de IA explicar conceitos científicos complexos para crianças de 10 anos. O prompt deve incluir:\n",
    "- Linguagem simples\n",
    "- Analogias adequadas para a idade\n",
    "- Estrutura clara\n",
    "- Elementos interativos\"\n",
    "\n",
    "Resposta: \"Explique [conceito científico] para uma criança de 10 anos. Use palavras simples, compare com coisas que ela conhece do dia a dia, organize em 3 partes principais e faça perguntas para manter o interesse.\"\n",
    "```\n",
    "\n",
    "**Quando usar:**\n",
    "- Otimização de prompts existentes\n",
    "- Criação de prompts para tarefas específicas\n",
    "- Quando você precisa de múltiplas variações de um prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df50e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo prático de Meta Prompting\n",
    "print(\"=== TESTE: Meta Prompting ===\")\n",
    "\n",
    "# Exemplo 1: Criação de prompt para ensino\n",
    "meta_prompt = \"\"\"Crie um prompt eficaz para fazer um modelo de IA explicar conceitos de programação para iniciantes. O prompt deve incluir:\n",
    "- Linguagem simples e acessível\n",
    "- Analogias com o mundo real\n",
    "- Estrutura clara e didática  \n",
    "- Exemplos práticos\n",
    "- Verificação de compreensão\n",
    "\n",
    "Forneça apenas o prompt otimizado, sem explicações adicionais.\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": meta_prompt}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "generated_prompt = response.choices[0].message.content\n",
    "print(\"Prompt gerado pelo Meta Prompting:\")\n",
    "print(generated_prompt)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Exemplo 2: Testando o prompt gerado\n",
    "test_content = generated_prompt + \"\\n\\nConceito: Variáveis em programação\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": test_content}\n",
    "    ],\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "print(\"Teste do prompt gerado:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ee17a",
   "metadata": {},
   "source": [
    "### Prompt Chaining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e43cc",
   "metadata": {},
   "source": [
    "Prompt Chaining é uma técnica onde você quebra uma tarefa complexa em múltiplos prompts sequenciais, onde a saída de um prompt serve como entrada para o próximo.\n",
    "\n",
    "**Características:**\n",
    "- Divide tarefas complexas em etapas menores\n",
    "- A saída de um prompt alimenta o próximo\n",
    "- Permite maior controle sobre cada etapa\n",
    "- Reduz a chance de erros em tarefas complexas\n",
    "\n",
    "**Exemplo:**\n",
    "```\n",
    "Prompt 1: \"Analise este texto e identifique os pontos principais: [texto]\"\n",
    "Saída 1: \"Pontos principais: A, B, C\"\n",
    "\n",
    "Prompt 2: \"Com base nos pontos principais: A, B, C, crie um resumo executivo de 100 palavras\"\n",
    "Saída 2: [Resumo executivo]\n",
    "\n",
    "Prompt 3: \"Transforme este resumo em uma apresentação de 3 slides: [resumo]\"\n",
    "```\n",
    "\n",
    "**Quando usar:**\n",
    "- Tarefas que envolvem múltiplas etapas de processamento\n",
    "- Quando você precisa de controle granular sobre cada fase\n",
    "- Para transformação de dados em múltiplos formatos\n",
    "- Análise complexa que requer validação em cada etapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b41b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo prático de Prompt Chaining\n",
    "print(\"=== TESTE: Prompt Chaining ===\")\n",
    "\n",
    "# Texto de exemplo para análise\n",
    "texto_exemplo = \"\"\"\n",
    "A inteligência artificial está transformando rapidamente diversos setores da economia. \n",
    "Na área da saúde, algoritmos de machine learning estão sendo usados para diagnósticos \n",
    "mais precisos e descoberta de novos medicamentos. No setor financeiro, a IA ajuda na \n",
    "detecção de fraudes e análise de riscos. Na educação, sistemas inteligentes personalizam \n",
    "o aprendizado para cada estudante. Entretanto, também surgem desafios éticos e questões \n",
    "sobre o futuro do trabalho.\n",
    "\"\"\"\n",
    "\n",
    "# Prompt 1: Identificar pontos principais\n",
    "print(\"1. Identificando pontos principais...\")\n",
    "response1 = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Analise este texto e identifique os 3 pontos principais em formato de lista:\\n\\n{texto_exemplo}\"}\n",
    "    ],\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "pontos_principais = response1.choices[0].message.content\n",
    "print(\"Pontos principais identificados:\")\n",
    "print(pontos_principais)\n",
    "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "\n",
    "# Prompt 2: Criar resumo executivo\n",
    "print(\"2. Criando resumo executivo...\")\n",
    "response2 = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Com base nos pontos principais identificados:\\n{pontos_principais}\\n\\nCrie um resumo executivo de exatamente 50 palavras.\"}\n",
    "    ],\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "resumo_executivo = response2.choices[0].message.content\n",
    "print(\"Resumo executivo:\")\n",
    "print(resumo_executivo)\n",
    "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "\n",
    "# Prompt 3: Criar apresentação\n",
    "print(\"3. Criando estrutura de apresentação...\")\n",
    "response3 = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Transforme este resumo em uma estrutura de apresentação de 3 slides com títulos e tópicos:\\n\\n{resumo_executivo}\"}\n",
    "    ],\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "print(\"Estrutura da apresentação:\")\n",
    "print(response3.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842bfd5",
   "metadata": {},
   "source": [
    "### Tree of Thoughts (ToT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185847f1",
   "metadata": {},
   "source": [
    "Tree of Thoughts (ToT) é uma técnica avançada que permite ao modelo explorar múltiplos caminhos de raciocínio simultaneamente, como uma árvore de decisão, avaliando e escolhendo os melhores caminhos.\n",
    "\n",
    "**Características:**\n",
    "- Explora múltiplas abordagens simultaneamente\n",
    "- Permite backtracking quando um caminho não funciona\n",
    "- Avalia a qualidade de cada \"pensamento\" ou etapa\n",
    "- Mais robusto que Chain-of-Thought linear\n",
    "\n",
    "**Exemplo:**\n",
    "```\n",
    "Problema: \"Como resolver o conflito entre equipes?\"\n",
    "\n",
    "Pensamento 1: Mediação direta\n",
    "├── Avaliação: Pode ser confrontativo\n",
    "├── Próximo passo: Reunião formal\n",
    "└── Resultado esperado: Resolução rápida\n",
    "\n",
    "Pensamento 2: Facilitação indireta\n",
    "├── Avaliação: Menos estressante\n",
    "├── Próximo passo: Conversas individuais\n",
    "└── Resultado esperado: Compreensão gradual\n",
    "\n",
    "Pensamento 3: Reestruturação de processos\n",
    "├── Avaliação: Solução sistemática\n",
    "├── Próximo passo: Análise de workflows\n",
    "└── Resultado esperado: Prevenção futura\n",
    "\n",
    "Melhor caminho: Combinação de 2 e 3\n",
    "```\n",
    "\n",
    "**Quando usar:**\n",
    "- Problemas complexos com múltiplas soluções possíveis\n",
    "- Quando você precisa explorar alternativas\n",
    "- Planejamento estratégico e tomada de decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87786e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo prático de Tree of Thoughts (ToT)\n",
    "print(\"=== TESTE: Tree of Thoughts (ToT) ===\")\n",
    "\n",
    "# Problema complexo para análise\n",
    "tot_prompt = \"\"\"Problema: Uma empresa de tecnologia está enfrentando alta rotatividade de funcionários (30% ao ano). \n",
    "Como CEO, você precisa resolver isso rapidamente.\n",
    "\n",
    "Explore 3 abordagens diferentes simultaneamente:\n",
    "\n",
    "ABORDAGEM 1 - Melhoria Salarial:\n",
    "- Avalie: Prós, contras e viabilidade\n",
    "- Próximos passos específicos\n",
    "- Resultado esperado\n",
    "\n",
    "ABORDAGEM 2 - Melhoria do Ambiente de Trabalho:\n",
    "- Avalie: Prós, contras e viabilidade  \n",
    "- Próximos passos específicos\n",
    "- Resultado esperado\n",
    "\n",
    "ABORDAGEM 3 - Programa de Desenvolvimento:\n",
    "- Avalie: Prós, contras e viabilidade\n",
    "- Próximos passos específicos\n",
    "- Resultado esperado\n",
    "\n",
    "ANÁLISE FINAL:\n",
    "- Compare as 3 abordagens\n",
    "- Recomende a melhor estratégia ou combinação\n",
    "- Justifique sua escolha\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": tot_prompt}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "print(\"Análise Tree of Thoughts:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Exemplo 2: Problema de planejamento\n",
    "planning_prompt = \"\"\"Você precisa planejar o lançamento de um novo produto mobile app em 6 meses.\n",
    "Explore simultaneamente 3 estratégias de go-to-market:\n",
    "\n",
    "ESTRATÉGIA A - Lançamento Gradual:\n",
    "- Análise de viabilidade: \n",
    "- Recursos necessários:\n",
    "- Riscos e mitigações:\n",
    "\n",
    "ESTRATÉGIA B - Lançamento Massivo:\n",
    "- Análise de viabilidade:\n",
    "- Recursos necessários: \n",
    "- Riscos e mitigações:\n",
    "\n",
    "ESTRATÉGIA C - Lançamento por Nicho:\n",
    "- Análise de viabilidade:\n",
    "- Recursos necessários:\n",
    "- Riscos e mitigações:\n",
    "\n",
    "DECISÃO: Escolha a melhor estratégia baseado na análise.\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": planning_prompt}\n",
    "    ],\n",
    "    temperature=0.6,\n",
    "    max_tokens=800\n",
    ")\n",
    "\n",
    "print(\"Planejamento com Tree of Thoughts:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb968461",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b0d285",
   "metadata": {},
   "source": [
    "Retrieval Augmented Generation (RAG) é uma técnica que combina a geração de texto do modelo com a recuperação de informações de uma base de conhecimento externa, permitindo respostas mais precisas e atualizadas.\n",
    "\n",
    "**Características:**\n",
    "- Combina geração com recuperação de informações\n",
    "- Acessa conhecimento externo e atualizado\n",
    "- Reduz alucinações do modelo\n",
    "- Permite citar fontes específicas\n",
    "\n",
    "**Componentes do RAG:**\n",
    "1. **Base de Conhecimento**: Documentos, artigos, bases de dados\n",
    "2. **Sistema de Recuperação**: Busca informações relevantes\n",
    "3. **Gerador**: Modelo de linguagem que cria a resposta\n",
    "4. **Integração**: Combina informações recuperadas com geração\n",
    "\n",
    "**Exemplo de Processo:**\n",
    "```\n",
    "Pergunta: \"Quais são as novidades do Azure AI em 2024?\"\n",
    "\n",
    "1. Recuperação: Busca documentos recentes sobre Azure AI\n",
    "2. Contexto: \"Azure AI Foundry foi lançado em 2024...\"\n",
    "3. Prompt: \"Com base nas informações: [contexto], responda: [pergunta]\"\n",
    "4. Resposta: Gerada com base no contexto recuperado\n",
    "```\n",
    "\n",
    "**Quando usar:**\n",
    "- Quando você precisa de informações atualizadas\n",
    "- Para reduzir alucinações\n",
    "- Em sistemas de Q&A corporativos\n",
    "- Quando o modelo precisa citar fontes específicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0232a728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo prático de RAG (Simulado)\n",
    "print(\"=== TESTE: Retrieval Augmented Generation (RAG) ===\")\n",
    "\n",
    "# Simulando uma base de conhecimento (normalmente seria recuperada de um banco de dados vetorial)\n",
    "knowledge_base = \"\"\"\n",
    "DOCUMENTO 1 - Azure AI Foundry (2024):\n",
    "O Azure AI Foundry é uma plataforma unificada para desenvolvimento de aplicações de IA generativa. \n",
    "Lançado em 2024, oferece ferramentas integradas para treinar, avaliar e implantar modelos de IA.\n",
    "Inclui recursos como prompt flow, avaliação automatizada e monitoramento de modelos.\n",
    "\n",
    "DOCUMENTO 2 - Azure OpenAI Service:\n",
    "O Azure OpenAI Service fornece acesso aos modelos GPT-4, GPT-3.5-turbo, DALL-E e Codex através de APIs REST.\n",
    "Oferece recursos empresariais como redes virtuais, chaves gerenciadas pelo cliente e compliance.\n",
    "Disponível em múltiplas regiões com diferentes modelos e capacidades.\n",
    "\n",
    "DOCUMENTO 3 - Prompt Engineering Best Practices:\n",
    "Técnicas essenciais incluem few-shot learning, chain-of-thought e prompt chaining.\n",
    "Importante ser específico, usar exemplos claros e estruturar bem as instruções.\n",
    "A ordem das informações no prompt pode afetar significativamente os resultados.\n",
    "\"\"\"\n",
    "\n",
    "# Pergunta do usuário\n",
    "pergunta = \"Quais são as principais características do Azure AI Foundry lançado em 2024?\"\n",
    "\n",
    "# Prompt RAG: Combinando contexto recuperado com a pergunta\n",
    "rag_prompt = f\"\"\"Com base nas informações fornecidas abaixo, responda à pergunta do usuário de forma precisa e cite as fontes relevantes.\n",
    "\n",
    "CONTEXTO RECUPERADO:\n",
    "{knowledge_base}\n",
    "\n",
    "PERGUNTA: {pergunta}\n",
    "\n",
    "INSTRUÇÕES:\n",
    "- Use apenas as informações fornecidas no contexto\n",
    "- Cite o documento específico quando relevante\n",
    "- Se a informação não estiver disponível no contexto, indique claramente\n",
    "- Seja preciso e objetivo na resposta\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": rag_prompt}\n",
    "    ],\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "print(\"Resposta RAG:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Exemplo 2: Pergunta que não está no contexto\n",
    "pergunta2 = \"Qual é o preço do Azure AI Foundry?\"\n",
    "\n",
    "rag_prompt2 = f\"\"\"Com base nas informações fornecidas abaixo, responda à pergunta do usuário.\n",
    "\n",
    "CONTEXTO RECUPERADO:\n",
    "{knowledge_base}\n",
    "\n",
    "PERGUNTA: {pergunta2}\n",
    "\n",
    "INSTRUÇÕES:\n",
    "- Use apenas as informações fornecidas no contexto\n",
    "- Se a informação não estiver disponível, responda: \"Informação não encontrada no contexto fornecido\"\n",
    "\"\"\"\n",
    "\n",
    "response2 = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": rag_prompt2}\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"Teste com informação não disponível:\")\n",
    "print(response2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c526786",
   "metadata": {},
   "source": [
    "### Active-Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2031b02a",
   "metadata": {},
   "source": [
    "Active-Prompt é uma técnica que seleciona automaticamente os exemplos mais úteis e informativos para usar em few-shot prompting, baseando-se na incerteza do modelo sobre certas questões.\n",
    "\n",
    "**Características:**\n",
    "- Seleção automática de exemplos mais úteis\n",
    "- Baseada na incerteza do modelo\n",
    "- Melhora a eficiência do few-shot learning\n",
    "- Reduz a necessidade de curadoria manual de exemplos\n",
    "\n",
    "**Como Funciona:**\n",
    "1. **Análise de Incerteza**: Identifica onde o modelo tem mais dúvidas\n",
    "2. **Seleção de Exemplos**: Escolhe exemplos que abordam essas incertezas\n",
    "3. **Prompting Adaptativo**: Usa os exemplos mais relevantes para cada consulta\n",
    "4. **Refinamento Contínuo**: Melhora com base no feedback\n",
    "\n",
    "**Exemplo Conceitual:**\n",
    "```\n",
    "Tarefa: Classificação de sentimentos\n",
    "\n",
    "Modelo identifica incerteza em:\n",
    "- Sarcasmo\n",
    "- Linguagem ambígua\n",
    "- Expressões culturais\n",
    "\n",
    "Active-Prompt seleciona exemplos que abordam especificamente:\n",
    "- \"Que maravilha, choveu no meu casamento!\" (Sarcasmo → Negativo)\n",
    "- \"Não sei se gostei...\" (Ambíguo → Neutro)\n",
    "- \"Tá de boa!\" (Cultural → Positivo)\n",
    "```\n",
    "\n",
    "**Quando usar:**\n",
    "- Quando você tem muitos exemplos disponíveis\n",
    "- Para otimizar automaticamente few-shot prompts\n",
    "- Em domínios onde a incerteza varia por tópico\n",
    "- Para sistemas adaptativos que melhoram com o uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f9c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo prático de Active-Prompt (Simulado)\n",
    "print(\"=== TESTE: Active-Prompt ===\")\n",
    "\n",
    "# Simulando o processo de seleção de exemplos baseado na incerteza\n",
    "def simulate_active_prompt():\n",
    "    # Pool de exemplos disponíveis\n",
    "    example_pool = [\n",
    "        (\"Que maravilha, choveu no meu casamento!\", \"Negativo\", \"sarcasmo\"),\n",
    "        (\"Este produto é ok, nada demais.\", \"Neutro\", \"ambiguidade\"),\n",
    "        (\"Adorei a experiência!\", \"Positivo\", \"direto\"),\n",
    "        (\"Tá de boa esse app!\", \"Positivo\", \"linguagem_informal\"),\n",
    "        (\"Não sei se gostei muito...\", \"Neutro\", \"incerteza\"),\n",
    "        (\"Simplesmente fantástico!\", \"Positivo\", \"entusiasmo\"),\n",
    "        (\"Podia ser melhor, né?\", \"Negativo\", \"crítica_indireta\")\n",
    "    ]\n",
    "    \n",
    "    # Pergunta que gera incerteza (linguagem informal + sarcasmo potencial)\n",
    "    query = \"Tá massa esse bagulho, viu!\"\n",
    "    \n",
    "    # Active-Prompt seleciona exemplos mais relevantes para casos similares\n",
    "    selected_examples = [\n",
    "        (\"Que maravilha, choveu no meu casamento!\", \"Negativo\"),  # sarcasmo\n",
    "        (\"Tá de boa esse app!\", \"Positivo\"),  # linguagem informal\n",
    "        (\"Podia ser melhor, né?\", \"Negativo\")  # tom ambíguo\n",
    "    ]\n",
    "    \n",
    "    return query, selected_examples\n",
    "\n",
    "# Executando simulação\n",
    "query, selected_examples = simulate_active_prompt()\n",
    "\n",
    "print(\"Exemplo de Active-Prompt em ação:\")\n",
    "print(f\"Frase para classificar: '{query}'\")\n",
    "print(\"\\nExemplos selecionados automaticamente baseados na incerteza:\")\n",
    "for i, (frase, sentimento) in enumerate(selected_examples, 1):\n",
    "    print(f\"{i}. '{frase}' → {sentimento}\")\n",
    "\n",
    "# Construindo prompt com exemplos selecionados dinamicamente\n",
    "active_prompt = \"Classifique o sentimento (Positivo, Negativo, Neutro) considerando contexto cultural e possível sarcasmo:\\n\\n\"\n",
    "\n",
    "for frase, sentimento in selected_examples:\n",
    "    active_prompt += f\"Frase: '{frase}' → Sentimento: {sentimento}\\n\"\n",
    "\n",
    "active_prompt += f\"\\nFrase: '{query}' → Sentimento: ?\"\n",
    "\n",
    "print(f\"\\n{'-'*50}\")\n",
    "print(\"Prompt construído dinamicamente:\")\n",
    "print(active_prompt)\n",
    "\n",
    "# Executando classificação\n",
    "response = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": active_prompt}\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(f\"\\n{'-'*50}\")\n",
    "print(\"Resultado do Active-Prompt:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Comparação: mesmo prompt sem Active-Prompt (exemplos genéricos)\n",
    "generic_prompt = \"\"\"Classifique o sentimento (Positivo, Negativo, Neutro):\n",
    "\n",
    "Frase: 'Eu amo este produto!' → Sentimento: Positivo\n",
    "Frase: 'Este serviço é terrível.' → Sentimento: Negativo\n",
    "Frase: 'O produto está ok.' → Sentimento: Neutro\n",
    "\n",
    "Frase: 'Tá massa esse bagulho, viu!' → Sentimento: ?\"\"\"\n",
    "\n",
    "response_generic = client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": generic_prompt}\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"Comparação com Few-Shot genérico:\")\n",
    "print(response_generic.choices[0].message.content)\n",
    "print(\"\\nNote: Active-Prompt selecionou exemplos mais relevantes para lidar com linguagem informal e possível sarcasmo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc55621",
   "metadata": {},
   "source": [
    "### Melhores Práticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f05e2fd",
   "metadata": {},
   "source": [
    "**Seja Específico.** Deixe o mínimo possível para interpretação. Restrinja o espaço operacional.\n",
    "\n",
    "**Seja Descritivo.** Use analogias para tornar as instruções mais claras.\n",
    "\n",
    "**Reforce as Instruções.** Às vezes pode ser necessário se repetir para o modelo. Forneça instruções antes e depois do seu conteúdo principal, use uma instrução e uma deixa, etc.\n",
    "\n",
    "**A Ordem Importa.** A ordem em que você apresenta informações ao modelo pode impactar o resultado. Se você coloca instruções antes do seu conteúdo (\"resuma o seguinte...\") ou depois (\"resuma o texto acima...\") pode fazer diferença no resultado. Até mesmo a ordem dos exemplos few-shot pode importar. Isso é conhecido como viés de recência.\n",
    "\n",
    "**Ofereça uma Alternativa ao Modelo.** Às vezes pode ser útil dar ao modelo um caminho alternativo se ele não conseguir completar a tarefa atribuída. Por exemplo, ao fazer uma pergunta sobre um texto, você pode incluir algo como \"responda com 'não encontrado' se a resposta não estiver presente.\" Isso pode ajudar o modelo a evitar gerar respostas falsas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab59629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 ÁREA DE EXPERIMENTAÇÃO\n",
    "# Use este espaço para testar as técnicas de Prompt Engineering\n",
    "\n",
    "# Exemplo: Teste sua própria técnica aqui\n",
    "# response = client.chat.completions.create(\n",
    "#     model=deployment_name,\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": \"Seu prompt aqui...\"}\n",
    "#     ],\n",
    "#     temperature=0.5\n",
    "# )\n",
    "# \n",
    "# print(response.choices[0].message.content)\n",
    "\n",
    "# Seu código aqui..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
